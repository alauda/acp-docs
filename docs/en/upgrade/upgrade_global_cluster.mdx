---
weight: 30
---

# Upgrade the global cluster

<Term name="productShort" /> consists of a **global cluster** and one or more **workload clusters**. The global cluster **must** be upgraded before any workload clusters.

This document walks you through the upgrade procedure for the global cluster.

If the global cluster is configured with the **global DR (Disaster Recovery)** solution, follow the [global DR procedure](#global_dr) strictly. Otherwise, follow the [Standard procedure](#standard) .

## Standard procedure \{#standard}

<Steps>

### Upload images

Copy the core package to **any control plane node** of the global cluster. Extract the package and `cd` into the extracted directory.

* If the global cluster uses the **built-in registry**, run:

  ```bash
  bash upgrade.sh --only-sync-image=true
  ```

* If the global cluster uses an **external registry**, you also need to provide the registry address:

  ```bash
  bash upgrade.sh --only-sync-image=true --registry <registry-address> --username <username> --password <password>
  ```

If you plan to upgrade the **Operator** and **Cluster Plugin** together during the global cluster upgrade, you can pre-push their images to the global cluster's registry in advance.
For bulk upload instructions, see [Push only images from all packages in a directory](../extend/upload_package.mdx#only_push_images).

<Directive type="info" title="INFO">
Uploading images typically takes about 2 hours, depending on your network and disk performance.

If your platform is configured for global disaster recovery (DR), remember that the **standby global cluster also requires image upload**. Be sure to plan your maintenance window accordingly. 
</Directive>

:::warning
When using `violet` to upload packages to a standby cluster, the parameter `--dest-repo <VIP addr of standby cluster>` must be specified.  
Otherwise, the packages will be uploaded to the image repository of the **primary cluster**, preventing the standby cluster from installing or upgrading extensions.  

Also be awared that either authentication info of the standby cluster's image registry or `--no-auth` parameter MUST be provided.
:::

For details of the `violet push` subcommand, please refer to [Upload Packages](../extend/upload_package.mdx#violet_push_usage).

### Trigger the upgrade

After the image upload is complete, run the following command to start the upgrade process:

```bash
bash upgrade.sh --skip-sync-image
```

Wait for the script to finish before proceeding.

If you have already pre-pushed the Operator and Cluster Plugin images to the global cluster's registry, you can then follow [Create only CRs from all packages in a directory](../extend/upload_package.mdx#only_create_cr). After running this command, wait about **10–15 minutes** until upgrade notifications appear for functional components. You will then be able to upgrade the Operator and Cluster Plugin together as part of the subsequent upgrade steps.

### Upgrade the global cluster

<Directive type="warning" title="WARNING">
If the platform has **Data Services** installed, you must also upgrade the related extensions when upgrading clusters.  
For details, see [Upgrade Data Services](../appservice/upgrade.mdx).
</Directive>

1. Log in to the Web Console of the global cluster and switch to **Administrator** view.
2. Navigate to **Clusters > Clusters**.
3. Click on the `global` cluster to open its detail view.
4. Go to the **Functional Components** tab.
5. Click the **Upgrade** button.

Review the available component updates in the dialog, and confirm to proceed.

<Directive type="info" title="INFO">
- Upgrading the Kubernetes version is optional. However, since service disruptions may occur regardless, we recommend including the Kubernetes upgrade to avoid multiple maintenance windows.

- If the **Alauda Container Platform GitOps** is installed in the global cluster, and after the upgrading, the pods of the plugin is running abnormally.Please refer to [Upgrading Alauda Container Platform GitOps](/gitops/upgrade/upgrade_gitops_plugin.html#procedure).
</Directive>

### Install Product Docs Plugin

:::info
The **Alauda Container Platform Product Docs** plugin provides access to product documentation within the platform. All help links throughout the platform will direct users to this documentation. If this plugin is not installed, clicking help links in the platform will result in 404 access errors.
:::

Starting from ACP 4.0, the built-in product documentation has been separated into the **Alauda Container Platform Product Docs** plugin. If you are upgrading from version 3.18, you need to install this plugin by following these steps:

1. Navigate to **Administrator**.

2. In the left sidebar, click **Marketplace** > **Cluster Plugins** and select the `global` cluster.

3. Locate the **Alauda Container Platform Product Docs** plugin and click **Install**.

</Steps>

### Post-upgrade

- <ExternalSiteLink name="ai" href="/upgrade/upgrade-from-aml-1.2.html" children="Upgrade Alauda AI" />

- <ExternalSiteLink name="devops" href="/upgrade/index.html" children="Upgrade Alauda DevOps" />

- <ExternalSiteLink name="servicemeshv1" href="/upgrade.html" children="Upgrade Alauda Service Mesh" />

## global DR procedure \{#global_dr}

<Steps>

### Verify data consistency

Follow your regular global DR inspection procedures to ensure that data in the **standby global cluster** is consistent with the **primary global cluster**.

If inconsistencies are detected, **contact technical support** before proceeding.

On **both** clusters, run the following command to ensure no `Machine` nodes are in a non-running state:

```bash
kubectl get machines.platform.tkestack.io
```

If any such nodes exist, contact technical support to resolve them before continuing.

### Uninstall the etcd sync plugin

<Tabs>
  <Tab label="Upgrading from 3.18">
  1. Access the Web Console of the **primary cluster** via its IP or VIP.
  2. Switch to the **Administrator** view.
  3. Navigate to **Catalog > Cluster Plugin**.
  4. Select `global` from the cluster dropdown.
  5. Find the **EtcdSync** plugin and click **Uninstall**. Wait for the uninstallation to complete.
  </Tab>
</Tabs>

### Upload images

Perform the **Upload images** step on **both** the standby cluster and the primary cluster.

See [Upload images in Standard procedure](#standard) for details.

### Upgrade the standby cluster

<Directive type="info" title="INFO">
Accessing the **standby cluster** Web Console is required to perform the upgrade.

Before proceeding, verify that the **ProductBase** resource of the standby cluster is correctly configured with the cluster VIP under `spec.alternativeURLs`.

If not, update the configuration as follows:

```yaml
apiVersion: product.alauda.io/v1alpha2
kind: ProductBase
metadata:
  name: base
spec:
  alternativeURLs:
    - https://<standby-cluster-vip>
```

</Directive>

On the **standby cluster**, follow the steps in the [Standard procedure](#standard) to complete the upgrade.

### Upgrade the primary cluster

After the standby cluster has been upgraded, proceed with the [Standard procedure](#standard) on the **primary cluster**.

### Reinstall the etcd sync plugin

Before reinstalling, verify that port `2379` is properly forwarded from both global cluster VIPs to their control plane nodes.

To reinstall:

1. Access the Web Console of the **standby global cluster** via its IP or VIP.
2. Switch to **Administrator** view.
3. Go to **Marketplace > Cluster Plugins**.
4. Select the `global` cluster.
5. Locate **Alauda Container Platform etcd Synchronizer**, click **Install**, and provide the required parameters.

To verify installation:

```bash
kubectl get po -n cpaas-system -l app=etcd-sync  # Ensure pod is 1/1 Running

kubectl logs -n cpaas-system $(kubectl get po -n cpaas-system -l app=etcd-sync --no-headers | awk '{print $1}' | head -1) | grep -i "Start Sync update"
# Wait until the logs contain "Start Sync update"

# Recreate the pod to trigger synchronization of resources with ownerReferences
kubectl delete po -n cpaas-system $(kubectl get po -n cpaas-system -l app=etcd-sync --no-headers | awk '{print $1}' | head -1)
```

### Check Synchronization Status

Run the following to verify the synchronization status:

```bash
curl "$(kubectl get svc -n cpaas-system etcd-sync-monitor -ojsonpath='{.spec.clusterIP}')/check"
```

**Explanation of output:**

- `"LOCAL ETCD missed keys:"` – Keys exist in the **primary cluster** but are missing in the standby. This often resolves after a pod restart.
- `"LOCAL ETCD surplus keys:"` – Keys exist in the **standby cluster** but not in the primary. Review these with your operations team before deletion.

</Steps>
