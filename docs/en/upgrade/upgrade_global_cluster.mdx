---
weight: 30
---

# Upgrade the global cluster

<Term name="productShort" /> consists of a **global cluster** and one or more **workload clusters**. The global cluster **must** be upgraded before any workload clusters.

This document walks you through the upgrade procedure for the global cluster.

If the global cluster is configured with the **global DR (Disaster Recovery)** solution, follow the [global DR procedure](#global_dr) strictly. Otherwise, follow the [Standard procedure](#standard) .

## Standard procedure \{#standard}

<Steps>

### Upload images

Copy the core package to **any control plane node** of the global cluster. Extract the package and `cd` into the extracted directory.

- If the global cluster uses the **built-in registry**, run:

  ```bash
  bash upgrade.sh --only-sync-image=true
  ```

- If the global cluster uses an **external registry**, you also need to provide the registry address:

  ```bash
  bash upgrade.sh --only-sync-image=true --registry <registry-address> --username <username> --password <password>
  ```

If you want to upgrade Operators and Cluster Plugins together with the cluster upgrade, you must upload the Extensions to the platform **before upgrading ACP Core** using **violet**.

For bulk upload instructions, see [Upload All Packages in a Directory](../extend/upload_package.mdx#upload_all_packages).

When using **`violet push`** on a **standby global cluster**, you must specify the `--dest-repo` parameter with the VIP of the standby cluster.
For details, see [Upload Packages in a Global DR Environment](../install/global_dr.mdx#upload_dr_packages).

<Directive type="info" title="INFO">
Uploading images typically takes about 2 hours, depending on your network and disk performance.

If your platform uses the global DR, remember that the **standby global cluster also requires image upload**, and plan your maintenance window accordingly.
</Directive>

### Trigger the upgrade

After the image upload is complete, run the following command to start the upgrade process:

```bash
bash upgrade.sh --skip-sync-image
```

Wait for the script to finish before proceeding.

### Upgrade the global cluster

<Directive type="warning" title="WARNING">
If the platform has **Data Services** installed, you must also upgrade the related extensions when upgrading clusters.
For details, see [Upgrade Data Services](../appservice/upgrade.mdx).
</Directive>

1. Log into the Web Console of the global cluster and switch to **Administrator** view.
2. Navigate to **Clusters > Clusters**.
3. Click on the `global` cluster to open its detail view.
4. Go to the **Functional Components** tab.
5. Click the **Upgrade** button.

Review the available component updates shown in the dialog, and confirm to continue.

<Directive type="info" title="INFO">
- Upgrading the Kubernetes version is optional. However, since service disruptions may occur regardless, we recommend including the Kubernetes upgrade to avoid multiple maintenance windows.

- If the **Alauda Container Platform GitOps** is installed in the global cluster, and after the upgrading, the pods of the plugin is running abnormally.Please refer to [Upgrading Alauda Container Platform GitOps](/gitops/upgrade/upgrade_gitops_plugin.html#procedure).
</Directive>

</Steps>

### Post-upgrade

- <ExternalSiteLink name="ai" href="/upgrade/upgrade-from-aml-1.2.html" children="Upgrade Alauda AI" />

- <ExternalSiteLink name="devops" href="/upgrade/index.html" children="Upgrade Alauda DevOps" />

- <ExternalSiteLink name="servicemesh" href="/upgrade.html" children="Upgrade Alauda Service Mesh" />

## global DR procedure \{#global_dr}

<Steps>

### Verify data consistency

Follow your regular global DR inspection procedures to ensure that data in the **standby global cluster** is consistent with the **primary global cluster**.

If inconsistencies are detected, **contact technical support** before proceeding.

On **both** clusters, run the following command to ensure no `Machine` nodes are in a non-running state:

```bash
kubectl get machines.platform.tkestack.io
```

If any such nodes exist, contact technical support to resolve them before continuing.

### Uninstall the etcd sync plugin

<Tabs>
  <Tab label="Upgrading from 3.18">
  1. Access the Web Console of the **primary cluster** via its IP or VIP.
  2. Switch to the **Administrator** view.
  3. Navigate to **Catalog > Cluster Plugin**.
  4. Select `global` from the cluster dropdown.
  5. Find the **EtcdSync** plugin and click **Uninstall**. Wait for the uninstallation to complete.
  </Tab>
</Tabs>

### Upload images

Perform the **Upload images** step on **both** the standby cluster and the primary cluster.

See [Upload images in Standard procedure](#standard) for details.

### Upgrade the standby cluster

<Directive type="info" title="INFO">
Accessing the **standby cluster** Web Console is required to perform the upgrade.

Before proceeding, verify that the **ProductBase** resource of the standby cluster is correctly configured with the cluster VIP under `spec.alternativeURLs`.

If not, update the configuration as follows:

```yaml
apiVersion: product.alauda.io/v1alpha2
kind: ProductBase
metadata:
  name: base
spec:
  alternativeURLs:
    - https://<standby-cluster-vip>
```

</Directive>

On the **standby cluster**, follow the steps in the [Standard procedure](#standard) to complete the upgrade.

### Upgrade the primary cluster

After the standby cluster has been upgraded, proceed with the [Standard procedure](#standard) on the **primary cluster**.

### Reinstall the etcd sync plugin

Before reinstalling, verify that port `2379` is properly forwarded from both global cluster VIPs to their control plane nodes.

To reinstall:

1. Access the Web Console of the **standby global cluster** via its IP or VIP.
2. Switch to **Administrator** view.
3. Go to **Marketplace > Cluster Plugins**.
4. Select the `global` cluster.
5. Locate **Alauda Container Platform etcd Synchronizer**, click **Install**, and provide the required parameters.

To verify installation:

```bash
kubectl get po -n cpaas-system -l app=etcd-sync  # Ensure pod is 1/1 Running

kubectl logs -n cpaas-system $(kubectl get po -n cpaas-system -l app=etcd-sync --no-headers | awk '{print $1}' | head -1) | grep -i "Start Sync update"
# Wait until the logs contain "Start Sync update"

# Recreate the pod to trigger synchronization of resources with ownerReferences
kubectl delete po -n cpaas-system $(kubectl get po -n cpaas-system -l app=etcd-sync --no-headers | awk '{print $1}' | head -1)
```

### Check Synchronization Status

Run the following to verify the synchronization status:

```bash
curl "$(kubectl get svc -n cpaas-system etcd-sync-monitor -ojsonpath='{.spec.clusterIP}')/check"
```

**Explanation of output:**

- `"LOCAL ETCD missed keys:"` – Keys exist in the **primary cluster** but are missing in the standby. This often resolves after a pod restart.
- `"LOCAL ETCD surplus keys:"` – Keys exist in the **standby cluster** but not in the primary. Review these with your operations team before deletion.

</Steps>
