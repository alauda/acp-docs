---
weight: 10
i18n:
  title:
    en: Prerequisites
---

# Prerequisites

Before installing the global cluster, you need to prepare hardware, network, and OS that meet the requirements.

:::danger
The platform currently does not support direct installation of the global cluster in an existing Kubernetes environment. If your environment already has a Kubernetes cluster, please back up your data and clean the environment before installation.
:::

## Capacity Planning \{#scale}

Before installation, you must select an appropriate installation scenario based on your goals and actual needs. Different scenarios have significant differences in infrastructure resource configuration and architecture design requirements. The following are planning recommendations for three typical scenarios:

<Tabs>
  <Tab label="Function Verification">
  **Scope of Application**
  Suitable for platform function verification, demo , or technical feasibility testing. This scenario is only used to verify the core functions of the platform and does not carry production-level application traffic. The resource configuration is at the minimum level.

  **Resource Configuration Requirements**
  | Dimension | Specification Requirements |
  |-----------|----------------------------|
  | Number of Nodes | 1 (physical machine or virtual machine) |
  | CPU | ≥16 cores |
  | Memory | ≥32GB |

  **Architecture Description**
  - **All-in-one**: The cluster has only one node, and all control plane components and applications run on that node.
  - **Lightweight Load**: Can only load Demo applications with no more than 10 Pods.
  - **Non-Production Use**: Does not support horizontal scaling and does not meet application continuity and high availability requirements.
  </Tab>

  <Tab label="ISV Integration Delivery">
  **Scope of Application**
  For the standardized delivery needs of ISVs (Independent Software Vendors), the global cluster handles both platform management and direct running of ISV applications, without the need to create additional workload clusters.

  **Resource Configuration Requirements**

  <table>
    <tr>
      <th style={{ whiteSpace: 'nowrap' }}>Deployment Plan</th>
      <th style={{ whiteSpace: 'nowrap' }}>Node Type</th>
      <th style={{ whiteSpace: 'nowrap' }}>Quantity</th>
      <th style={{ whiteSpace: 'nowrap' }}>Minimum Specifications</th>
      <th style={{ whiteSpace: 'nowrap' }}>Recommended Specifications</th>
      <th style={{ whiteSpace: 'nowrap' }}>Remarks</th>
    </tr>
    <tr>
      <td rowSpan={2}><b>Minimum Configuration</b></td>
      <td>Control Plane Node</td>
      <td>3</td>
      <td>8 cores 16GB + ISV Requirements</td>
      <td>12 cores 24GB + ISV Requirements</td>
      <td>Recommended to reserve approximately 30% redundant resources</td>
    </tr>
    <tr>
      <td>Worker Node</td>
      <td>0</td>
      <td>—</td>
      <td>—</td>
      <td>ISV applications run shared on control plane nodes</td>
    </tr>
    <tr>
      <td rowSpan={2}><b>Recommended Configuration</b></td>
      <td>Control Plane Node</td>
      <td>3</td>
      <td>8 cores 16GB</td>
      <td>12 cores 24GB</td>
      <td>Runs Kubernetes components and platform control plane</td>
    </tr>
    <tr>
      <td>Worker Node</td>
      <td>≥2</td>
      <td>According to ISV application load requirements</td>
      <td>—</td>
      <td>Independently runs ISV applications, recommended to implement HA deployment</td>
    </tr>
  </table>

  **Architecture Description**
  - **Production-Ready**: The global cluster runs both control plane components and ISV applications simultaneously.
  - **Platform and Application Isolation**: ISV applications are recommended to run on dedicated worker nodes to avoid resource contention with the control plane.
  - **Scaling**: For every 50 new application Pods, add 1 worker node according to ISV application resource requirements.

  </Tab>
  <Tab label="Multi-Cluster Management (Data Center Level)">
  **Scope of Application**
  Suitable for large enterprise data center environments that require unified management of multiple workload clusters across clouds and regions. As the number of workload clusters increases, the global cluster must dynamically scale worker nodes and resource configurations to ensure high availability and high performance.

  **Core Features**
  - **Hybrid Control**: Supports unified management of cross-cloud and cross-region workload clusters.
  - **Elastic Scaling**: Global cluster resources scale linearly with the number of accessed workload clusters.
  - **Physical Isolation**: It is recommended that the control plane and compute plane be physically isolated to ensure system stability.

  **Baseline Resource Configuration**
  <table>
    <thead>
      <tr>
        <th style={{ whiteSpace: 'nowrap' }}>Node Type</th>
        <th style={{ whiteSpace: 'nowrap' }}>Quantity</th>
        <th style={{ whiteSpace: 'nowrap' }}>Minimum Specifications</th>
        <th style={{ whiteSpace: 'nowrap' }}>Recommended Specifications</th>
        <th style={{ whiteSpace: 'nowrap' }}>Remarks</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Control Plane Node</td>
        <td>3 or 5</td>
        <td>8 cores 16GB</td>
        <td>16 cores 32GB</td>
        <td>Enable anti-affinity policy</td>
      </tr>
      <tr>
        <td>Worker Node</td>
        <td>≥2</td>
        <td>12 cores 24GB</td>
        <td>24 cores 48GB</td>
        <td>Recommended to reserve 30% resource redundancy</td>
      </tr>
    </tbody>
  </table>

  **Dynamic Scaling Rules**
  1. **Vertical Scaling**: Single nodes can be upgraded in gradients (e.g., 12 cores 24GB → 24 cores 48GB → 48 cores 96GB).
  2. **Horizontal Scaling**: For every 10 accessed workload clusters, it is recommended to expand compute resources by 20% (increase node count or upgrade node specifications).
  3. **Monitoring Triggered**: When CPU/memory utilization continuously exceeds 70%, initiate scaling measures.

  **Architecture Description**
  - The control plane consists of 3 nodes forming an ETCD cluster. It is recommended to enable TLS encryption and periodic snapshots.
  - Platform critical components are recommended to be deployed on independent worker nodes to avoid resource contention.
  - Disaster recovery recommendation: Deploy across multiple availability zones to ensure worker nodes are distributed in at least 2 physical fault domains.
  </Tab>
</Tabs>

<Directive type="tip" title="Overall Planning Guidelines">
1. **Resource Redundancy**: Production environments are recommended to reserve at least 30% resource margin to cope with sudden loads.
2. **Network Planning**: The global cluster should be deployed in an independent VPC or VLAN to ensure bandwidth ≥1Gbps.
3. **Storage Isolation**: ETCD storage is recommended to use NVMe SSD and be physically isolated from application storage.
</Directive>

## Machines

:::info
This section describes the minimum hardware requirements for building a highly available global cluster. If you have completed capacity planning, please prepare the corresponding resources according to [Capacity Planning](#scale), or scale up as needed after installation.
:::

### Basic Requirements

At least **3** physical machines or virtual machines must be provided as control plane nodes for the cluster. The minimum configuration for each node is as follows:

| **Category** | **Minimum Requirements** |
|----------|--------------|
| **CPU** | ≥ 8 cores, clock speed ≥ 2.5GHz<br />No over-provisioning; disable power saving mode |
| **Memory** | ≥ 16GB<br />No over-provisioning; recommended to use at least six-channel DDR4 |
| **Hard Drive** | Single device IOPS ≥ 2000<br />Throughput ≥ 200MB/s<br />Must use SSD |

### ARM Architecture Requirements

For ARM architectures (such as Kunpeng 920), it is recommended to increase the configuration to **2 times** that of the x86 minimum configuration, but not less than **1.5 times**.
For example: If x86 requires 8 cores 16GB, then ARM should reach at least 12 cores 24GB, and the recommended configuration is 16 cores 32GB.

## Supported OS and Kernels

:::warning
1. **Kernel Version Requirements:**
   These kernel versions have been officially released and validated by our platform tests. In your actual deployment, adherence to the **A.B.C major version numbers** is crucial, while subsequent minor versions can vary.
2. **Unsupported Environments:**
   If the OS, kernel version, or CPU architecture does not meet the requirements, please contact technical support.
:::

<Tabs>
  <Tab label="Red Hat Enterprise Linux (RHEL)">
  - RHEL 7.8: `3.10.0-1127.el7.x86_64`
  - RHEL 8.0 to 8.6: `4.18.0-80.el8.x86_64` to `4.18.0-372.9.1.el8.x86_64`
  <Directive type="info" title="INFO">
  RHEL 7.8 does not support **Calico Vxlan IPv6**.
  </Directive>
  </Tab>

  <Tab label="CentOS">
  - CentOS 7.6 to 7.9: `3.10.0-1127` to `3.11`
  <Directive type="info" title="INFO">
  CentOS does not support **Calico Vxlan IPv6**.
  </Directive>
  </Tab>

  <Tab label="Ubuntu">
  - Ubuntu 20.04 LTS: `5.4.0-124-generic`
  - Ubuntu 22.04 LTS: `5.15.0-56-generic`
  <Directive type="info" title="INFO">
  Ubuntu HWE (Hardware Enablement) versions are not supported.
  </Directive>
  </Tab>

  <Tab label="Kylin Linux Advanced Server">
  - Kylin V10: `4.19.90-11.ky10.x86_64`
  - Kylin V10 SP1: `4.19.90-23.8.v2101.ky10.x86_64`
  - Kylin V10 SP2: `4.19.90-24.4.v2101.ky10.x86_64`
  - Kylin V10 SP3: `4.19.90-52.22.v2207.ky10.x86_64`
  <Directive type="danger" title="DANGER">
  - Kylin V10, V10-SP1, and V10-SP2 have known kernel issues that may cause **NodePort network access failures**; it is recommended to upgrade to **Kylin V10-SP3**.
  - ARM architecture only supports `Kunpeng 920`. For other models, please contact technical support.
  </Directive>
  </Tab>
</Tabs>

## Network

Before installation, the following network resources must be pre-configured. If a hardware LoadBalancer cannot be provided, the installer supports configuring **haproxy + keepalived** as a software load balancer, but you need to understand:

- **Poorer Performance**: Software load balancing performance is lower than hardware LoadBalancer.
- **Higher Complexity**: If you are not familiar with keepalived, it may cause the global cluster to be unavailable, problem troubleshooting will take a long time, and seriously affect platform reliability.

### Network Resources \{#network-resources}

<table>
  <thead>
    <tr>
      <th style={{ whiteSpace: 'nowrap' }}>Resource</th>
      <th style={{ whiteSpace: 'nowrap' }}>Mandatory</th>
      <th style={{ whiteSpace: 'nowrap' }}>Quantity</th>
      <th style={{ whiteSpace: 'nowrap' }}>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>global VIP</td>
      <td>Mandatory</td>
      <td>1</td>
      <td>Used for nodes in the cluster to access kube-apiserver, configured in the load balancing device to ensure high availability. This IP can also be used as the access address for the platform Web UI. Workload clusters in the same network as the global cluster can also access the global cluster through this IP.</td>
    </tr>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>External IP</td>
      <td>Optional</td>
      <td>1</td>
      <td>When there are workload clusters that are not in the same network as the global cluster, such as a hybrid cloud scenario, it must be provided. Workload clusters in other networks access the global cluster through this IP. This IP needs to be configured in the load balancing device to ensure high availability. This IP can also be used as the access address for the platform Web UI.</td>
    </tr>
    <tr>
      <td>Domain Name</td>
      <td>Optional</td>
      <td>1</td>
      <td>If you need to access the global cluster or platform Web UI through a domain name, please provide it in advance and ensure that the domain name resolution is correct.</td>
    </tr>
    <tr>
      <td>Certificate</td>
      <td>Optional</td>
      <td>1</td>
      <td>It is recommended to use a trusted certificate to avoid browser security warnings; if not provided, the installer will generate a self-signed certificate, but there may be security risks when using HTTPS.</td>
    </tr>
  </tbody>
</table>

<Directive type="warning" title="Note">
A domain name must be provided in the following cases:
1. The global cluster needs to support IPv6 access.
2. A disaster recovery plan for the global cluster is planned.
</Directive>

<Directive type="tip" title="TIP">
If the platform needs to configure multiple access addresses (for example, addresses for internal and external networks), please prepare the corresponding IP addresses or domain names in advance according to the table above. You can configure them in the installation parameters later, or add them according to the product documentation after installation.
</Directive>

### Network Configuration

<table>
  <thead>
    <tr>
      <th style={{ whiteSpace: 'nowrap' }}>Type</th>
      <th style={{ whiteSpace: 'nowrap' }}>Requirement Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>Network Speed</td>
      <td>Speed of global cluster and workload cluster in the same network ≥1Gbps (recommended 10Gbps); cross-network speed ≥100Mbps (recommended 1Gbps). Insufficient speed will significantly reduce data query performance.</td>
    </tr>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>Network Latency</td>
      <td>Latency ≤2ms in the same network; latency ≤100ms (recommended ≤30ms) across networks.</td>
    </tr>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>Network Policy</td>
      <td>Please refer to [LoadBalancer Forwarding Rules](#port-forward) to ensure that the necessary ports are open; when using Calico CNI, ensure that the **IP-in-IP** protocol is enabled.</td>
    </tr>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>IP Address Range</td>
      <td>The global cluster nodes should avoid using the 172.16-32 network segment. If it has been used, please adjust the Docker configuration (add the bip parameter) to avoid conflicts.</td>
    </tr>
  </tbody>
</table>

### LoadBalancer Forwarding Rules \{#port-forward}

This rule is designed to ensure that the global cluster can receive traffic from the LoadBalancer normally. Please check the network policy according to the following table to ensure that the relevant ports are open.

<table>
  <thead>
    <tr>
      <th style={{ whiteSpace: 'nowrap' }}>Source IP</th>
      <th style={{ whiteSpace: 'nowrap' }}>Protocol</th>
      <th style={{ whiteSpace: 'nowrap' }}>Destination IP</th>
      <th style={{ whiteSpace: 'nowrap' }}>Destination Port</th>
      <th style={{ whiteSpace: 'nowrap' }}>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>global VIP, External IP</td>
      <td style={{ whiteSpace: 'nowrap' }}>TCP</td>
      <td style={{ whiteSpace: 'nowrap' }}>All control plane node IPs</td>
      <td style={{ whiteSpace: 'nowrap' }}>443</td>
      <td>
        Provides access services for the platform Web UI, image repository, and Kubernetes API Server through the HTTPS protocol. The default port is `443`. If you need to use a custom HTTPS port, please do the following:<br />
        - Replace the destination port in the port forwarding rule with your custom port number.<br />
        - Later, in the Web UI installation parameters, fill in your custom port number.
      </td>
    </tr>
    <tr>
      <td style={{ whiteSpace: 'nowrap' }}>global VIP, External IP</td>
      <td style={{ whiteSpace: 'nowrap' }}>TCP</td>
      <td style={{ whiteSpace: 'nowrap' }}>All control plane node IPs</td>
      <td style={{ whiteSpace: 'nowrap' }}>11443</td>
      <td>
        Provides access services for the image repository to nodes in the cluster through this port.<br />
        <b>Note:</b> If you plan to use an external image repository to install the global cluster, rather than the default internal image repository, you do not need to configure this port.<br />
      </td>
    </tr>
  </tbody>
</table>

<Directive type="info" title="INFO">
- It is recommended to configure health checks on the LoadBalancer to monitor the port status.
- If you plan to implement a disaster recovery plan for the global cluster, you need to open port `2379` for all control plane nodes for ETCD data synchronization between the primary and disaster recovery clusters.
- The platform only supports HTTPS by default. If HTTP support is required, you need to open the HTTP port for all control plane nodes.
</Directive>
