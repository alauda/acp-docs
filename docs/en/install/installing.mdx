---
weight: 30
---

# Installing

This section describes the specific steps for installing the `global` cluster.

Before starting the installation, please ensure that you have completed the prerequisite checks, installation package download and verification, node preprocessing, and other preparatory work.

## Process

<Steps>
### Upload and Extract Installation Package

Upload the Core Package installation package to any machine of the `global` cluster control plane nodes, and extract it according to the following command:

```shell
# Assume that the /root/cpaas-install folder already exists on the machine
tar -xvf {Path to Core Package File}/{Core Package File Name} -C /root/cpaas-install
cd /root/cpaas-install/installer || exit 1
```

<Directive type="info" title="INFO">
- This machine will become the first control plane node after the `global` cluster installation is complete.
- After the Core Package is extracted, at least **100GB** of disk space is required. Please ensure sufficient storage resources.
- If you have already downloaded extensions, complete the ACP Core installation first, and then follow [Extend](/extend) to upload and install them.
</Directive>

### Start the Installer \{#start_installer}

Execute the following installation script to start the installer. After the installer starts successfully, the command line terminal will output the web console access address.

After waiting for about 5 minutes, you can use a browser on your PC to access the web console provided by the installer.

```bash
bash setup.sh
```

<Directive type="warning" title="WARNING">
Ensure that the IP address and port 8080 of the node where the installer is located can be accessed normally, so that the web console provided by the installer can be accessed smoothly after the installer starts successfully.
</Directive>

#### Network Mode and IP Family

```bash
bash setup.sh --network-mode calico
```

The `--network-mode` parameter affects the CNI of the `global` cluster created by the installer. If this parameter is not specified, the CNI of the `global` cluster will default to Kube-OVN. If you want Calico as the CNI, you must explicitly specify `--network-mode calico`.

```bash
bash setup.sh --ip-family ipv6
```

If you plan to create a `global` cluster with Single-stack Network IPv6, you must explicitly specify `--ip-family ipv6` when starting the installer. Without this parameter, the `global` cluster created by the installer will support Single-stack Network IPv4 and Dual-stack Network by default.

### Parameter Configuration

After completing the installation parameter configuration according to the page guide, confirm the installation.

[Parameter Description](#parameters) provides detailed descriptions of key parameters. Please read carefully and configure according to actual needs.

### Verify Successful Installation

After the installation is complete, the platform access URL will be displayed on the page. Click the **Access** button to open the platform Web UI and verify whether the platform is accessible.

Next, run the following commands on the installation node to verify the installation status:

```shell
# Check if there are any failed Charts
kubectl get apprelease --all-namespaces
# Check if there are any Pods not in Running or Completed status
kubectl get pod --all-namespaces | awk '{if ($4 != "Running" && $4 != "Completed")print}' | awk -F'[/ ]+' '{if ($3 != $4)print}'
```

### Install Product Docs Plugin

:::info
The **Alauda Container Platform Product Docs** plugin provides access to product documentation within the platform. All help links throughout the platform will direct users to this documentation. If this plugin is not installed, clicking help links in the platform will result in 404 access errors.
:::

1. Navigate to **Administrator**.

2. In the left sidebar, click **Marketplace** > **Cluster Plugins** and select the `global` cluster.

3. Locate the **Alauda Container Platform Product Docs** plugin and click **Install**.

</Steps>

## Parameter Description \{#parameters}

<table>
  <tr>
    <th style={{ whiteSpace: 'nowrap' }}>Parameter</th>
    <th style={{ whiteSpace: 'nowrap' }}>Description</th>
  </tr>
  <tr>
    <td><b>Kubernetes Version</b></td>
    <td>
      All optional versions are rigorously tested for stability and compatibility.<br />
      <b>Recommendation:</b> Choose the latest version for optimal features and support.
    </td>
  </tr>
  <tr>
    <td><b>Cluster Network Protocol</b></td>
    <td>
      Supports three modes: IPv4 single stack, IPv6 single stack, IPv4/IPv6 dual stack.<br />
      <b>Note:</b> If you select dual stack mode, ensure all nodes have correctly configured IPv6 addresses; the network protocol cannot be changed after setting.
    </td>
  </tr>
  <tr>
    <td><b>Cluster Address</b></td>
    <td>
      <ul>
        Enter the pre-prepared domain name. If no domain name is available, enter the pre-prepared <code>`global` VIP</code>.<br />
        <code>Self-Built VIP</code> is disabled by default, only enable it if you have not provided a LoadBalancer. After enabling, the installer will automatically deploy <code>keepalived</code> to provide software load balancing support.<br />
        <b>Note:</b> The following conditions must be met when using <code>Self-Built VIP</code>,<br />
        - A usable VRID is available;<br />
        - The host network supports the VRRP protocol;<br />
        - All control plane nodes and the VIP must be on the same subnet.<br />
      </ul>
      <ul>
        <b>Tip:</b> For single-node deployments in feature experience scenarios, you can directly enter the node IP. There is no need to enable <code>Self-Built VIP</code> or prepare network resources such as <code>`global` VIP</code>.
      </ul>
    </td>
  </tr>
  <tr>
    <td><b>Platform Access Address</b></td>
    <td>
      <ul>
        If you do not need to distinguish between <b>Cluster Address</b> and <b>Platform Access Address</b>, enter the same address as the <b>Cluster Address</b>.<br />
        If you need to distinguish, for example, if the `global` cluster is only for internal network access and the platform needs to provide external network access, enter the pre-prepared domain name or <code>External IP</code>.<br />
        The platform uses HTTPS access by default and does not enable HTTP. If you need to enable HTTP access, enable it in <b>Advanced Settings</b> (not recommended).<br />
        <b>Note:</b> A domain name must be entered in the following cases,<br />
        - A disaster recovery plan for the `global` cluster is planned;<br />
        - The platform needs to support IPv6 access.<br />
      </ul>
      <ul>
        <b>Tip:</b> If you need to configure more platform access addresses, you can add them in <b>Other Settings > Other Platform Access Addresses</b> in the next step. Or, after installation, add them in platform management according to the user manual.<br />
      </ul>
    </td>
  </tr>
  <tr>
    <td><b>Certificate</b></td>
    <td>
      The platform provides self-signed certificates to support HTTPS access by default.<br />
      If you need to use a custom certificate, you can upload an existing certificate.
    </td>
  </tr>
  <tr>
    <td><b>Image Repository</b></td>
    <td>
      The <code>Platform Deployment</code> image repository is used by default, which contains images of all components.<br />
      If you need to use an <code>External</code> image repository, please contact technical support to obtain the image synchronization plan before configuring.
    </td>
  </tr>
  <tr>
    <td><b>Container Network</b></td>
    <td>
      The default subnet and Service network segment of the cluster cannot overlap.<br />
      When using the Kube-OVN Overlay network, ensure that the container network and the host network are not in the same network segment, otherwise it may cause network exceptions.
    </td>
  </tr>
  <tr>
    <td><b>Node Name</b></td>
    <td>
      If you select <code>Host Name as Node Name</code>, ensure that the host names of all nodes are unique.
    </td>
  </tr>
  <tr>
    <td><b>`global` Cluster Platform Node Isolation</b></td>
    <td>
      Enable only when you plan to run application workloads in the `global` cluster.<br />
      <b>After enabling:</b><br />
      - Nodes can be set to <code>Platform Exclusive</code>, i.e., only run platform components, ensuring platform and application workloads are isolated;<br />
      - Workloads of the DaemonSet type are excluded.
    </td>
  </tr>
  <tr>
    <td><b>Add Node</b></td>
    <td>
      <b>Control Plane Node:</b>
      <ul>
        - Supports adding 1 or 3 control plane nodes (3 for high availability configuration);<br />
        - If <code>Platform Exclusive</code> is enabled, <code>Deployable Applications</code> is forced to be disabled, and control plane nodes only run platform components;<br />
        - If <code>Platform Exclusive</code> is disabled, you can choose whether to enable <code>Deployable Applications</code>, allowing control plane nodes to run application workloads.<br />
      </ul>

      <b>Worker Node:</b>
      <ul>
        - If <code>Platform Exclusive</code> is enabled, <code>Deployable Applications</code> is forced to be disabled;<br />
        - If <code>Platform Exclusive</code> is disabled, <code>Deployable Applications</code> is forced to be enabled.<br />
      </ul>

      <p>When using Kube-OVN, you can specify the node network card by entering the gateway name.</p>
      <p>If the node availability check fails, please adjust it according to the page prompt and add it again.</p>
    </td>
  </tr>
</table>

## Installer Cleanup

Normally, the installer will be automatically deleted after installation. If the installer is not automatically deleted after 30 minutes of installation, please execute the following command on the node where the installer is located to force delete the installer container:

```bash
docker rm -f minialauda-control-plane
```

## Additional Resources

- [Upload and Install Extensions](../extend)
