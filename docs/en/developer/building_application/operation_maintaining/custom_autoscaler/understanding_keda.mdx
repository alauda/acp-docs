---
weight: 17
i18n:
  title:
    en: Understanding Custom Autoscaler
    zh: 了解自定义自动伸缩器
---


# Understanding Custom Autoscaler

## Understanding KEDA
KEDA is a single-purpose and lightweight component that can be added into any Kubernetes cluster. KEDA works alongside standard Kubernetes components like the [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) and can extend functionality without overwriting or duplication. With KEDA, you can explicitly map the apps you want to use event-driven scale, with other apps continuing to function. This makes KEDA a flexible and safe option to run alongside any number of any other Kubernetes applications or frameworks.

See the official documentation for more details: [Keda Documentation](https://keda.sh/docs/2.17/)

### How KEDA works
KEDA monitors external event sources and adjusts your app’s resources based on the demand. Its main components work together to make this possible:

1. **KEDA Operator** keeps track of event sources and changes the number of app instances up or down, depending on the demand.
2. **Metrics Server** provides external metrics to Kubernetes’ HPA so it can make scaling decisions.
3. **Scalers** connect to event sources like message queues or databases, pulling data on current usage or load.
4. **Custom Resource Definitions (CRDs)**define how your apps should scale based on triggers like queue length or API request rates.

In simple terms, KEDA listens to what’s happening outside Kubernetes, fetches the data it needs, and scales your apps accordingly. It’s efficient and integrates well with Kubernetes to handle scaling dynamically.

### KEDA Custom Resources (CRDs) 

KEDA uses **Custom Resource Definitions (CRDs)** to manage scaling behavior:

- **ScaledObject**: Links your app (like a Deployment or StatefulSet) to an external event source, defining how scaling works.
- **ScaledJob**: Handles batch processing tasks by scaling Jobs based on external metrics.
- **TriggerAuthentication**: Provides secure ways to access event sources, supporting methods like environment variables or cloud-specific credentials.

These CRDs give you control over scaling while keeping your apps secure and responsive to demand.

**ScaledObject Example**:

The following example targets CPU utilization of entire pod. If the pod has multiple containers, it will be sum of all the containers in it.
```yaml
kind: ScaledObject
metadata:
  name: cpu-scaledobject
  namespace: default
spec:
  scaleTargetRef:
    name: my-deployment
  triggers:
  - type: cpu
    metricType: Utilization # Allowed types are 'Utilization' or 'AverageValue'
    metadata:
      value: "50"
```
