---
weight: 10
sourceSHA: 2f8179063844d4c60e090d316f2f2bfc060c8d8eb07230209df4f39ab4c5c25a
---

# Creating a Ceph Object Storage Class

Ceph object storage provides a storage access method based on COSI (Container Object Storage Interface) for the platform, offering massive and flexible Ceph object storage services suitable for big data, backup and recovery, machine learning, and other scenarios.

## Terminology

| Term       | Full Name                                 | Description                                                                                                                  |
| ----------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **COSI**    | Container Object Storage Interface      | The Container Object Storage Interface aims to serve as a common abstraction layer across multiple object storage vendors, dynamically providing object storage services for workloads. Additionally, third-party storage vendors can write plugins using COSI, which can be exposed as a new storage system in Kubernetes without modifying the core Kubernetes code. |

## Prerequisites

Prepare a Ceph distributed storage service in advance using one of the following methods:

- Use the internal Ceph distributed storage service of the platform. For details, please refer to [Creating a Storage Service](/storage/storagesystem_ceph/functions/create_service_stand.mdx).

- Use an external Ceph distributed storage service.

## Steps

### Deploy the Container Object Storage Interface Plugin

In order to utilize the object storage functionality, you must first deploy the Container Object Storage Interface plugin.

1. Go to **Platform Management**.

2. In the left navigation bar, click **Cluster Management** > **Clusters**.

3. Click on the ***Name of the Cluster*** where the component will be deployed.

4. Under the **Plugins** tab, click the ⋮ to the right of **Container Object Storage Interface** > **Deploy**.

5. Wait until the deployment status indicates **Deployment Successful** to complete the deployment.

### Deploy the Volume Plugin

1. Go to **Platform Management**.

2. In the left navigation bar, click **Storage Management** > **Object Storage Classes**.

3. Click **Create Object Storage Class**.

4. Click **Deploy** on the **Ceph Object Storage** card to redirect to the corresponding cluster **Plugins** deployment page.

5. Click the ⋮ to the right of **Ceph Object Storage Volume Plugin** > **Deploy**.

6. Fill in the `Access Address` from [Get Information](#getaksk) into the **Access Address** field.

7. Choose **Key**.

   - **Integrating External Ceph Object Storage**: Click **Create**, and enter the values of `accesskey` and `secretkey` from [Integrating External Ceph Object Storage](#out) into the corresponding **ACCESSKEY** and **SECRETKEY** **Values** fields. Then, click **Create** and choose to use this secret dictionary.

   - **Integrating Internal Ceph Object Storage**: Create a secret dictionary named **ceph-cosi-ops-secret** as per the method in [Integrating Internal Ceph Object Storage](#in), and choose to use this secret dictionary.

8. Click **Deploy**.

### Create a Storage Class

1. Go to **Platform Management**.

2. In the left navigation bar, click **Storage Management** > **Object Storage Classes**.

   **Note**: The following content is provided as an example in form format; you can also choose to complete the operation using YAML.

3. Click **Create Object Storage Class**.

4. Select **Ceph Object Storage** and click **Next**.

5. Refer to the following notes to configure certain parameters.

   | Parameter       | Description                                                       |
   | -------------- | --------------------------------------------------------------- |
   | **Recycling Policy** | The recycling policy of the bucket. The default is **Delete**, which means that when the bucket is deleted, the associated storage bucket will also be deleted.          |
   | **Assigned Project** | Please assign the project that can use this type of storage.<br />If there are currently no projects needing this type of storage, you may choose not to assign a project for now, and update the project later. |

6. Click **Create**.

## Related Operations

### Get Information\{#getaksk}

Due to differences in the storage integration locations, the methods for obtaining the **Access Address**, **ACCESSKEY**, and **SECRETKEY** also vary. Please choose one of the following methods based on your configuration.

- **Integrating External Ceph Object Storage**: \{#out}

  - **Access Address**: Contact relevant personnel to obtain the Ceph object storage service access address, for example: `http://10.7.122.127:7480`.

  - **ACCESSKEY** and **SECRETKEY**: Contact relevant personnel to obtain the `accesskey` and `secretkey` with administrator privileges.

- **Integrating Internal Ceph Object Storage**: \{#in}

  - **Access Address**: Fill in the **Internal Address** obtained in the [View Object Storage Pool Address](/storage/storagesystem_ceph/functions/pool_management.mdx#storepool) section, for example: `http://10.7.122.127:7480`.

  - **Keys**: Execute the following six commands in the CLI tool of the cluster where the **Deploy Ceph Object Storage Volume Plugin** is located to create a secret dictionary named **ceph-cosi-ops-secret** and save the related key information.

    ```
    kubectl patch csv -n rook-ceph $(kubectl get csv -n rook-ceph | grep -vi name | awk '{print $1}') --type=json -p='[{"op": "replace", "path": "/spec/install/spec/deployments/1/spec/replicas", "value": 1}]'
    ```

    ```
    kubectl exec -ti -n rook-ceph $(kubectl get pod -n rook-ceph | grep tools | awk '{print $1}') -- radosgw-admin user create --uid cosi-ops --display-name "cosi ops user" --caps "buckets=*;users=*;usage=read;metadata=read;zone=read"
    ```

    ```
    export ACCESS_KEY=$(kubectl exec -ti -n rook-ceph $(kubectl get pod -n rook-ceph | grep tools | awk '{print $1}') -- radosgw-admin user info --uid=cosi-ops | grep access_key | awk -F'"' '{print $4}')
    ```

    ```
    export SECRET_KEY=$(kubectl exec -ti -n rook-ceph $(kubectl get pod -n rook-ceph | grep tools | awk '{print $1}') -- radosgw-admin user info --uid=cosi-ops | grep secret_key | awk -F'"' '{print $4}')
    ```

    ```
    kubectl create secret generic -n cpaas-system ceph-cosi-ops-secret --from-literal=ACCESSKEY="$ACCESS_KEY" --from-literal=SECRETKEY="$SECRET_KEY"
    ```

    ```
    kubectl -n cpaas-system label secret ceph-cosi-ops-secret cosi-driver-ops=true
    ```
