---
weight: 16
sourceSHA: 32d3a23cf70a7ab882b2fc166a0bcd21b04b193cf13b8b0534c2e057cfdfb21b
---

# Create Standard Type Cluster

A standard-type cluster is the most typical deployment method for Ceph storage. It distributes data replicas across hard drives on different hosts, ensuring that if a single host fails, the data copies on other hosts can still maintain service availability.

## Prerequisites

### Prepare Package
- **Download** the **Alauda Container Platform Storage Essentials** installation package corresponding to your platform architecture.

- **Upload** the **Alauda Container Platform Storage Essentials** installation package using the Upload Packages mechanism.

- **Download** the **Alauda Build of Rook-Ceph** installation package corresponding to your platform architecture.

- **Upload** the **Alauda Build of Rook-Ceph** installation package using the Upload Packages mechanism.

### Prepare Infrastructure
- At least 3 nodes are required in the storage cluster.

- Each node must have at least 1 blank hard disk or 1 unformatted hard disk partition available.

- The available hard disk capacity is recommended to be greater than 50 G.

- If you are using an attached Kubernetes cluster with Containerd as the runtime component, please ensure that the `LimitNOFILE` parameter value in the `/etc/systemd/system/containerd.service` file is configured to `1048576` on all nodes of the cluster, to ensure successful deployment of distributed storage. For configuration instructions, please refer to [Modifying Containerd Configuration Information]().

  **Note**: When upgrading from versions earlier than v3.10.2 to the current version, if you need to deploy Ceph distributed storage on your custom Kubernetes cluster with Containerd as the runtime component, you must also set the `LimitNOFILE` parameter value in the `/etc/systemd/system/containerd.service` file to `1048576` on all nodes of the cluster.

## Precautions

**Creating Storage Service** and **Accessing Storage Service** only support selecting one method.

## Procedure

<Steps>

### Deploy Alauda Container Platform Storage Essentials
1. Login, go to the **Platform Management** page.

2. Click **Marketplace** > **OperatorHub** to enter the **OperatorHub** page.

3. Find the **Alauda Container Platform Storage Essentials**, click **Install**, and navigate to the **Install Alauda Container Platform Storage Essentials** page.

    Configuration Parameters:
    | **Parameter**   | **Recommended Configuration**                          |
    | :-------------- | :----------------------------------------------------- |
    | **Channel**     | The default channel is `stable`.                        |
    | **Installation Mode**  | `Cluster`: All namespaces in the cluster share a single Operator instance for creation and management, resulting in lower resource usage. |
    | **Installation Place** | Select `Recommended`, Namespace only support **acp-storage**.|
    | **Upgrade Strategy**   | `Manual`: When there is a new version in the Operator Hub, manual confirmation is required to upgrade the Operator to the latest version. |

### Deploy Operator

1. Navigate to **Platform Management**.

2. In the left sidebar, click **Storage Management** > **Distributed Storage**.

3. Click **Configure Now**.

4. In the **Deploy Operator** wizard page, click the **Deploy Operator** button at the bottom right.

   - When the page automatically advances to the next step, it indicates that the Operator has been deployed successfully.

   - If the deployment fails, please refer to the prompt on the interface **Clean Up Deployed Information and Retry**, and redeploy the Operator; if you wish to return to the distributed storage selection page, click **Application Store**, first uninstall the resources in the already deployed **rook-operator**, and then uninstall **rook-operator**.

### Create Cluster

1. In the **Create Cluster** wizard page, configure the relevant parameters and click the **Create Cluster** button at the bottom right.

   | Parameter         | Explanation                                                                                                                                                                                             |
   | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
   | **Cluster Type**     | Select **Standard**.                                                                                                                                                                                        |
   | **Device Class Type**    | Device classes are groupings of hard disks; you can customize device classes according to your storage needs, allocating different storage content to disks of varying performance. <ul><li>**Default Device Class**: The platform will automatically categorize the types of hard disks in the cluster nodes. For instance, creating device classes named `hdd`, `ssd`, `nvme`.</li><li>**Custom Device Class**: Customize the name of the device class for specific combinations of disks in the node; adding multiple device classes is supported. The same hard disk can only belong to one device class.</li></ul> |
   | **Device Class - Name**   | The name of the device class. When selecting **Custom Device Class**, the device class cannot use the following names: `hdd`, `ssd`, `nvme`.                                                                                                                                                         |
   | **Device Class - Storage Devices** | Choose **Blank Hard Disk** or **Unformatted Hard Disk Partition** on the nodes. <ul><li>When the "Open All Blank Devices" switch is on: All blank devices under the node will be added to the device class;</li><li>When the "Open All Blank Devices" switch is off: Manually input the names of the blank devices under the node, such as `sda`.</li></ul>                                                                             |
   | **Snapshot**         | When enabled, it supports creating PVC snapshots and using snapshots to configure new PVCs for quick backup and recovery of business data. <br />If you did not enable snapshots when creating storage, you can still enable them as needed from the **Operations** section on the storage cluster details page. <br />**Note**: Please ensure that you have [deployed volume snapshot plugins](/configure/storage/functions/snapshot_con.mdx) for the current cluster before using.                      |
   | **Monitoring Alarm**   | When enabled, it will provide out-of-the-box monitoring metric collection and alerting capabilities, see [Monitoring and Alarming](../functions/monitor.mdx). <br />**Note**: If not enabled at this time, you will need to find alternative solutions for storage monitoring and alarms. For example, manually configuring monitoring dashboards and alert strategies in the operation and maintenance center.                                   |

2. Click **Advanced Configuration** for advanced component configuration.

   | Parameter        | Explanation                                                                                                                                                                                      |
   | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
   | **Network Configuration**   | <ul><li>**Host Network**: The storage cluster will use the host network, and you should fill in the relevant network optimization parameters in the optimization parameters column, such as configuring the `public` and `cluster` subnets. If left blank, the default host subnet will be used.<br></br>**Note**: Using the host network may expose security risks due to unencrypted (plaintext) transmission of data through host ports. Please contact the platform support team to obtain the encrypted transmission solution.</li><li>**Container Network**: The storage cluster will use container networking; you can create subnets in network management and assign them to the `rook-ceph` namespace. If left blank, the default subnet will be used.<br></br>**Note**:<br></br>IPv6 not supported.<br></br>When using the container network, storage is only accessible within the cluster. <br></br>Failures or restarts of the Ceph CSI Pod may result in service interruptions.</li></ul> |
   | **Optimization Parameters**   | Supports filling parameters in Ceph configuration file format; the system will overwrite the default parameters based on the provided content. <br />**Note**: After first filling in or modifying initialization parameters, please click on the initialization parameters; successful initialization is required before a cluster can be created.                                                                                                  |
   | **Component Fixed-point Deployment** | You can deploy components to specified nodes; at least three nodes are required to ensure minimum availability. The components eligible for fixed-point deployment configuration include MON, MGR, MDS, RGW.                                                                                                                 |

   - When the page automatically advances to the next step, it indicates that the Ceph cluster has been deployed successfully.

   - If the creation fails, you may click to clean up **Created Information or Retry** to automatically clean up the resources and recreate the cluster, or manually clean up resources according to the documentation [Distributed Storage Service Resource Cleanup](../how_to/clean_ceph.mdx).

### Create Storage Pool

1. In the **Create Storage Pool** wizard page, configure the relevant parameters and click the **Create Storage Pool** button at the bottom right.

   | Parameter         | Explanation                                                                                                                                    |
   | ----------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
   | **Storage Type**     | <ul><li> File Storage: Provides secure, reliable, and scalable shared file storage services. Suitable for file sharing, data backup, etc.</li><li> Block Storage: Provides high IOPS and low-latency storage services. Suitable for databases, virtualization, etc.</li><li> Object Storage: Provides standard S3 interface storage services, suitable for big data, backup archiving, cloud storage, etc. </li></ul>|
   | **Replica Count**    | The larger the number of replicas, the higher the redundancy and data security; however, the utilization rate of storage will decrease. It is usually set to 3 to meet most needs.                                                                                     |
   | **Device Class** | Uniformly classify types for the same type of device or disks of the same business logic, selecting from the device classes added in the previous step. <ul><li>When selecting a device class, data will be stored in the chosen device class.</li><li>If no device class is selected, data will be randomly stored across all devices in the storage pool.</li></ul>            |

   If it is object storage, you also need to configure the following parameters:

   | Parameter         | Explanation                                      |
   | ----------------- | -------------------------------------------- |
   | **Region**           | Specify the region where the storage pool is located.                            |
   | **Gateway Type**     | Default is S3 and cannot be modified.                          |
   | **Internal Port**    | Specify the port for internal access in the cluster.                           |
   | **External Access**   | Enabling/disabling external access will create/destroy Nodeport type Service. |
   | **Instance Count**    | The number of resource instances for object storage.                          |

   - When the page automatically advances to the next step, it indicates that the storage pool has been deployed successfully.

   - If the deployment fails, please refer to the interface prompts to check the core components, and then click **Clean Up Created Information and Retry** to recreate the storage pool.

2. Click **Create Storage Pool**. In the **Details** tab, you can view information about the created storage pool.

</Steps>

## Related Operations

### Create Stretch Type Cluster

For details, please refer to [Create Stretch Type Cluster](./create_service_extend.mdx).

### Cleanup Distributed Storage

For details, please refer to [Cleanup Distributed Storage](../how_to/clean_ceph.mdx).