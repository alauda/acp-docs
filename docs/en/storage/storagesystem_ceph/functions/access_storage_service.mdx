---
weight: 30
sourceSHA: 2219f163a1eab2d290508c4b182c90258bec6da5885662ffb20b60bef63e155f
---

# Accessing Storage Services

Accessing storage services supports two methods of integration: first, integrating distributed storage resources from other business clusters within the platform to ensure storage and business isolation for easier management and maintenance; second, connecting external Ceph storage resources for distributed storage use.

## Prerequisites

### Prepare Storage

Choose one of the following:

- Distributed storage has been deployed in other business clusters, and a storage pool has been created. Please record the name of the storage pool for later integration use.

- External Ceph storage outside the platform (version â‰¥ 14.2.3) has been created with a storage pool. Please record the name of the storage pool for later integration use.

### Open Ports

| Destination IP  | Destination Port | Source IP                  | Source Port |
| :-------------- | :--------------- | :------------------------- | :----------:|
| IP of Ceph node | 3300, 6789, 6800-7300, 7480      | IP of all nodes in business cluster | any         |

### Obtain Authentication Information (External Ceph)

If the prepared storage is external Ceph storage, authentication information must be obtained using the following commands.

| Parameter                 | Method of Acquisition                                                                                               |
| :------------------------ | :------------------------------------------------------------------------------------------------------------------ |
| **FSID**                  | `ceph fsid`                                                                                                        |
| **MON Component Information** | `ceph mon dump`<br />Must be in \{name= IP\} format, e.g. *a=192.168.100.100:6789*.                                |
| **Admin Key**              | `ceph auth get-key client.admin`                                                                                  |
| **Storage Pool**           | <ul><li>File storage: Use `ceph fs ls` command to get the `name` value.</li><li>Block storage: `ceph osd dump` \| `grep "application rbd"` \| `awk '{print $3}'` </li></ul>|
| ***Data Storage Pool***    | (only needed for file storage) Use `ceph fs ls` command to get the `data pools` value.                                      |

## Procedure

**Note**: The following steps take **accessing external Ceph storage** as an example, the operations for accessing distributed storage are similar.

<Steps>

1. In the left navigation bar, click **Storage Management** > **Distributed Storage**.

2. Click **Access Storage**.

3. On the **Access Configuration** wizard page, select **External Ceph**.

   | Parameter        | Description |
   | :--------------- | :---------- |
   | **Snapshot**     | When enabled, supports creating PVC snapshots and using snapshots to configure new PVCs for quick backup and restoration of business data. <br />If snapshots were not enabled during storage access, you can still enable them later in the **Operations** section of the storage cluster details page as needed. <br />**Note**: Please ensure that you have [deployed the volume snapshot plugin](/storage/pv_snapshot/functions/snapshot_con.mdx) for the current cluster before use. |
   | **Network Configuration** | <ul><li>**Host Network**: Computing components in this cluster will access the **storage cluster** using the **host network**.</li><li>**Container Network**: Computing components in this cluster will access the **storage cluster** using the **container network**. You can create a subnet in network management and assign the subnet to the `rook-ceph` namespace. If left empty, the default subnet will be used.</li></ul> |
   | **Other Parameters** | Please fill in the authentication parameters for the external Ceph obtained in the prerequisites.                                            |

4. On the **Create Storage Class** wizard page, complete the configuration and click **Access**.

   | Parameter         | Description |
   | :---------------- | :---------- |
   | **Type**          | Based on the type of storage pool created above, the default corresponding storage class will be: <br /><ul><li>File Storage: CephFS File Storage</li><li> Block Storage: CephRBD Block Storage </li></ul>|
   | **Reclaim Policy** | Reclaim policy for persistent volumes. <br /><ul><li> Delete: When the persistent volume claim is deleted, the bound persistent volume will also be deleted.</li><li> Retain: Even if the persistent volume claim is deleted, the bound persistent volume will still be retained.</li></ul>|
   | **Project Allocation** | Projects that can use this type of storage. <br />If there are currently no projects requiring this type of storage, you may choose not to allocate projects for now and update them later.|

5. Wait approximately 1-5 minutes for the successful integration.

</Steps>

## Follow-up Actions

- Create Storage Classes: [CephFS File Storage](/storage/storageclass_file/functions/cephfs_storageclass.mdx), [CephRBD Block Storage](/storage/storageclass_block/functions/cephrbd_storageclass.mdx)

- Developers using the above storage classes to create persistent volume claims can extend usage with volume snapshots and scaling features.

**Note**: If you need to maintain storage pools, storage device configurations, etc., for external storage, operations must be performed in the management platform of the storage cluster.
