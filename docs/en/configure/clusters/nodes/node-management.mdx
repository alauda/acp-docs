---
title: "Manage Nodes"
description: "Supports updating node labels and adding or removing custom node labels."
weight: 20
---

# Manage Nodes


## Update Node Labels


[Labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) are key-value pairs attached to nodes that can define node attributes. After setting labels for nodes, you can easily filter or select nodes by labels. For example: directing Pods to be scheduled to specific nodes.

Supports updating node labels for nodes in normal state, adding or removing custom node labels.

### Procedure

1. In the left navigation bar, click **Cluster Management** > **Clusters**.

2. Click the ***cluster name*** where the node with labels to be updated is located.

3. Under the **Nodes** tab, click **Update Node Labels** on the right side of the node with labels to be updated.

4. Add, modify, or delete node labels.

5. Click **OK**.  
   After successfully updating node labels, the number of node labels changes. You can view all label information of the node in the **Node Labels** item in the **Node** information bar.
   
   
## Stop/Resume Node Scheduling

By setting the scheduling state of nodes, you can control whether newly created Pods in the cluster are allowed to be scheduled to the node.

* **Stop Scheduling**: Newly created Pods are not allowed to be scheduled to the node, but existing Pods running on the node are not affected.

* **Resume Scheduling**: Newly created Pods are allowed to be scheduled to the node.

### Procedure

1. In the left navigation bar, click **Clusters** > **Clusters**.

2. Click the ***cluster name*** where the node to stop/resume scheduling is located.

3. Under the **Nodes** tab, click  **Stop Scheduling/Resume Scheduling** on the right side of the node to set scheduling state.

4. Click **OK**.
     

## Evict Pods
   
Evict all Pods except those managed by DaemonSet (daemon set) from nodes in normal state to other nodes in the cluster, and set the node to unschedulable state.

**Note**: Data from locally stored Pods will be lost after eviction. Please proceed with caution.

### Procedure

1. In the left navigation bar, click **Cluster Management** > **Clusters**.

2. Click the ***cluster name*** where the node to evict Pods is located.

3. Under the **Nodes** tab, click the ***node name*** to evict Pods.
    
4. In the upper right corner, click **Actions** > **Evict Pods**.

5. Review the information of Pods to be evicted, then click **Evict**.


## Set Taints


Set taint information for nodes in normal state.

Taints are a property of nodes that allow nodes to refuse to run certain types of Pods or even evict Pods. Taints work together with tolerations on Pods to prevent Pods from being assigned to inappropriate nodes. One or more taints can be applied to each node, and Pods that cannot tolerate these taints will not be accepted by the node.

For example: For a node where we find its memory utilization has reached 91%, it is not recommended to continue scheduling new Pods to this node. We can set a taint for it. After setting the taint, Kubernetes will not schedule Pods to this node.

[Learn more...](https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/)

### Procedure

1. In the left navigation bar, click **Cluster Management** > **Clusters**.

2. Click the ***cluster name*** where the node to set taints is located.

3. Under the **Nodes** tab, click **Set Taints** on the right side of the node to set taints.

4. Refer to the following description to set the key, value, and effect of taints. Multiple taints can be added to a node.

    Taint attributes consist of `key=value [effect]`.
    
    `key=value` is used to match Pod tolerations. The taint indicates that the node has been contaminated by `key=value`, and Pod scheduling is not allowed or should avoid scheduling to this node, unless the Pod can tolerate (Tolerations) the `key=value` taint.
    
    effect is the effect of the taint, with the following three options:
    
    * **NoSchedule**: Indicates scheduling is not allowed, and already scheduled resources are not affected.
    
    * **PreferNoSchedule**: Indicates try not to schedule.
    
    * **NoExecute**: Indicates scheduling is not allowed, and already scheduled resources will be deleted after `tolerationSeconds`.

5. Click **OK**.

## Label and Taint Management

The platform supports batch setting of labels and taints for nodes.

### Constraints and Limitations

* Before setting device labels, you need to deploy device plugins on the cluster first, such as NVIDIA GPU MPS device plugin, NVIDIA GPU device plugin, GPU Manager device plugin, etc.

    **Tip**: Device labels are actually node labels. For your convenience, the platform categorizes node labels that device plugins depend on as device labels for quick configuration.

### Procedure

1. In the left navigation bar, click **Clusters** > **Clusters**.

2. Click the ***cluster name*** where you want to manage labels and taints.

3. Under the **Nodes** tab, multi-select the nodes you want to manage, and click the **Label and Taint Management** button.

    **Tip**: You can enter the node labels you care about in the search box on the node list page to quickly filter out the list of nodes you want to manage labels and taints for.

4. In **Batch Operations**, add and fill in the operations you want to perform, then click OK to submit the batch operations to the cluster.

    * **Node Labels**: You can **add/update** specified labels for selected nodes, or **delete** specified labels. When selecting delete, the platform will filter out all label lists on the selected nodes. When the value is set to **Any**, it represents deleting labels on all nodes containing the specified label key.
    
    * **Taints**: You can **add/update** specified taints for selected nodes, or **delete** specified taints. When selecting delete, the platform will filter out all taint lists on the selected nodes. When the value is set to **Any**, it represents deleting taints on all nodes containing the specified taint key.
    
    * **Device Labels**: You can set the devices you want to use for selected nodes, where the device list comes from device plugins you have deployed in this cluster.


## Enable/Disable Virtualization Switch

When nodes in an on-premises cluster are physical machines, you can control whether Kubernetes is allowed to schedule virtual machines (VMI, VirtualMachineInstance) to the node by enabling/disabling the node virtualization switch.

When the switch is enabled, newly created virtual machines are allowed to be scheduled to the physical machine node; when the switch is disabled, newly created virtual machines are prohibited from being scheduled to the physical machine node, but this does not affect virtual machines already running on the node.

**Tip**: For related operations and precautions, please refer to [Prepare Virtualization Environment](/virtualization/virtualization/overview.mdx).

## Delete On-Premises Cluster Nodes

Supports deleting nodes in clusters of type on-premises. For example: deleting failed nodes in on-premises clusters.

### Constraints and Limitations

* Nodes in imported clusters are not supported for deletion.

* When there is only one control plane node in the cluster, deleting this control plane node is not supported.

### Procedure

1. In the left navigation bar, click **Cluster Management** > **Clusters**.

2. Click the ***cluster name*** of type **On-Premises** where the node to be deleted is located.

3. Under the **Nodes** tab, click **Delete** on the right side of the node to be deleted.

    **Tip**: If you need to clean up resources under the node after deleting a Linux node, click **Download Cleanup Script** at the bottom of the dialog to download the cleanup script to local. After the node is successfully deleted, log in to the node and execute the cleanup script.
    
4. Enter the node name, then click **Delete**.
