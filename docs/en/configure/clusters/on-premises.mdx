---
weight: 20
title: Creating an On-Premise Cluster
---

# Creating an On-Premise Cluster

## Prerequisites

### Node Requirements

1. If you downloaded a single-architecture installation package from [Download Installation Package](/install/prepare/download.mdx#download_installation_package), ensure your node machines have the same architecture as the package. Otherwise, nodes won't start due to missing architecture-specific images.
2. Verify that your node operating system and kernel are supported. See [Supported OS and Kernels](/install/prepare/prerequisites.mdx#supported_os_and_kernels) for details.
3. Perform availability checks on node machines. For specific check items, refer to [Node Preprocessing > Node Checks](/install/prepare/node_preprocessing.mdx#node_checks).
4. If node machine IPs cannot be directly accessed via SSH, provide a SOCKS5 proxy for the nodes. The `global` cluster will access nodes through this proxy service.

### Load Balancing

For production environments, a load balancer is required for cluster control plane nodes to ensure high availability.
You can provide your own hardware load balancer or enable `Self-built VIP`, which provides software load balancing using haproxy + keepalived.
We recommend using a hardware load balancer because:

- **Better Performance**: Hardware load balancing performs better than software load balancing.
- **Lower Complexity**: If you're unfamiliar with keepalived, misconfigurations could make the cluster unavailable, leading to lengthy troubleshooting and seriously affecting cluster reliability.

When using your own hardware load balancer, you can use the load balancer's VIP as the `IP Address / Domain` parameter. If you have a domain name that resolves to the load balancer's VIP, you can use that domain as the `IP Address / Domain` parameter.
Note:

- The load balancer must correctly forward traffic to ports `6443`, `11780`, and `11781` on all control plane nodes in the cluster.
- If your cluster has only one control plane node and you use that node's IP as the `IP Address / Domain` parameter, the cluster cannot be scaled from a single node to a highly available multi-node setup later. Therefore, we recommend providing a load balancer even for single-node clusters.

When enabling `Self-built VIP`, you need to prepare:

1. An available VRID
2. A host network that supports the VRRP protocol
3. All control plane nodes and the VIP must be on the same subnet, and the VIP must be different from any node IP.

### Connecting `global` Cluster and Workload Cluster

The platform requires mutual access between the `global` cluster and workload clusters. If they're not on the same network, you need to:

1. Provide `External Access` for the workload cluster to ensure the `global` cluster can access it. Network requirements must ensure `global` can access ports `6443`, `11780`, and `11781` on all control plane nodes.
2. Add an additional address to `global` that the workload cluster can access. When creating a workload cluster, add this address to the cluster's annotations with the key `cpaas.io/platform-url` and the value set to the public access address of `global`.

### Image Registry

Cluster images support Platform Built-in, Private Repository, and Public Repository options.

- **Platform Built-in**: Uses the image registry provided by the `global` cluster. If the cluster cannot access `global`, see [Add External Address for Built-in Registry](/configure/clusters/how-to/add_external_addr_for_global_registry.mdx).
- **Private Repository**: Uses your own image registry. For details on pushing required images to your registry, contact technical support.
- **Public Repository**: Uses the platform's public image registry. Before using, complete [Updating Public Repository Credentials](/configure/clusters/how-to/update_public_repository_credentials.mdx).

### Container Networking

If you plan to use Kube-OVN's Underlay for your cluster, refer to [Preparing Kube-OVN Underlay Physical Network](/configure/networking/how_to/kubeovn_underlay_py.mdx).

## Creation Procedure

1. Enter the **Platform Management** view, and click **Clusters/Clusters** in the left navigation bar.

2. Click **Create Cluster**.

3. Configure the following sections according to the instructions below: Basic Info, Container Network, Node Settings, and Extended Parameters.

### Basic Info

<table>
  <tr>
    <th style={{ whiteSpace: "nowrap" }}>Parameter</th>
    <th style={{ whiteSpace: "nowrap" }}>Description</th>
  </tr>
  <tr>
    <td>
      <b>Kubernetes Version</b>
    </td>
    <td>
      All optional versions are rigorously tested for stability and
      compatibility.
      <br />
      <b>Recommendation:</b> Choose the latest version for optimal features and
      support.
    </td>
  </tr>
  <tr>
    <td>
      <b>Container Runtime</b>
    </td>
    <td>
      Containerd is provided as the default container runtime.
      <br />
      If you prefer using Docker as the container runtime, please refer to
      [Choosing a Container Runtime](/configure/clusters/how-to/choose_cri.mdx).
    </td>
  </tr>
  <tr>
    <td>
      <b>Cluster Network Protocol</b>
    </td>
    <td>
      Supports three modes: IPv4 single stack, IPv6 single stack, IPv4/IPv6 dual
      stack.
      <br />
      <b>Note:</b> If you select dual stack mode, ensure all nodes have
      correctly configured IPv6 addresses; the network protocol cannot be
      changed after setting.
    </td>
  </tr>
  <tr>
    <td>
      <b>Cluster Endpoint</b>
    </td>
    <td>
      <code>IP Address / Domain</code>: Enter the pre-prepared domain name or
      VIP if no domain name is available.
      <br />
      <code>Self-Built VIP</code>: Disabled by default. Only enable if you
      haven't provided a LoadBalancer. When enabled, the installer will
      automatically deploy <code>keepalived</code> for software load balancing
      support.
      <br />
      <code>External Access</code>: Enter the externally accessible address
      prepared for the cluster when it's not in the same network environment as
      the <code>global</code> cluster.
    </td>
  </tr>
</table>

### Container Network

<Tabs>
  <Tab label="Kube-OVN">
  An enterprise-grade Cloud Native Kubernetes container network orchestration system developed by Alauda. It brings mature networking capabilities from the OpenStack domain to Kubernetes, supporting cross-cloud network management, traditional network architecture and infrastructure interconnection, and edge cluster deployment scenarios, while greatly enhancing Kubernetes container network security, management efficiency, and performance.
  <table>
    <tr>
      <th style={{ whiteSpace: 'nowrap' }}>Parameter</th>
      <th style={{ whiteSpace: 'nowrap' }}>Description</th>
    </tr>
    <tr>
      <td><b>Subnet</b></td>
      <td>
        Also known as Cluster CIDR, represents the <b>default subnet</b> segment. After cluster creation, additional subnets can be added.
      </td>
    </tr>
    <tr>
      <td><b>Transmit Mode</b></td>
      <td>
        <b>Overlay</b>: A virtual network abstracted over the infrastructure that doesn't consume physical network resources. When creating an Overlay default subnet, all Overlay subnets in the cluster use the same cluster NIC and node NIC configuration.<br />
        <b>Underlay</b>: This transmission method relies on physical network devices. It can directly allocate physical network addresses to Pods, ensuring better performance and connectivity with the physical network. Nodes in an Underlay subnet must have multiple NICs, and the NIC used for bridge networking must be exclusively used by Underlay and not carry other traffic like SSH. When creating an Underlay default subnet, the cluster NIC is actually a default NIC for bridge networking, and the node NIC is the node NIC configuration in the bridge network.<br />
        - <b>Default Gateway</b>: The physical network gateway address, which is the gateway address for the Cluster CIDR segment (must be within the Cluster CIDR address range).<br />
        - <b>VLAN ID</b>: Virtual LAN identifier (VLAN number), e.g., <code>0</code>.<br />
        - <b>Reserved IPs</b>: Set reserved IPs that won't be automatically allocated, such as IPs in the subnet that are already used by other devices.
      </td>
    </tr>
    <tr>
      <td><b>Service CIDR</b></td>
      <td>
        IP address range used by Kubernetes Services of type ClusterIP. Cannot overlap with the default subnet range.
      </td>
    </tr>
    <tr>
      <td><b>Join CIDR</b></td>
      <td>
        In Overlay transmission mode, this is the IP address range used for communication between nodes and pods. Cannot overlap with the default subnet or Service CIDR.
      </td>
    </tr>
  </table>
  </Tab>

  <Tab label="Calico">
  Calico is a layer 3 networking solution that provides secure network connections for containers.

  <table>
    <tr>
      <th style={{ whiteSpace: 'nowrap' }}>Parameter</th>
      <th style={{ whiteSpace: 'nowrap' }}>Description</th>
    </tr>
    <tr>
      <td><b>Default Subnet</b></td>
      <td>
        Also known as Cluster CIDR, represents the <b>default subnet</b> segment. After cluster creation, additional subnets can be added.
      </td>
    </tr>
    <tr>
      <td><b>Service CIDR</b></td>
      <td>
        IP address range used by Kubernetes Services of type ClusterIP. Cannot overlap with the default subnet range.
      </td>
    </tr>
  </table>
  </Tab>

{" "}

<Tab label="Flannel">
  Flannel provides a flat network environment for all containers in the cluster,
  giving containers created on different node hosts a unique virtual IP address
  across the entire cluster. The pod subnet is divided evenly among the cluster
  nodes according to the mask, and pods on each node are assigned IP addresses
  from the segment allocated to that node. This improves communication
  efficiency between containers without having to consider IP translation
  issues.
  <table>
    <tr>
      <th style={{ whiteSpace: "nowrap" }}>Parameter</th>
      <th style={{ whiteSpace: "nowrap" }}>Description</th>
    </tr>
    <tr>
      <td>
        <b>Cluster CIDR</b>
      </td>
      <td>
        IP address range used by pods created when the cluster starts. Supports
        setting the maximum number of IP addresses that can be allocated to pods
        on each node under the current container network.
        <br />
        <b>Note</b>: The platform will automatically calculate the maximum
        number of nodes the cluster can accommodate based on the above
        configuration and display it in the hint below the input field.
        <br />
        <b>Important</b>: After cluster creation, the cluster network cannot be
        changed, so please plan the network carefully.
      </td>
    </tr>
    <tr>
      <td>
        <b>Service CIDR</b>
      </td>
      <td>
        IP address range used by Kubernetes Services of type ClusterIP. Cannot
        overlap with the container subnet range.
      </td>
    </tr>
  </table>
</Tab>

  <Tab label="Custom">
  If you need to install other network plugins, select <b>Custom</b> mode. You can manually install network plugins after the cluster is successfully created.

  <table>
    <tr>
      <th style={{ whiteSpace: 'nowrap' }}>Parameter</th>
      <th style={{ whiteSpace: 'nowrap' }}>Description</th>
    </tr>
    <tr>
      <td><b>Cluster CIDR</b></td>
      <td>
        IP address range used by pods created when the cluster starts.
      </td>
    </tr>
    <tr>
      <td><b>Service CIDR</b></td>
      <td>
        IP address range used by Kubernetes Services of type ClusterIP. Cannot overlap with the container subnet range.
      </td>
    </tr>
  </table>
  </Tab>
</Tabs>

### Node Settings

<table>
  <tr>
    <th style={{ whiteSpace: "nowrap" }}>Parameter</th>
    <th style={{ whiteSpace: "nowrap" }}>Description</th>
  </tr>
  <tr>
    <td>
      <b>Network Interface Card</b>
    </td>
    <td>
      The name of the host network interface device used by the cluster network
      plugin.
      <br />
      <b>Note</b>:<br />
      - When selecting Underlay transmission mode for the Kube-OVN default subnet,
      you must specify the network interface name, which will be the default NIC
      for bridge networking.
      <br />- The platform's network interface traffic monitoring by default
      recognizes traffic on interfaces named like{" "}
      <code>eth\.\|en\.\|wl\.*\|ww\.*</code>. If you use interfaces with
      different naming conventions, please refer to [Collect Network Data from
      Custom-Named Network
      Interfaces](/observability/monitor/how_to/special_network_card_name.mdx)
      after cluster onboarding to modify the relevant resources and ensure the
      platform can properly monitor network interface traffic.
    </td>
  </tr>
  <tr>
    <td>
      <b>Node Name</b>
    </td>
    <td>
      You can choose to use either the node IP or hostname as the node name on
      the platform.
      <br />
      <b>Note</b>: When choosing to use hostname as the node name, ensure that
      the hostnames of nodes added to the cluster are unique.
    </td>
  </tr>
  <tr>
    <td>
      <b>Nodes</b>
    </td>
    <td>
      <b>Add nodes</b> to the cluster, or <b>Recovery from draft</b> temporarily
      saved node information. See the detailed parameter descriptions for adding
      nodes below.
    </td>
  </tr>
  <tr>
    <td>
      <b>Monitoring Type</b>
    </td>
    <td>
      Supports <b>Prometheus</b> and <b>VictoriaMetrics</b>.<br />
      When selecting <b>VictoriaMetrics</b> as the monitoring component, you must
      configure the <b>Deploy Type</b>:<br />
      <b>- Deploy VictoriaMetrics</b>: Deploys all related components, including{" "}
      <b>VMStorage</b>, <b>VMAlert</b>, <b>VMAgent</b>, etc.
      <br />
      <b>- Deploy VictoriaMetrics Agent</b>: Only deploys the log collection
      component, <b>VMAgent</b>. When using this deployment method, you need to
      associate with a VictoriaMetrics instance already deployed on another
      cluster in the platform to provide monitoring services for the cluster.
    </td>
  </tr>
  <tr>
    <td>
      <b>Monitoring Nodes</b>
    </td>
    <td>
      Select nodes for deploying cluster monitoring components. Supports
      selecting compute nodes and control plane nodes that allow application
      deployment.
      <br />
      To avoid affecting cluster performance, it's recommended to prioritize
      compute nodes. After the cluster is successfully created, monitoring
      components with storage type <b>Local Volume</b> will be deployed on the
      selected nodes.
    </td>
  </tr>
</table>

**Node Addition Parameters**

<table>
  <tr>
    <th style={{ whiteSpace: "nowrap" }}>Parameter</th>
    <th style={{ whiteSpace: "nowrap" }}>Description</th>
  </tr>
  <tr>
    <td>
      <b>Type</b>
    </td>
    <td>
      <b>Control Plane Node</b>: Responsible for running components such as
      kube-apiserver, kube-scheduler, kube-controller-manager, etcd, container
      network, and some platform management components in the cluster. When{" "}
      <b>Application Deployable</b> is enabled, control plane nodes can also be
      used as compute nodes.
      <br />
      <b>Worker Node</b>: Responsible for hosting business pods running on the
      cluster.
    </td>
  </tr>
  <tr>
    <td>
      <b>IPv4 Address</b>
    </td>
    <td>
      The IPv4 address of the node. For clusters created in internal network
      mode, enter the node's <b>private IP</b>.
    </td>
  </tr>
  <tr>
    <td>
      <b>IPv6 Address</b>
    </td>
    <td>
      Valid when the cluster has IPv4/IPv6 dual stack enabled. The IPv6 address
      of the node.
    </td>
  </tr>
  <tr>
    <td>
      <b>Application Deployable</b>
    </td>
    <td>
      Valid when <b>Node Type</b> is <b>Control Plane Node</b>. Whether to allow
      business applications to be deployed on this control plane node,
      scheduling business-related pods to this node.
    </td>
  </tr>
  <tr>
    <td>
      <b>Display Name</b>
    </td>
    <td>The display name of the node.</td>
  </tr>
  <tr>
    <td>
      <b>SSH Connection IP</b>
    </td>
    <td>
      The IP address that can connect to the node when accessing it via SSH
      service.
      <br />
      If you can log in to the node using{" "}
      <code>ssh &lt;username&gt;@&lt;node's IPv4 address&gt;</code>, this
      parameter is not required; otherwise, enter the node's public IP or NAT
      external IP to ensure the <code>global</code> cluster and proxy can
      connect to the node via this IP.
    </td>
  </tr>
  <tr>
    <td>
      <b>Network Interface Card</b>
    </td>
    <td>
      Enter the name of the network interface used by the node. The priority of
      network interface configuration effectiveness is as follows (from left to
      right, in descending order):
      <br />
      <b>Kube-OVN Underlay</b>: Node NIC > Cluster NIC
      <br />
      <b>Kube-OVN Overlay</b>: Node NIC > Cluster NIC > NIC corresponding to the
      node's default route
      <br />
      <b>Calico</b>: Cluster NIC > NIC corresponding to the node's default route
      <br />
      <b>Flannel</b>: Cluster NIC > NIC corresponding to the node's default
      route
    </td>
  </tr>
  <tr>
    <td>
      <b>Associated Bridge Network</b>
    </td>
    <td>
      <b>Note</b>: When creating a cluster, bridge network configuration is not
      supported; this option is only available when <b>adding nodes</b> to a
      cluster that already has Underlay subnets created.
      <br />
      Select an existing [Add Bridge
      Network](/configure/networking/functions/configure_subnet.mdx#kube-ovn_underlay_bridge_network).
      If you don't want to use the bridge network's default NIC, you can
      configure the node NIC separately.
    </td>
  </tr>
  <tr>
    <td>
      <b>SSH Port</b>
    </td>
    <td>
      SSH service port number, e.g., <code>22</code>.
    </td>
  </tr>
  <tr>
    <td>
      <b>SSH Username</b>
    </td>
    <td>
      SSH username, needs to be a user with root privileges, e.g.,{" "}
      <code>root</code>.
    </td>
  </tr>
  <tr>
    <td>
      <b>Proxy</b>
    </td>
    <td>
      Whether to access the node's SSH port through a proxy. When the{" "}
      <code>global</code> cluster cannot directly access the node to be added
      via SSH (e.g., the <code>global</code> cluster and workload cluster are
      not in the same subnet; the node IP is an internal IP that the{" "}
      <code>global</code> cluster cannot directly access), this switch needs to
      be turned on and proxy-related parameters configured. After configuring
      the proxy, node access and deployment can be achieved through the proxy.
      <br />
      <b>Note</b>: Currently, only SOCKS5 proxy is supported.
      <br />
      <b>Access URL</b>: Proxy server address, e.g.,{" "}
      <code>192.168.1.1:1080</code>.<br />
      <b>Username</b>: Username for accessing the proxy server.
      <br />
      <b>Password</b>: Password for accessing the proxy server.
    </td>
  </tr>
  <tr>
    <td>
      <b>SSH Authentication</b>
    </td>
    <td>
      Authentication method and corresponding authentication information for
      logging into the added node. Options include:
      <br />
      <b>Password</b>: Requires a username with root privileges and the
      corresponding <b>SSH password</b>.<br />
      <b>Key</b>: Requires a <b>private key</b> with root privileges and the <b>
        private key password
      </b>.
    </td>
  </tr>
  <tr>
    <td>
      <b>Save Draft</b>
    </td>
    <td>
      Saves the currently configured data in the dialog as a draft and closes
      the <b>Add Node</b> dialog.
      <br />
      Without leaving the <b>Create Cluster</b> page, you can select{" "}
      <b>Restore from draft</b> to open the <b>Add Node</b> dialog and restore
      the configuration data saved as a draft.
      <br />
      <b>Note</b>: The data restored from the draft is the most recently saved
      draft data.
    </td>
  </tr>
</table>

### Extended Parameters

**Note**:

- Apart from required configurations, it's not recommended to set extended parameters, as incorrect settings may make the cluster unavailable and cannot be modified after cluster creation.

- If a entered **Key** duplicates a default parameter **Key**, it will override the default configuration.

**Procedure**

1. Click **Extended Parameters** to expand the extended parameter configuration area. You can optionally set the following extended parameters for the cluster:

<table>
  <tr>
    <th style={{ whiteSpace: "nowrap" }}>Parameter</th>
    <th style={{ whiteSpace: "nowrap" }}>Description</th>
  </tr>
  <tr>
    <td>
      <b>Docker Parameters</b>
    </td>
    <td>
      <code>dockerExtraArgs</code>, additional configuration parameters for
      Docker, which will be written to <code>/etc/sysconfig/docker</code>.
      Modification is not recommended. To configure Docker through the{" "}
      <code>daemon.json</code> file, it must be configured as key-value pairs.
    </td>
  </tr>
  <tr>
    <td>
      <b>Kubelet Parameters</b>
    </td>
    <td>
      <code>kubeletExtraArgs</code>, additional configuration parameters for
      Kubelet.
      <br />
      <b>Note</b>: When the <b>Container Network</b>'s <b>Node IP Count</b>{" "}
      parameter is entered, a default <b>Kubelet Parameter</b> configuration
      with the key <code>max-pods</code> and a value of <b>Node IP Count</b> is
      automatically generated. This sets the maximum number of pods that can run
      on any node in the cluster. This configuration is not displayed in the
      interface.
      <br />
      Adding a new <code>max-pods: maximum number of runnable pods</code>{" "}
      key-value pair in the <b>Kubelet Parameters</b> area will override the
      default value. Any positive integer is allowed, but it's recommended to
      use the default value (Node IP Count) or enter a value not exceeding{" "}
      <code>256</code>.
    </td>
  </tr>
  <tr>
    <td>
      <b>Controller Manager Parameters</b>
    </td>
    <td>
      <code>controllerManagerExtraArgs</code>, additional configuration
      parameters for the Controller Manager.
    </td>
  </tr>
  <tr>
    <td>
      <b>Scheduler Parameters</b>
    </td>
    <td>
      <code>schedulerExtraArgs</code>, additional configuration parameters for
      the Scheduler.
    </td>
  </tr>
  <tr>
    <td>
      <b>APIServer Parameters</b>
    </td>
    <td>
      <code>apiServerExtraArgs</code>, additional configuration parameters for
      the APIServer.
    </td>
  </tr>
  <tr>
    <td>
      <b>APIServer URL</b>
    </td>
    <td>
      <code>publicAlternativeNames</code>, APIServer access addresses issued in
      the certificate. Only IPs or domain names can be entered, with a maximum
      of 253 characters.
    </td>
  </tr>
  <tr>
    <td>
      <b>Cluster Annotations</b>
    </td>
    <td>
      Cluster annotation information, marking cluster characteristics in
      metadata in the form of key-value pairs for platform components or
      business components to obtain relevant information.
    </td>
  </tr>
</table>

4. Click **Create**. You'll return to the cluster list page where the cluster will be in the **Creating** state.

## Post-Creation Steps

### Viewing Creation Progress

On the cluster list page, you can view the list of created clusters. For clusters in the **Creating** state, you can check the execution progress.

**Procedure**

1. Click the small icon **View Execution Progress** to the right of the cluster status.

2. In the execution progress dialog that appears, you can view the cluster's execution progress (status.conditions).

   **Tip**: When a certain type is in progress or in a failed state with a reason, hover your cursor over the corresponding reason (shown in blue text) to view detailed information about the reason (status.conditions.reason).

### Associating with Projects

After the cluster is created, you can add it to projects in the project management view.
