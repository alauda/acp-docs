---
weight: 20
title: Evaluating Resources for Global Cluster
---

# Evaluating Resources for Global Cluster

## Overview

This topic provides recommended practices and resource evaluation guidelines for Multi-Cluster in <Term name="productShort" />.

Proper node sizing ensures the control plane can efficiently manage all registered clusters, handle synchronization traffic, and process user API and Web Console requests without performance degradation.

## Control Plane Node Sizing

The Global cluster's control plane is responsible for:

- Maintaining cluster registration and metadata.
- Handling inbound API requests from the Web Console and CLI.
- Coordinating synchronization and heartbeat messages with managed clusters.
- Managing internal controllers and resource reconciliation loops.

Because the Global Cluster must handle both **management operations** and **data aggregation** from all connected clusters, resource allocation should be planned according to the expected scale and workload intensity.

### Baseline Hardware Requirements

For initial evaluation or development environments, you can start with the following minimum configuration:

| Resource | Minimum Requirement | Supported Scale                          |
| -------- | ------------------- | ---------------------------------------- |
| CPU      | 12 cores            | Up to 10 managed clusters                |
| Memory   | 24 GB               | Up to 10 managed clusters                |
| Storage  | 200 GB SSD          | Depends on logging and metrics retention |

This configuration is suitable for functional testing and low-concurrency scenarios.
It is **not recommended for production** due to limited concurrency and recovery capacity.

### Recommended Production Sizing

The production-scale sizing depends primarily on:

- Number of **managed clusters**
- Frequency of **synchronization cycles**
- **Concurrent API request rate** (from users or automation)
- Volume of **streaming requests**

The following table provides reference configurations validated through internal performance testing.

| Scale Tier     | Managed Clusters | Control Plane Nodes | CPU per Node | Memory per Node | Notes                                                     |
| -------------- | ---------------- | ------------------- | ------------ | --------------- | --------------------------------------------------------- |
| **Small**      | ≤ 20             | 3                   | 4 cores      | 16 GB           | Suitable for small-scale environments or single region    |
| **Medium**     | ≤ 50             | 3                   | 8 cores      | 32 GB           | Default production setup                                  |
| **Large**      | ≤ 100            | 3                   | 16 cores     | 64 GB           | Supports heavy Web Console usage and frequent sync cycles |
| **Enterprise** | ≤ 500            | 5                   | 16 cores     | 64 GB           | Requires horizontal scaling and dedicated infra nodes     |

<Directive type="warning">
  These recommendations are general guidelines. Actual requirements depend on your cluster topology, user concurrency, and synchronization frequency.
</Directive>

### Vertical Scaling Guidelines

When increasing load per node (for example, 2× more clusters or higher user concurrency), follow these adjustments:

| Parameter  | Scaling Recommendation                        |
| ---------- | --------------------------------------------- |
| **CPU**    | +50% for every 50 additional managed clusters |
| **Memory** | +50% for every 50 additional managed clusters |

### Horizontal Scaling Guidelines

When exceeding 100 managed clusters or encountering persistent API latency above 500 ms:

**Add control plane nodes** (up to 5 total) to distribute request handling and controller workloads.

### Resource Validation and Monitoring

After deployment, continuously monitor the following metrics to validate node sizing:

| Metric                  | Recommended Range      |
| ----------------------- | ---------------------- |
| Node CPU utilization    | 60–75% under peak load |
| Node Memory utilization | ≤80% sustained         |
| API request latency     | P95 \< 500ms           |
| etcd commit latency     | P99 \< 50ms            |

<Tabs>
  <Tab label="Node CPU utilization">
    ```javascript
    100 * (1 - avg by (instance)(rate(node_cpu_seconds_total{mode="idle"}[5m])))
    ```
  </Tab>

  <Tab label="Node Memory utilization">
    ```javascript
    100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))
    ```
  </Tab>

  <Tab label="API request latency">
    ```javascript
    histogram_quantile(
      0.95,
      sum(rate(apiserver_request_duration_seconds_bucket{verb!="WATCH",resource!="events"}[5m])) by (le)
    )
    ```
  </Tab>

  <Tab label="etcd commit latency">
    ```javascript
    histogram_quantile(
      0.99,
      sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (le)
    )
    ```
  </Tab>
</Tabs>

<Directive type="note">
  If sustained resource usage consistently exceeds recommended thresholds, scale vertically (add CPU/memory) or horizontally (add nodes) before user-facing performance degradation occurs.
</Directive>

## Summary

When sizing the Global Cluster:

1. Begin with **3 nodes × 8 cores × 32 GB** for moderate-scale deployments (≤50 clusters).
2. Scale **vertically** for higher request concurrency or heavy Web Console usage.
3. Scale **horizontally** beyond 100 clusters to maintain API responsiveness.
4. Re-evaluate sizing after every significant increase in managed cluster count or sync frequency.

Following these guidelines ensures predictable performance and operational stability as your Multi-Cluster environment grows.
