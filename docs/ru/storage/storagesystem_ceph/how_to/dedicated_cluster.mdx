---
weight: 61
sourceSHA: 8ca15fa9b2822ccdced974ffe44fc12ee337553634357ec11fd6cdb9a2d5c96d
---

# Настройка выделенного кластера для распределенного хранения

Развертывание выделенного кластера подразумевает использование независимого кластера для развертывания распределенного хранения платформы, при этом другие бизнес-кластеры внутри платформы получают доступ и используют предоставляемые им услуги хранения через интеграцию.\
Чтобы обеспечить производительность и стабильность распределенного хранения платформы, в выделенном кластере для хранения разворачиваются только основные компоненты платформы и компоненты распределенного хранения, избегая совместного размещения других бизнес-нагрузок. Этот подход к развертыванию является рекомендуемой доброй практикой для распределенного хранения платформы.

## Архитектура

Архитектура разделения хранения и вычислений

![](../../../../en/storage/storagesystem_ceph/assets/dedicated_cluster_architecture.png)

## Требования к инфраструктуре

### Требования к платформе

Поддерживается в версиях 3.18 и выше.

### Требования к кластеру

Рекомендуется использовать кластеры на базе «железных» серверов в качестве выделенных кластеров для хранения.

### Требования к ресурсам

См. [Основные концепции](../concepts/concept.mdx) для компонентов развертывания распределенного хранения.

Каждый компонент имеет определенные требования к CPU и памяти. Рекомендуемые конфигурации следующие:

| Процесс | CPU | Память |
| :------ | :-: | :----: |
| MON     |  2c |   3Gi  |
| MGR     |  3c |   4Gi  |
| MDS     |  3c |   8Gi  |
| RGW     |  2c |   4Gi  |
| OSD     |  4c |   8Gi  |

В кластере обычно работают:

- 3 MON
- 2 MGR
- несколько OSD
- 2 MDS (если используется CephFS)
- 2 RGW (если используется CephObjectStorage)

Основываясь на распределении компонентов, следующие рекомендации по ресурсам для каждого узла применимы:

| CPU                        | Память                       |
| :------------------------- | :--------------------------- |
| 16c + (4c \* OSD на узел) | 20Gi + (8Gi \* OSD на узел) |

### Требования к накопителям

Рекомендуется развертывать 12 или меньше накопителей на узел. Это помогает сократить время восстановления после сбоя узла.

#### Требования к типу накопителей

Рекомендуется использовать корпоративные SSD с емкостью 10TiB или меньше на устройство и гарантировать, что все диски идентичны по размеру и типу.

#### Планирование емкости

Перед развертыванием необходимо спланировать емкость хранения в соответствии с конкретными бизнес-требованиями. По умолчанию распределенная система хранения использует стратегию избыточности с 3 репликами. Поэтому usable capacity рассчитывается путем деления общей сырой емкости хранения (с всех накопителей) на 3.

Пример для 30(N) узлов (количество реплик = 3), сценарий usable capacity следующий:

| Размер накопителя (D) | Накопитель на узел (M) | Общая емкость (D*M*N) | usable capacity (D*M*N/3) |
| :--------------------: | :---------------------: | :--------------------: | :------------------------: |
|         0.5 TiB        |              3          |         45 TiB         |          15 TiB           |
|          2 TiB         |              6          |        360 TiB         |          120 TiB          |
|          4 TiB         |              9          |        1080 TiB        |          360 TiB          |

#### Мониторинг емкости и расширение

1. **Проактивное планирование емкости**

   Всегда следите за тем, чтобы usable storage capacity превышала потребление. Если хранилище полностью исчерпано, восстановление требует ручного вмешательства и не может быть решено простым удалением или перемещением данных.

2. **Оповещения о емкости**

   Кластер вызывает оповещения при достижении двух порогов:

   - **80% использования** ("почти полный"): Проактивно **освободите место** или масштабируйте кластер.
   - **95% использования** ("полный"): Хранилище полностью исчерпано, и стандартные команды не могут освободить место. Немедленно обратитесь в службу поддержки платформы.

   Всегда быстро реагируйте на оповещения и регулярно контролируйте использование хранилища, чтобы избежать сбоев.

3. **Рекомендации по масштабированию**

   - **Избегайте**: Добавления накопителей к существующим узлам.
   - **Рекомендуется**: Масштабироваться, добавляя новые узлы хранения.
   - **Требование**: Новые узлы должны использовать накопители, идентичные по размеру, типу и количеству существующим узлам.

### Требования к сети

Распределенное хранилище должно использовать **HostNetwork**.

#### Изоляция сети

Сеть делится на два типа:

- **Публичная сеть**: Используется для взаимодействия клиентов с компонентами хранения (например, запросы ввода-вывода).
- **Кластерная сеть**: Посвящена репликации данных между репликами и балансировке данных (например, восстановление).

Чтобы обеспечить качество обслуживания и стабильность производительности:

1. Для выделенных кластеров хранения:\
   Зарезервируйте два сетевых интерфейса на каждом хосте:
   - Публичная сеть: Для связи клиентов и компонентов.
   - Кластерная сеть: Для внутренней репликации и трафика сбалансировки.
2. Для бизнес-кластов:\
   Зарезервируйте один сетевой интерфейс на каждом хосте для доступа к публичной сети хранения.

Пример конфигурации изоляции сети

![](../../../../en/storage/storagesystem_ceph/assets/dedicated_cluster_network_architecture.png)

#### Требования к скорости интерфейса сети

1. **Узлы хранения**
   - **Публичная сеть** и **Кластерная сеть** требуют сетевых интерфейсов 10GbE или выше.

2. **Узлы бизнес-кластера**
   - Сетевой интерфейс, используемый для доступа к **публичной сети** хранения, должен быть 10GbE или выше.

## Процедура

<Steps>
  ### Развертывание оператора

  1. Получите доступ к **Управлению платформой**.

  2. В левом боковом меню нажмите **Управление хранилищем** > **Распределенное хранилище**.

  3. Нажмите **Создать сейчас**.

  4. На странице мастера **Развертывания оператора** нажмите кнопку **Развернуть оператора** в правом нижнем углу.
     - Когда страница автоматически перейдет к следующему шагу, это указывает на успешное развертывание оператора.
     - Если развертывание не удалось, пожалуйста, обратитесь к подсказке на интерфейсе **Очистить развернутую информацию и повторить попытку** и повторно разверните оператора; если вы хотите вернуться к странице выбора распределенного хранилища, нажмите **Магазин приложений**, сначала удалите ресурсы в уже развернутом **rook-operator**, а затем удалите **rook-operator**.

  ### Создание кластера Ceph

  Выполните команды на **управляющем узле** кластера хранения.

  <details>
    <summary>Нажмите, чтобы просмотреть</summary>

    ```yaml
    cat << EOF | kubectl create -f -
    apiVersion: ceph.rook.io/v1
    kind: CephCluster
    metadata:
      name: ceph-cluster
      namespace: rook-ceph
    spec:
      cephConfig:
        global:
          mon_memory_target: "3221225472"
          mds_cache_memory_limit: "8589934592"
          osd_memory_target: "8589934592"
          bluefs_buffered_io: "false"
        mon:
          auth_allow_insecure_global_id_reclaim: "true"
          mon_warn_on_insecure_global_id_reclaim: "false"
          mon_warn_on_insecure_global_id_reclaim_allowed: "false"
      cephVersion:
        image: build-harbor.alauda.cn/3rdparty/ceph/ceph:v18.2.4-0
      dashboard:
        enabled: true
      dataDirHostPath: /var/lib/rook
      mgr:
        count: 2
        modules:
        - enabled: true
          name: pg_autoscaler
      mon:
        count: 3
      monitoring:
        enabled: true
      network:
        ipFamily: IPv4
        addressRanges:
          public:
          - <public network cidr>
          cluster:
          - <cluster network cidr>
        provider: host
      placement:
        all:
          tolerations:
          - effect: NoSchedule
            operator: Exists
          - key: "node-role.kubernetes.io/master"
            operator: "Exists"
            effect: "NoSchedule"
          - key: "node-role.kubernetes.io/control-plane"
            operator: "Exists"
            effect: "NoSchedule"
          - key: "node-role.kubernetes.io/cpaas-system"
            operator: "Exists"
            effect: "NoSchedule"
        mgr:
          podAffinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - rook-ceph-mgr
                  topologyKey: kubernetes.io/hostname
      priorityClassNames:
        all: system-node-critical
      resources:
        crashcollector:
          limits:
            cpu: 200m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 64Mi
        mgr:
          requests:
            cpu: "3"
            memory: 4Gi
        mon:
          requests:
            cpu: "2"
            memory: 3Gi
        osd:
          requests:
            cpu: "4"
            memory: 8Gi
      storage:
        <storage devices>
    EOF
    ```
  </details>

  **Параметры**:

  - **public network cidr**: CIDR публичной сети хранения (например, `- 10.0.1.0/24`).
  - **cluster network cidr**: CIDR кластерной сети хранения (например, `- 10.0.2.0/24`).
  - **storage devices**: Укажите накопители, которые будут использоваться распределенным хранилищем.\
    Пример форматирования:
    ```
      nodes:
      - name: storage-node-01
        devices:
        - name: /dev/disk/by-id/wwn-0x5000cca01dd27d60
        useAllDevices: false
      - name: storage-node-02
        devices:
        - name: sdb
        - name: sdc
        useAllDevices: false
      - name: storage-node-03
        devices:
        - name: sdb
        - name: sdc
        useAllDevices: false
    ```
      <Directive type="info" title="Совет">
        Используйте Всемирное уникальное имя (WWN) диска для стабильного именования, что избегает зависимости от нестабильных путей устройства, таких как `sdb`, которые могут изменяться после перезагрузок.
      </Directive>

  ### Создать хранилищные пулы

  Доступны три типа хранилищных пулов. Выберите и создайте подходящие в зависимости от ваших бизнес-требований.

  #### Создание файлового пула

  Выполните команды на **управляющем узле** кластера хранения.

  <details>
    <summary>Нажмите, чтобы просмотреть</summary>

    ```yaml
    cat << EOF | kubectl apply -f -
    apiVersion: ceph.rook.io/v1
    kind: CephFilesystem
    metadata:
      name: cephfs
      namespace: rook-ceph
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          requireSafeReplicaSize: true
          size: 3
      dataPools:
      - failureDomain: host
        replicated:
          requireSafeReplicaSize: true
          size: 3
      preserveFilesystemOnDelete: false
      metadataServer:
        activeCount: 1
        activeStandby: true
        placement:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - rook-ceph-mds
              topologyKey: kubernetes.io/hostname
          tolerations:
          - effect: NoSchedule
            operator: Exists
        resources:
          requests:
            cpu: "3"
            memory: 8Gi
    EOF
    ```
  </details>

  #### Создание блочного пула

  Выполните команды на **управляющем узле** кластера хранения.

  <details>
    <summary>Нажмите, чтобы просмотреть</summary>

    ```yaml
    cat << EOF | kubectl apply -f -
    apiVersion: ceph.rook.io/v1
    kind: CephBlockPool
    metadata:
      name: block
      namespace: rook-ceph
    spec:
      failureDomain: host
      replicated:
        size: 3
    EOF
    ```
  </details>

  #### Создание объектного пула

  Выполните команды на **управляющем узле** кластера хранения.

  <details>
    <summary>Нажмите, чтобы просмотреть</summary>

    ```yaml
    cat << EOF | kubectl apply -f -
    apiVersion: ceph.rook.io/v1
    kind: CephObjectStore
    metadata:
      name: object
      namespace: rook-ceph
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          requireSafeReplicaSize: true
          size: 3
      dataPool:
        failureDomain: host
        replicated:
          requireSafeReplicaSize: true
          size: 3
      preservePoolsOnDelete: false
      gateway:
        instances: 2
        placement:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - rook-ceph-rgw
              topologyKey: kubernetes.io/hostname
          tolerations:
          - effect: NoSchedule
            operator: Exists
        port: 7480
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
    EOF
    ```
  </details>
</Steps>

## Дальнейшие действия

Когда другим кластерам необходимо использовать услуги распределенного хранения, обратитесь к следующим руководствам.\
[Доступ к услугам хранения](/storage/storagesystem_ceph/functions/access_storage_service.mdx)
