---
weight: 64
sourceSHA: 42488c8a6dff243d30ca516a98b158dcd6fe4117eb4a764a43ab08b4fc3ef69f
---

# Восстановление после катастрофы в хранилище файлов

CephFS Mirror — это функция файловой системы Ceph, разработанная для обеспечения асинхронной репликации данных между различными кластерами Ceph, что обеспечивает восстановление после катастрофы между кластерами. Его основная функция заключается в синхронизации данных в режиме первичный-резервный, что гарантирует, что резервный кластер может быстро взять на себя услуги в случае сбоя первичного кластера.

:::warning

- CephFS Mirror выполняет инкрементальную синхронизацию на основе снимков, с установленным по умолчанию интервалом создания снимков раз в час (настраиваемым). Разностные данные между первичным и резервным кластерами, как правило, состоят из объема данных, записанных за один цикл создания снимка.
- CephFS Mirror предоставляет только резервное копирование данных подлежащего хранилища и не может обрабатывать резервное копирование ресурсов Kubernetes. Пожалуйста, используйте функцию **Резервное копирование и восстановление** платформы для резервного копирования ресурсов PVC и PV в сочетании.
  :::

## Терминология

| Термин                | Объяснение                                       |
| :-------------------- | :------------------------------------------------ |
| **Первичный кластер** | Кластер, который в настоящее время предоставляет услуги хранения. |
| **Резервный кластер** | Кластер для резервного копирования.              |

## Конфигурация резервного копирования

### Предварительные требования

- Подготовьте два кластера, подходящих для развертывания хранилища Alauda Container Platform (ACP) с Ceph, а именно первичный кластер и резервный кластер, убедившись, что сети между кластерами взаимосвязаны.
- Версии платформы, используемые обоими кластерами (v3.12 и выше), должны быть одинаковыми.
- [Создайте распределенный сервис хранения](../../installation/create_service_stand.mdx) как в первичном, так и в резервном кластерах.
- Создайте пулы файлового хранилища с **одинаковыми именами** как в первичном, так и в резервном кластерах.

### Процедура

<Steps>
  ### Включите зеркалирование для пула файлового хранилища в резервном кластере

  Выполните следующие команды на контрольном узле резервного кластера:

  <Tabs>
    <Tab label="Командная строка">
      ```bash
      kubectl -n rook-ceph patch cephfilesystem <fs-name> \
      --type merge -p '{"spec":{"mirroring":{"enabled": true}}}'
      ```
    </Tab>

    <Tab label="Вывод">
      ```bash
      cephfilesystem.ceph.rook.io/<fs-name> patched
      ```
    </Tab>
  </Tabs>

  **Параметры**:

  - `<fs-name>`: Имя пула файлового хранилища.

  ### <a id="token" />Получите токен пира

  Этот токен является ключевой учетной записью для установления соединения зеркалирования между двумя кластерами.

  Выполните следующие команды на контрольном узле резервного кластера:

  <Tabs>
    <Tab label="Команда">
      ```bash
      kubectl get secret -n rook-ceph \
      $(kubectl -n rook-ceph get cephfilesystem <fs-name> -o jsonpath='{.status.info.fsMirrorBootstrapPeerSecretName}') \
      -o jsonpath='{.data.token}' | base64 -d
      ```
    </Tab>

    <Tab label="Вывод">
      ```bash
      # Ввиду участия конфиденциальной информации, вывод был сокращен.
      eyJmc2lkIjogImMyYjAyNmMzLTA3ZGQtNDA3Z...
      ```
    </Tab>
  </Tabs>

  **Параметры**:

  - `<fs-name>`: Имя пула файлового хранилища.

  ### Создайте секрет пира в первичном кластере

  После получения токена пира из резервного кластера необходимо создать секрет пира в первичном кластере.

  Выполните следующие команды на контрольном узле первичного кластера:

  <Tabs>
    <Tab label="Команда">
      ```bash
      kubectl -n rook-ceph create secret generic fs-secondary-site-secret \
      --from-literal=token=<token> \
      --from-literal=pool=<fs-name>
      ```
    </Tab>

    <Tab label="Вывод">
      ```bash
      secret/fs-secondary-site-secret created
      ```
    </Tab>
  </Tabs>

  **Параметры**:

  - `<token>`: Токен, полученный на [шаге 2](#token).
  - `<fs-name>`: Имя пула файлового хранилища.

  ### Включите зеркалирование для пула файлового хранилища в первичном кластере

  Выполните следующие команды на контрольном узле первичного кластера:

  <Tabs>
    <Tab label="Команда">
      ```bash
      kubectl -n rook-ceph patch cephfilesystem <fs-name> --type merge -p \
      '{
        "spec": {
          "mirroring": {
            "enabled": true, 
            "peers": {
              "secretNames": [
                "fs-secondary-site-secret"
              ]
            }, 
            "snapshotSchedules": [
              {
                "path": "/", 
                "interval": "<schedule-interval>"
              }
            ], 
            "snapshotRetention": [
              {
                "path": "/", 
                "duration": "<retention-policy>"
              }
            ]
          }
        }
      }'
      ```
    </Tab>

    <Tab label="Пример">
      ```bash
      kubectl -n rook-ceph patch cephfilesystem cephfs --type merge -p \
      '{
        "spec": {
          "mirroring": {
            "enabled": true, 
            "peers": {
              "secretNames": [
                "fs-secondary-site-secret"
              ]
            }, 
            "snapshotSchedules": [
              {
                "path": "/", 
                "interval": "1h"
              }
            ], 
            "snapshotRetention": [
              {
                "path": "/", 
                "duration": "h 1"
              }
            ]
          }
        }
      }'
      ```
    </Tab>

    <Tab label="Вывод">
      ```bash
      cephfilesystem.ceph.rook.io/<fs-name> patched
      ```
    </Tab>
  </Tabs>

  **Параметры**:

  - `<fs-name>`: Имя пула файлового хранилища.
  - `<schedule-interval>`: Цикл выполнения снимка. Для получения дополнительных сведений см. [официальную документацию](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-schedules).
  - `<retention-policy>`: Политика хранения снимков. Для получения дополнительных сведений см. [официальную документацию](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-retention-policies).

  ### Развертывание демона Mirror в первичном кластере

  Демон Mirror постоянно отслеживает изменения данных в пуле файлового хранилища (с включенным зеркалированием). Он периодически создает снимки и отправляет разности снимков в резервный кластер по сети.

  Выполните следующие команды на контрольном узле первичного кластера:

  <Tabs>
    <Tab label="Команда">
      ```yaml
      cat << EOF | kubectl apply -f -
      apiVersion: ceph.rook.io/v1
      kind: CephFilesystemMirror
      metadata:
        name: cephfs-mirror
        namespace: rook-ceph
      spec:
        placement:
          tolerations:
          - key: NoSchedule
            operator: Exists
        resources:
          limits:
            cpu: "500m"
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"
        priorityClassName: system-node-critical
      EOF
      ```
    </Tab>

    <Tab label="Вывод">
      ```bash
      cephfilesystemmirror.ceph.rook.io/cephfs-mirror created
      ```
    </Tab>
  </Tabs>
</Steps>

## Переключение на резервирование

В случае сбоя первичного кластера вы можете сразу продолжить использовать CephFS в резервном кластере.

### Предварительные требования

- Ресурсы Kubernetes первичного кластера были резервно скопированы и восстановлены в резервном кластере, включая PVC, PV и рабочие нагрузки приложений.
