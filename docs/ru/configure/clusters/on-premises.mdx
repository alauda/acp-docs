---
weight: 20
title: Создание локального кластера
sourceSHA: eac53df4fa3274ff6635e9b301e013fd01f958fe4e72c72ef445b765d88d660b
---

# Создание локального кластера

## Предварительные требования

### Требования к узлам

1. Если вы скачали установочный пакет с одноархитектурной версией из [Загрузить установочный пакет](/install/prepare/download.mdx#download_installation_package), убедитесь, что архитектура ваших узлов соответствует архитектуре пакета. В противном случае узлы не запустятся из-за отсутствия изображений для конкретной архитектуры.
2. Убедитесь, что ваша операционная система узла и ядро поддерживаются. См. [Поддерживаемые ОС и ядра](/install/prepare/prerequisites.mdx#supported_os_and_kernels) для получения подробной информации.
3. Выполните проверки доступности на машинах узлов. Для получения конкретных пунктов проверки обратитесь к [Предварительная обработка узлов > Проверки узлов](/install/prepare/node_preprocessing.mdx#node_checks).
4. Если IP-адреса машин узлов не могут быть доступны напрямую через SSH, предоставьте прокси SOCKS5 для узлов. Кластер `global` будет получать доступ к узлам через этот прокси-сервис.

### Балансировка нагрузки

Для производственных сред требуется балансировщик нагрузки для узлов контрольной плоскости кластера, чтобы обеспечить высокую доступность. Вы можете предоставить свой собственный аппаратный балансировщик нагрузки или включить `Самостроимый VIP`, который предоставляет программную балансировку нагрузки с использованием haproxy + keepalived. Мы рекомендуем использовать аппаратный балансировщик нагрузки, потому что:

- **Лучшая производительность**: Аппаратная балансировка нагрузки работает лучше, чем программная.
- **Низкая сложность**: Если вы не знакомы с keepalived, неправильные конфигурации могут сделать кластер недоступным, что приведет к длительной отладке и серьезно повредит надежности кластера.

При использовании собственного аппаратного балансировщика нагрузки вы можете использовать VIP балансировщика как параметр `IP Address / Domain`. Если у вас есть доменное имя, которое разрешается в VIP балансировщика, вы можете использовать это доменное имя в качестве параметра `IP Address / Domain`. Обратите внимание:

- Балансировщик должен правильно перенаправлять трафик на порты `6443`, `11780` и `11781` на всех узлах контрольной плоскости в кластере.
- Если в вашем кластере есть только один узел контрольной плоскости и вы используете IP этого узла в качестве параметра `IP Address / Domain`, кластер не может быть масштабирован с одного узла до высокодоступной многонодовой конфигурации. Поэтому мы рекомендуем предоставлять балансировщик нагрузки даже для кластеров с одним узлом.

При включении `Самостроимого VIP` вам потребуется подготовить:

1. Доступный VRID
2. Хостовая сеть, поддерживающая протокол VRRP
3. Все узлы контрольной плоскости и VIP должны находиться в одной подсети, и VIP должен отличаться от любого IP-адреса узла.

### Подключение к `global` кластеру и нагрузочному кластеру

Платформа требует взаимного доступа между кластером `global` и нагрузочными кластерами. Если они не находятся в одной сети, вам необходимо:

1. Обеспечить `Внешний доступ` для нагрузочного кластера, чтобы гарантировать, что кластер `global` сможет получить доступ к нему. Сетевые требования должны гарантировать, что `global` имеет доступ к портам `6443`, `11780` и `11781` на всех узлах контрольной плоскости.
2. Добавить дополнительный адрес в `global`, который может быть доступен нагрузочному кластеру. При создании нагрузочного кластера добавьте этот адрес в аннотации кластера с ключом `cpaas.io/platform-url` и значением, установленным на публичный доступный адрес `global`.

### Регистр изображений

Изображения кластера поддерживают варианты встроенного платформенного, частного и публичного репозитория.

- **Платформенное встроенное**: Использует реестр изображений, предоставленный кластером `global`. Если кластер не может получить доступ к `global`, смотрите [Добавление внешнего адреса для встроенного реестра](/configure/clusters/how-to/add_external_addr_for_global_registry.mdx).
- **Частный репозиторий**: Использует ваш собственный реестр изображений. Для получения информации о загрузке необходимых изображений в ваш реестр обратитесь в техническую поддержку.
- **Публичный репозиторий**: Использует публичный реестр изображений платформы. Перед использованием завершите [Обновление учетных данных публичного репозитория](/configure/clusters/how-to/update_public_repository_credentials.mdx).

### Сетевая инфраструктура контейнеров

Если вы планируете использовать Kube-OVN's Underlay для вашего кластера, обратитесь к [Подготовка физической сети Kube-OVN Underlay](/configure/networking/how_to/kubeovn_underlay_py.mdx).

## Процедура создания

1. Перейдите в представление **Управление платформой**, и в левом навигационном меню выберите **Кластеры/Кластеры**.

2. Нажмите **Создать кластер**.

3. Настройте следующие разделы в соответствии с приведенными ниже инструкциями: Основная информация, Сетевая инфраструктура, Настройки узлов и Дополнительные параметры.

### Основная информация

<table>
  <tr>
    <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
    <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
  </tr>

  <tr>
    <td><b>Версия Kubernetes</b></td>

    <td>
      Все необязательные версии строго протестированы на стабильность и совместимость.<br />
      <b>Рекомендация:</b> Выберите последнюю версию для оптимальных возможностей и поддержки.
    </td>
  </tr>

  <tr>
    <td><b>Контейнерный движок</b></td>

    <td>
      Containerd предоставляется в качестве стандартного контейнерного движка.<br />
      Если вы предпочитаете использовать Docker в качестве контейнерного движка, обратитесь к [Выбор контейнерного движка](/configure/clusters/how-to/choose_cri.mdx).
    </td>
  </tr>

  <tr>
    <td><b>Протокол сети кластера</b></td>

    <td>
      Поддерживает три режима: однослойный IPv4, однослойный IPv6, двойной стек IPv4/IPv6.<br />
      <b>Примечание:</b> Если вы выберете режим двойного стека, убедитесь, что все узлы имеют правильно настроенные IPv6 адреса; сетевой протокол не может быть изменен после настройки.
    </td>
  </tr>

  <tr>
    <td><b>Конечная точка кластера</b></td>

    <td>
      <code>IP Address / Domain</code>: Введите заранее подготовленное доменное имя или VIP, если доменное имя недоступно.<br />
      <code>Самостроимый VIP</code>: По умолчанию отключен. Включите только в том случае, если вы не предоставили балансировщик нагрузки. При включении установщик автоматически развернет <code>keepalived</code> для поддержки программной балансировки нагрузки.<br />
      <code>Внешний доступ</code>: Введите адрес, доступный извне, подготовленный для кластера, когда он находится не в одной сетевой среде с кластером <code>global</code>.
    </td>
  </tr>
</table>

### Сетевая инфраструктура

<Tabs>
  <Tab label="Kube-OVN">
    Корпоративная облачная нативная система оркестрации контейнерной сети Kubernetes, разработанная Alauda. Она приносит зрелые сетевые возможности из домена OpenStack в Kubernetes, поддерживает многокраевые сетевые управления, традиционную сетевую архитектуру и инфраструктурные соединения, а также сценарии развертывания на границе, обеспечивая большую безопасность контейнерной сети Kubernetes, эффективность управления и производительность.

    <table>
      <tr>
        <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
        <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
      </tr>

      <tr>
        <td><b>Подсеть</b></td>

        <td>
          Также известна как Cluster CIDR, представляет собой сегмент <b>по умолчанию</b> под сети. После создания кластера можно добавить дополнительные подсети.
        </td>
      </tr>

      <tr>
        <td><b>Режим передачи</b></td>

        <td>
          <b>Overlay</b>: Виртуальная сеть, абстрагированная от инфраструктуры, не потребляющая физических сетевых ресурсов. При создании стандартной подсети Overlay все подсети Overlay в кластере используют одну и ту же конфигурацию сетевого интерфейса кластера и сетевого интерфейса узла.<br />
          <b>Underlay</b>: Этот метод передачи основывается на физических сетевых устройствах. Он может напрямую выделять физические сетевые адреса pod, обеспечивая лучшую производительность и соединение с физической сетью. Узлы в подсети Underlay должны иметь несколько сетевых интерфейсов, а интерфейс, используемый для мостовой сетевой инфраструктуры, должен использоваться исключительно для Underlay и не передавать другой трафик, такой как SSH. При создании стандартной подсети Underlay сетевой интерфейс кластера фактически является стандартным интерфейсом для мостовой сетевой инфраструктуры, а сетевой интерфейс узла — это конфигурация сетевого интерфейса узла в мостовой сети.<br />

          - <b>Стандартный шлюз</b>: Физический адрес шлюза сети, который является адресом шлюза для сегмента Cluster CIDR (должен находиться в диапазоне адресов Cluster CIDR).<br />
          - <b>VLAN ID</b>: Идентификатор виртуальной локальной сети (номер VLAN), например, <code>0</code>.<br />
          - <b>Зарезервированные IP</b>: Установите зарезервированные IP-адреса, которые не будут автоматически выделены, такие как IP-адреса в подсети, которые уже используются другими устройствами.
        </td>
      </tr>

      <tr>
        <td><b>Service CIDR</b></td>

        <td>
          Диапазон IP-адресов, используемых службами Kubernetes типа ClusterIP. Не может пересекаться с диапазоном стандартной подсети.
        </td>
      </tr>

      <tr>
        <td><b>Join CIDR</b></td>

        <td>
          В режиме передачи Overlay это диапазон IP-адресов, используемый для связи между узлами и pods. Не может пересекаться со стандартной подсетью или Service CIDR.
        </td>
      </tr>
    </table>
  </Tab>

  <Tab label="Calico">
    Calico — это решение для сетевого управления третьего уровня, которое обеспечивает надежные сетевые соединения для контейнеров.

    <table>
      <tr>
        <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
        <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
      </tr>

      <tr>
        <td><b>Стандартная подсеть</b></td>

        <td>
          Также известна как Cluster CIDR, представляет собой сегмент <b>по умолчанию</b> под сети. После создания кластера можно добавить дополнительные подсети.
        </td>
      </tr>

      <tr>
        <td><b>Service CIDR</b></td>

        <td>
          Диапазон IP-адресов, используемых службами Kubernetes типа ClusterIP. Не может пересекаться с диапазоном стандартной подсети.
        </td>
      </tr>
    </table>
  </Tab>

  <Tab label="Flannel">
    Flannel предоставляет плоскую сетевую среду для всех контейнеров в кластере, давая контейнерам, созданным на разных узлах, уникальный виртуальный IP-адрес по всему кластеру. Подсеть pod равномерно делится между узлами кластера в соответствии с маской, и pods на каждом узле получают IP-адреса из сегмента, выделенного для этого узла. Это улучшает эффективность связи между контейнерами без необходимости учитывать вопросы перевода IP-адресов.

    <table>
      <tr>
        <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
        <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
      </tr>

      <tr>
        <td><b>Cluster CIDR</b></td>

        <td>
          Диапазон IP-адресов, используемых pod, созданными при запуске кластера. Поддерживает установку максимального количества IP-адресов, которые могут быть выделены pod на каждом узле в текущей контейнерной сети.<br />
          <b>Примечание</b>: Платформа автоматически рассчитает максимальное количество узлов, которые может вместить кластер, на основе вышеуказанной конфигурации и отобразит это в подсказке ниже поля ввода.<br />
          <b>Важно</b>: После создания кластера сеть кластера не может быть изменена, поэтому пожалуйста, тщательно планируйте сеть.
        </td>
      </tr>

      <tr>
        <td><b>Service CIDR</b></td>

        <td>
          Диапазон IP-адресов, используемых службами Kubernetes типа ClusterIP. Не может пересекаться с диапазоном контейнерной подсети.
        </td>
      </tr>
    </table>
  </Tab>

  <Tab label="Custom">
    Если вам нужно установить другие сетевые плагины, выберите режим <b>Custom</b>. Вы можете вручную установить сетевые плагины после успешного создания кластера.

    <table>
      <tr>
        <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
        <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
      </tr>

      <tr>
        <td><b>Cluster CIDR</b></td>

        <td>
          Диапазон IP-адресов, используемых pod, созданными при запуске кластера.
        </td>
      </tr>

      <tr>
        <td><b>Service CIDR</b></td>

        <td>
          Диапазон IP-адресов, используемых службами Kubernetes типа ClusterIP. Не может пересекаться с диапазоном контейнерной подсети.
        </td>
      </tr>
    </table>
  </Tab>
</Tabs>

### Настройки узлов

<table>
  <tr>
    <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
    <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
  </tr>

  <tr>
    <td><b>Сетевой интерфейс</b></td>

    <td>
      Название устройства сетевого интерфейса хоста, используемого плагином сетевой инфраструктуры кластера.<br />
      <b>Примечание</b>:<br />

      - При выборе режима передачи Underlay для стандартной подсети Kube-OVN вы должны указать имя сетевого интерфейса, который будет стандартным NIC для мостовой сетевой инфраструктуры.<br />
      - Мониторинг трафика интерфейсов сетевой инфраструктуры платформы по умолчанию распознает трафик на интерфейсах с такими именами, как <code>eth.|en.|wl.*|ww.*</code>. Если вы используете интерфейсы с другой номенклатурой, вам нужно обратиться к [Сбор данных о сети с сетевых интерфейсов с собственными именами](/observability/monitor/how_to/special_network_card_name.mdx) после onboard кластера, чтобы изменить соответствующие ресурсы и обеспечить правильный мониторинг трафика сетевого интерфейса платформой.
    </td>
  </tr>

  <tr>
    <td><b>Имя узла</b></td>

    <td>
      Вы можете выбрать использовать либо IP узла, либо имя хоста в качестве имени узла на платформе.<br />
      <b>Примечание</b>: При выборе имени хоста в качестве имени узла убедитесь, что имена хостов узлов, добавленных в кластер, уникальны.
    </td>
  </tr>

  <tr>
    <td><b>Узлы</b></td>

    <td>
      <b>Добавить узлы</b> в кластер или <b>Восстановить из черновика</b> временно сохраненную информацию об узлах. См. подробные описания параметров для добавления узлов ниже.
    </td>
  </tr>

  <tr>
    <td><b>Тип мониторинга</b></td>

    <td>
      Поддерживает <b>Prometheus</b> и <b>VictoriaMetrics</b>.<br />
      При выборе <b>VictoriaMetrics</b> в качестве компонента мониторинга вы должны настроить <b>Тип развертывания</b>:<br />
      <b>- Развернуть VictoriaMetrics</b>: Развертывает все связанные компоненты, включая <b>VMStorage</b>, <b>VMAlert</b>, <b>VMAgent</b> и т. д.<br />
      <b>- Развернуть VictoriaMetrics Agent</b>: Развертывает только компонент сбора логов, <b>VMAgent</b>. При использовании этого метода развертывания вам необходимо ассоциироваться с экземпляром VictoriaMetrics, уже развернутым на другом кластере в платформе, для предоставления услуг мониторинга для кластера.
    </td>
  </tr>

  <tr>
    <td><b>Мониторинг узлов</b></td>

    <td>
      Выберите узлы для развертывания компонентов мониторинга кластера. Поддерживает выбор вычислительных узлов и узлов контрольной плоскости, позволяющих развертывание приложений.<br />
      Чтобы избежать воздействия на производительность кластера, рекомендуется приоритизировать вычислительные узлы. После успешного создания кластера компоненты мониторинга с типом хранения <b>Локальный том</b> будут развернуты на выбранных узлах.
    </td>
  </tr>
</table>

**Параметры добавления узлов**

<table>
  <tr>
    <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
    <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
  </tr>

  <tr>
    <td><b>Тип</b></td>

    <td>
      <b>Узел контрольной плоскости</b>: Ответственен за запуск компонентов, таких как kube-apiserver, kube-scheduler, kube-controller-manager, etcd, сетевой инфраструктуры контейнера и некоторых компонентов управления платформой в кластере. Когда <b>Применимое для приложений</b> включено, узлы контрольной плоскости также могут использоваться в качестве вычислительных узлов.<br />
      <b>Рабочий узел</b>: Ответственен за размещение бизнес pod, работающих в кластере.
    </td>
  </tr>

  <tr>
    <td><b>IPv4 адрес</b></td>

    <td>
      IPv4 адрес узла. Для кластеров, созданных в режиме внутренней сети, введите <b>приватный IP</b> узла.
    </td>
  </tr>

  <tr>
    <td><b>IPv6 адрес</b></td>

    <td>
      Действителен, когда кластер имеет разрешенный двойной стек IPv4/IPv6. IPv6 адрес узла.
    </td>
  </tr>

  <tr>
    <td><b>Применимо для приложений</b></td>

    <td>
      Действительно, когда <b>Тип узла</b> равен <b>Узел контрольной плоскости</b>. Позволяет ли бизнес-приложениям развертываться на этом узле контрольной плоскости, планируя связанные с бизнесом pods на этот узел.
    </td>
  </tr>

  <tr>
    <td><b>Отображаемое имя</b></td>

    <td>
      Отображаемое имя узла.
    </td>
  </tr>

  <tr>
    <td><b>SSH соединительный IP</b></td>

    <td>
      IP-адрес, который может соединяться с узлом, когда доступ к нему осуществляется через SSH-службу.<br />
      Если вы можете войти в узел, используя <code>ssh \<username>@\<IPv4 адрес узла></code>, этот параметр не требуется; в противном случае введите публичный IP узла или NAT внешний IP, чтобы гарантировать, что кластер <code>global</code> и прокси могут подключаться к узлу через этот IP.
    </td>
  </tr>

  <tr>
    <td><b>Сетевой интерфейс</b></td>

    <td>
      Введите имя сетевого интерфейса, используемого узлом. Приоритет эффективности конфигурации сетевого интерфейса следующий (слева направо, по убывающей):<br />
      <b>Kube-OVN Underlay</b>: Узел NIC > NIC кластера<br />
      <b>Kube-OVN Overlay</b>: Узел NIC > NIC кластера > NIC, соответствующий маршруту по умолчанию узла<br />
      <b>Calico</b>: NIC кластера > NIC, соответствующий маршруту по умолчанию узла<br />
      <b>Flannel</b>: NIC кластера > NIC, соответствующий маршруту по умолчанию узла
    </td>
  </tr>

  <tr>
    <td><b>Связанная мостовая сеть</b></td>

    <td>
      <b>Примечание</b>: При создании кластера конфигурация мостовой сети не поддерживается; эта опция доступна только при <b>добавлении узлов</b> в кластер, в котором уже созданы подсети Underlay.<br />
      Выберите существующую [Добавить мостовую сеть](/configure/networking/functions/subnet_kubeovn_underlay.mdx#kube-ovn_underlay_bridge_network). Если вы не хотите использовать стандартный NIC мостовой сети, вы можете отдельно настроить NIC узла.
    </td>
  </tr>

  <tr>
    <td><b>SSH порт</b></td>

    <td>
      Номер порта службы SSH, например, <code>22</code>.
    </td>
  </tr>

  <tr>
    <td><b>SSH имя пользователя</b></td>

    <td>
      Имя пользователя SSH, должно быть пользователем с правами root, например, <code>root</code>.
    </td>
  </tr>

  <tr>
    <td><b>Прокси</b></td>

    <td>
      Доступ к порту SSH узла через прокси. Когда кластер <code>global</code> не может напрямую получить доступ к узлу, который нужно добавить через SSH (например, когда кластер <code>global</code> и нагрузочный кластер не находятся в одной подсети; IP узла является внутренним IP, который кластер <code>global</code> не может напрямую получить доступ), этот переключатель должен быть включен, и параметры прокси должны быть настроены. После настройки прокси доступ и развертывание узла можно осуществить через прокси.<br />
      <b>Примечание</b>: В настоящее время поддерживается только прокси SOCKS5.<br />
      <b>URL доступа</b>: Адрес прокси-сервера, например, <code>192.168.1.1:1080</code>.<br />
      <b>Имя пользователя</b>: Имя пользователя для доступа к прокси-серверу.<br />
      <b>Пароль</b>: Пароль для доступа к прокси-серверу.
    </td>
  </tr>

  <tr>
    <td><b>SSH аутентификация</b></td>

    <td>
      Метод аутентификации и соответствующая информация для входа на добавленный узел. Опции включают:<br />
      <b>Пароль</b>: Требуется имя пользователя с правами root и соответствующий <b>SSH пароль</b>.<br />
      <b>Ключ</b>: Требуется <b>закрытый ключ</b> с правами root и <b>пароль закрытого ключа</b>.
    </td>
  </tr>

  <tr>
    <td><b>Сохранить черновик</b></td>

    <td>
      Сохраняет текущие настроенные данные в диалоге как черновик и закрывает диалог <b>Добавить узел</b>.<br />
      Не покидая страницу <b>Создать кластер</b>, вы можете выбрать <b>Восстановить из черновика</b>, чтобы открыть диалоговое окно <b>Добавить узел</b> и восстановить конфигурационные данные, сохраненные как черновик.<br />
      <b>Примечание</b>: Данные, восстановленные из черновика, — это последние сохраненные данные черновика.
    </td>
  </tr>
</table>

### Дополнительные параметры

**Примечание**:

- Кроме обязательных конфигураций не рекомендуется устанавливать дополнительные параметры, так как неправильные настройки могут сделать кластер недоступным и не могут быть изменены после создания кластера.

- Если введенный **Ключ** дублирует Ключ по умолчанию, он заменит стандартную конфигурацию.

**Процедура**

1. Нажмите **Дополнительные параметры**, чтобы развернуть область конфигурации дополнительных параметров. Вы можете дополнительно установить следующие параметры для кластера:

<table>
  <tr>
    <th style={{ whiteSpace: 'nowrap' }}>Параметр</th>
    <th style={{ whiteSpace: 'nowrap' }}>Описание</th>
  </tr>

  <tr>
    <td><b>Параметры Docker</b></td>

    <td>
      <code>dockerExtraArgs</code>, дополнительные параметры конфигурации для Docker, которые будут записаны в <code>/etc/sysconfig/docker</code>. Изменение не рекомендуется. Чтобы настроить Docker через файл <code>daemon.json</code>, он должен быть сконфигурирован как пары ключ-значение.
    </td>
  </tr>

  <tr>
    <td><b>Параметры Kubelet</b></td>

    <td>
      <code>kubeletExtraArgs</code>, дополнительные параметры конфигурации для Kubelet.<br />
      <b>Примечание</b>: Когда вводится параметр <b>Количество IP узлов</b> в <b>Сетевой инфраструктуре</b>, автоматически создается стандартная конфигурация <b>Параметра Kubelet</b> с ключом <code>max-pods</code> и значением <b>Количество IP узлов</b>. Это устанавливает максимальное количество pods, которые могут работать на любом узле в кластере. Эта конфигурация не отображается в интерфейсе.<br />
      Добавление новой пары ключ-значение <code>max-pods: максимальное количество запускаемых pods</code> в области <b>Параметры Kubelet</b> заменит стандартное значение. Разрешено любое положительное целое число, но рекомендуется использовать стандартное значение (Количество IP узлов) или вводить значение, не превышающее <code>256</code>.
    </td>
  </tr>

  <tr>
    <td><b>Параметры контроллера управления</b></td>

    <td>
      <code>controllerManagerExtraArgs</code>, дополнительные параметры конфигурации для контроллера управления.
    </td>
  </tr>

  <tr>
    <td><b>Параметры планировщика</b></td>

    <td>
      <code>schedulerExtraArgs</code>, дополнительные параметры конфигурации для планировщика.
    </td>
  </tr>

  <tr>
    <td><b>Параметры APIServer</b></td>

    <td>
      <code>apiServerExtraArgs</code>, дополнительные параметры конфигурации для APIServer.
    </td>
  </tr>

  <tr>
    <td><b>URL APIServer</b></td>

    <td>
      <code>publicAlternativeNames</code>, адреса доступа к APIServer, выданные в сертификате. Можно вводить только IP-адреса или доменные имена, максимум 253 символа.
    </td>
  </tr>

  <tr>
    <td><b>Аннотации кластера</b></td>

    <td>
      Информация аннотаций кластера, обозначающая характеристики кластера в метаданных в виде пар ключ-значение, чтобы компоненты платформы или бизнес-компоненты могли получить соответствующую информацию.
    </td>
  </tr>
</table>

4. Нажмите **Создать**. Вы вернетесь на страницу списка кластеров, где кластер будет находиться в состоянии **Создается**.

## Шаги после создания

### Просмотр хода создания

На странице списка кластеров вы можете просмотреть список созданных кластеров. Для кластеров в состоянии **Создается** можно проверить ход выполнения.

**Процедура**

1. Нажмите на маленькую иконку **Посмотреть ход выполнения** справа от состояния кластера.

2. В диалоговом окне хода выполнения, которое появится, вы можете просмотреть ход выполнения кластера (status.conditions).

   **Совет**: Когда какой-либо тип находится в процессе или в состоянии сбоя с соответствующей причиной, наведите курсор на соответствующую причину (выделенную синем текстом), чтобы увидеть подробную информацию о причине (status.conditions.reason).

### Ассоциация с проектами

После создания кластера вы можете добавить его в проекты в представлении управления проектами.
