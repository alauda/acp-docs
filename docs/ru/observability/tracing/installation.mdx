---
weight: 15
sourceSHA: e6395947f2e1aacd9c23f9cc160a4d6cd3c82e5f5da9416adccfa15729be0489
---

# Установка

:::warning

**Этот документ по развертыванию применим только к сценариям интеграции контейнерной платформы с системой трассировки.**

Компоненты **Tracing** и **Service Mesh** являются взаимно исключающими. Если вы уже развернули компонент Service Mesh, сначала удалите его.

:::

Этот гид предоставляет администраторам кластера процесс установки системы трассировки на кластер Alauda Container Platform.

Предварительные требования:

- У вас есть доступ к кластеру Alauda Container Platform с учетной записью, имеющей разрешения `platform-admin-system`.
- У вас установлен CLI `kubectl`.
- Компонент `Elasticsearch` настроен для хранения данных трассировки, включая URL доступа и информацию о `Basic Auth`.

## Установка оператора Jaeger

### Установка оператора Jaeger с помощью веб-консоли

Вы можете установить оператор Jaeger из раздела **Marketplace** → **OperatorHub** на платформе Alauda Container Platform, где перечислены доступные операторы.

**Шаги**

- В представлении **Управление платформой** веб-консоли выберите **кластер**, в который вы хотите развернуть оператор Jaeger, затем перейдите в **Marketplace** → **OperatorHub**.

- Используйте строку поиска, чтобы найти `Alauda build of Jaeger` в каталоге. Нажмите на заголовок **Alauda build of Jaeger**.

- Ознакомьтесь с вводной информацией об операторе на странице **Alauda build of Jaeger**. Нажмите **Установить**.

- На странице **Установка**:

  - Выберите **Ручной** для **Стратегии обновления**. Для стратегии одобрения `Manual` OLM создаст запросы на обновление. В качестве администратора кластера вы должны вручную одобрить запросы на обновление OLM, чтобы обновить оператор до новой версии.
  - Выберите канал **stable (по умолчанию)**.
  - Выберите **Рекомендуемое** для **Местоположения установки**. Установите оператор в рекомендуемом пространстве имен `jaeger-operator`, чтобы оператор мог мониторить и быть доступным во всех пространствах имен внутри кластера.

- Нажмите **Установить**.

- Убедитесь, что **Статус** отображается как **Успешно**, чтобы подтвердить, что оператор Jaeger был установлен правильно.

- Проверьте, что все компоненты оператора Jaeger были успешно установлены. Войдите в кластер через терминал и выполните следующую команду:

  ```bash
  kubectl -n jaeger-operator get csv
  ```

  **Пример вывода**

  ```bash
  NAME                     DISPLAY           VERSION   REPLACES   PHASE
  jaeger-operator.vx.x.0   Jaeger Operator   x.x.0                Успешно
  ```

  Если поле `PHASE` показывает `Успешно`, это означает, что оператор и его компоненты были успешно установлены.

## Развертывание экземпляра Jaeger

Экземпляр Jaeger и его связанные ресурсы могут быть установлены с помощью скрипта `install-jaeger.sh`, который принимает три параметра:

- `--es-url`: URL доступа к Elasticsearch.
- `--es-user-base64`: Имя пользователя `Basic Auth` для Elasticsearch, закодированное в base64.
- `--es-pass-base64`: Пароль `Basic Auth` для Elasticsearch, закодированный в base64.

Скопируйте скрипт установки из **ДЕТАЛИ**, войдите в кластер, в который вы хотите его установить, сохраните его как `install-jaeger.sh` и выполните его после предоставления прав на выполнение:

:::details

```bash
#!/bin/bash

set -euo pipefail

# get arg
while [ "$#" -gt 0 ]; do
    case $1 in
        --es-url=*)
            ES_URL="${1#*=}"
            ;;
        --es-user-base64=*)
            ES_USER_BASE64="${1#*=}"
            ;;
        --es-pass-base64=*)
            ES_PASS_BASE64="${1#*=}"
            ;;
        *)
            echo "unknown argument: $1"
            exit 1
            ;;
    esac
    shift
done

# print arg
echo "ES_URL: $ES_URL"
echo "ES_USER_BASE64: $ES_USER_BASE64"
echo "ES_PASS_BASE64: $ES_PASS_BASE64"

# get global-info from ConfigMap
CLUSTER_NAME=$(kubectl get configmap global-info -n kube-public -o jsonpath='{.data.clusterName}')
echo "CLUSTER_NAME: ${CLUSTER_NAME}"
ISSUER_URL=$(kubectl get configmap global-info -n kube-public -o jsonpath='{.data.oidcIssuer}')
CLIENT_ID=$(kubectl get configmap global-info -n kube-public -o jsonpath='{.data.oidcClientID}')
CLIENT_SECRET=$(kubectl get configmap global-info -n kube-public -o jsonpath='{.data.oidcClientSecret}')
CLIENT_SECRET_BASE64=$(echo -n "${CLIENT_SECRET}" | base64 -w0)
PLATFORM_URL=$(kubectl get configmap global-info -n kube-public -o jsonpath='{.data.platformURL}')
echo "PLATFORM_URL: ${PLATFORM_URL}"

TARGET_NAMESPACE="cpaas-system"
JAEGER_BASEPATH="clusters/$CLUSTER_NAME/acp/jaeger"

_apply_resource() {
  if [ -z "$1" ]; then
    echo "Usage: _apply_resource <yaml_content>"
    return 1
  fi

  local yaml_content="$1"
  echo "$yaml_content" | kubectl apply -f -
}

_install_configmap() {
  local yaml_content=$(cat <<EOF
apiVersion: v1
data:
  OAUTH2_PROXY_CLIENT_ID: $CLIENT_ID
  OAUTH2_PROXY_COOKIE_SECURE: "false"
  OAUTH2_PROXY_EMAIL_DOMAINS: "*"
  OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
  OAUTH2_PROXY_INSECURE_OIDC_ALLOW_UNVERIFIED_EMAIL: "true"
  OAUTH2_PROXY_OIDC_ISSUER_URL: $ISSUER_URL
  OAUTH2_PROXY_PROVIDER: oidc
  OAUTH2_PROXY_PROXY_PREFIX: /$JAEGER_BASEPATH/oauth2
  OAUTH2_PROXY_REDIRECT_URL: $PLATFORM_URL/$JAEGER_BASEPATH/oauth2/callback
  OAUTH2_PROXY_SCOPE: openid profile email groups ext
  OAUTH2_PROXY_SKIP_JWT_BEARER_TOKENS: "true"
  OAUTH2_PROXY_SKIP_PROVIDER_BUTTON: "true"
  OAUTH2_PROXY_SSL_INSECURE_SKIP_VERIFY: "true"
  OAUTH2_PROXY_UPSTREAMS: http://127.0.0.1:16686
kind: ConfigMap
metadata:
  name: jaeger-oauth2-proxy
  namespace: $TARGET_NAMESPACE
EOF
)
  _apply_resource "$yaml_content"
}

_install_secret() {
  local yaml_content=$(cat <<EOF
apiVersion: v1
data:
  OAUTH2_PROXY_CLIENT_SECRET: $CLIENT_SECRET_BASE64
  OAUTH2_PROXY_COOKIE_SECRET: $CLIENT_SECRET_BASE64
kind: Secret
metadata:
  name: jaeger-oauth2-proxy
  namespace: $TARGET_NAMESPACE
type: Opaque
---
apiVersion: v1
data:
  ES_PASSWORD: $ES_PASS_BASE64
  ES_USERNAME: $ES_USER_BASE64
kind: Secret
metadata:
  name: jaeger-elasticsearch-basic-auth
  namespace: $TARGET_NAMESPACE
type: Opaque
EOF
)
  _apply_resource "$yaml_content"
}

_install_sa() {
  local yaml_content=$(cat <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jaeger-prod-acp
  namespace: $TARGET_NAMESPACE
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: jaeger-prod-acp
  namespace: $TARGET_NAMESPACE
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: jaeger-prod-acp
  namespace: $TARGET_NAMESPACE
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: jaeger-prod-acp
subjects:
- kind: ServiceAccount
  name: jaeger-prod-acp
  namespace: $TARGET_NAMESPACE
EOF
)
  _apply_resource "$yaml_content"
}

_install_jaeger() {
  local yaml_content=$(cat <<EOF
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger-prod
  namespace: $TARGET_NAMESPACE
spec:
  collector:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: jaeger-prod-collector
            topologyKey: kubernetes.io/hostname
    replicas: 1
    resources:
      limits:
        cpu: "2"
        memory: 512Mi
      requests:
        cpu: 250m
        memory: 256Mi
  imagePullSecrets:
    - name: global-registry-auth
  ingress:
    enabled: false
  labels:
    service_name: jaeger
  query:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: jaeger-prod-query
            topologyKey: kubernetes.io/hostname
    annotations:
      oauth2-proxy.github.io/image: ""
      oauth2-proxy.github.io/inject: "true"
      oauth2-proxy.github.io/oidc-configmap: jaeger-oauth2-proxy
      oauth2-proxy.github.io/oidc-secret: jaeger-oauth2-proxy
      oauth2-proxy.github.io/proxyCPULimit: 100m
      oauth2-proxy.github.io/proxyMemoryLimit: 128Mi
    options:
      query:
        base-path: /$JAEGER_BASEPATH
    replicas: 1
    resources:
      limits:
        cpu: "1"
        memory: 512Mi
      requests:
        cpu: 250m
        memory: 256Mi
  resources:
    limits:
      cpu: 100m
      memory: 300Mi
    requests:
      cpu: 100m
      memory: 300Mi
  sampling:
    options: {}
  serviceAccount: jaeger-prod-acp
  storage:
    dependencies:
      enabled: false
      resources: {}
      schedule: 55 23 * * *
    elasticsearch:
      name: elasticsearch
      nodeCount: 3
      redundancyPolicy: SingleRedundancy
    esIndexCleaner:
      enabled: true
      numberOfDays: 7
      resources: {}
      schedule: 55 23 * * *
    esRollover:
      resources: {}
      schedule: 0 0 * * *
    options:
      es.asm.cname: jaeger-elasticsearch-basic-auth
      es.asm.cnamespace: $TARGET_NAMESPACE
      es.index-prefix: acp-tracing-$CLUSTER_NAME
      es.max-span-age: 168h0m0s
      es.server-urls: $ES_URL
      es.tls.enabled: true
      es.tls.skip-host-verify: true
    secretName: ""
    type: elasticsearch
  strategy: production
  tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
  ui:
    options:
      dependencies:
        menuEnabled: false
EOF
)
  _apply_resource "$yaml_content"
}

_install_pod_monitor() {
  local yaml_content=$(cat <<EOF
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  labels:
    monitoring: pods
    prometheus: kube-prometheus
  name: jaeger-monitor
  namespace: $TARGET_NAMESPACE
spec:
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames:
      - $TARGET_NAMESPACE
  podMetricsEndpoints:
    - interval: 60s
      path: /metrics
      port: admin-http
  selector:
    matchLabels:
      app.kubernetes.io/instance: jaeger-prod
EOF
)
  _apply_resource "$yaml_content"
}

_install_ingress() {
  local alb_annotation=""
  if [[ "$CLUSTER_NAME" == "global" ]]; then
  alb_annotation=$(cat <<EOF
    alb.ingress.cpaas.io/rewrite-request: |
      {"headers_var":{"Authorization":"cookie_cpaas_id_token"}}
EOF
  )
  fi
  local ingress_class=""
  if [[ "$CLUSTER_NAME" != "global" ]]; then
    ingress_class="  ingressClassName: cpaas-system"
  fi

  local yaml_content=$(cat <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jaeger-query
  namespace: $TARGET_NAMESPACE
  annotations:
    nginx.ingress.kubernetes.io/enable-cors: "true"
$alb_annotation
spec:
$ingress_class
  rules:
    - http:
        paths:
          - path: /$JAEGER_BASEPATH
            pathType: ImplementationSpecific
            backend:
              service:
                name: jaeger-prod-query
                port:
                  number: 4180
EOF
)
  _apply_resource "$yaml_content"
}

# final install
_install_configmap
_install_secret
_install_sa
_install_jaeger
_install_pod_monitor
_install_ingress

echo "Установка Jaeger завершена"
```

:::

**Пример выполнения скрипта**:

```bash
./install-jaeger.sh --es-url='https://xxx' --es-user-base64='xxx' --es-pass-base64='xxx'
```

**Пример вывода скрипта**:

```bash
ES_URL: https://xxx
ES_USER_BASE64: xxx
ES_PASS_BASE64: xxx
CLUSTER_NAME: cluster-xxx
PLATFORM_URL: https://xxx
INSTALLED_CSV: jaeger-operator.vx.x.x
OAUTH2_PROXY_IMAGE: build-harbor.alauda.cn/3rdparty/oauth2-proxy/oauth2-proxy:vx.x.x
configmap/jaeger-oauth2-proxy created
secret/jaeger-oauth2-proxy created
secret/jaeger-elasticsearch-basic-auth created
serviceaccount/jaeger-prod-acp created
role.rbac.authorization.k8s.io/jaeger-prod-acp created
rolebinding.rbac.authorization.k8s.io/jaeger-prod-acp created
jaeger.jaegertracing.io/jaeger-prod created
podmonitor.monitoring.coreos.com/jaeger-monitor created
ingress.networking.k8s.io/jaeger-query created
Установка Jaeger завершена
```

## Установка оператора OpenTelemetry

### Установка оператора OpenTelemetry с помощью веб-консоли

Вы можете установить оператор OpenTelemetry из раздела **Marketplace** → **OperatorHub** на платформе Alauda Container Platform, где перечислены доступные операторы.

**Шаги**

- В представлении **Управление платформой** веб-консоли выберите **кластер**, в который вы хотите развернуть оператор OpenTelemetry, затем перейдите в **Marketplace** → **OperatorHub**.

- Используйте строку поиска, чтобы найти `Alauda build of OpenTelemetry` в каталоге. Нажмите на заголовок **Alauda build of OpenTelemetry**.

- Ознакомьтесь с вводной информацией об операторе на странице **Alauda build of OpenTelemetry**. Нажмите **Установить**.

- На странице **Установка**:

  - Выберите **Ручной** для **Стратегии обновления**. Для стратегии одобрения `Manual` OLM создаст запросы на обновление. В качестве администратора кластера вы должны вручную одобрить запросы на обновление OLM, чтобы обновить оператор до новой версии.
  - Выберите канал **alpha (по умолчанию)**.
  - Выберите **Рекомендуемое** для **Местоположения установки**. Установите оператор в рекомендуемом пространстве имен `opentelemetry-operator`, чтобы оператор мог мониторить и быть доступным во всех пространствах имен внутри кластера.

- Нажмите **Установить**.

- Убедитесь, что **Статус** отображается как **Успешно**, чтобы подтвердить, что оператор OpenTelemetry был установлен правильно.

- Проверьте, что все компоненты оператора OpenTelemetry были успешно установлены. Войдите в кластер через терминал и выполните следующую команду:

  ```bash
  kubectl -n opentelemetry-operator get csv
  ```

  **Пример вывода**

  ```bash
  NAME                            DISPLAY                  VERSION   REPLACES   PHASE
  openTelemetry-operator.vx.x.0   OpenTelemetry Operator   x.x.0                Успешно
  ```

  Если поле `PHASE` показывает `Успешно`, это означает, что оператор и его компоненты были успешно установлены.

## Развертывание экземпляров OpenTelemetry

Экземпляры OpenTelemetry и их связанные ресурсы могут быть установлены с помощью скрипта `install-otel.sh`.

Скопируйте скрипт установки из **ДЕТАЛИ**, войдите в кластер, в который вы хотите его установить, сохраните его как `install-otel.sh` и выполните его после предоставления прав на выполнение:

:::details

```bash
#!/bin/bash

set -euo pipefail

TARGET_NAMESPACE="cpaas-system"

# get global-info from ConfigMap
CLUSTER_NAME=$(kubectl get configmap global-info -n kube-public -o jsonpath='{.data.clusterName}')
echo "CLUSTER_NAME: ${CLUSTER_NAME}"

_apply_resource() {
  if [ -z "$1" ]; then
    echo "Usage: _apply_resource <yaml_content>"
    return 1
  fi

  local yaml_content="$1"
  echo "$yaml_content" | kubectl apply -f -
}

_install_rbac() {
  local yaml_content=$(cat <<EOF
apiVersion: v1
imagePullSecrets:
  - name: global-registry-auth
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: $TARGET_NAMESPACE
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector:cpaas-system:cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: otel-collector
    namespace: $TARGET_NAMESPACE
EOF
)
  _apply_resource "$yaml_content"
}

_install_otel_collector() {
  local yaml_content=$(cat <<EOF
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: $TARGET_NAMESPACE
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: otel-collector
          topologyKey: kubernetes.io/hostname
  config:
    exporters:
      debug: {}
      otlp:
        balancer_name: round_robin
        endpoint: dns:///jaeger-prod-collector-headless.$TARGET_NAMESPACE:4317
        tls:
          insecure: true
      prometheus:
        endpoint: 0.0.0.0:8889
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
    processors:
      batch: {}
      filter/metric_apis:
        metrics:
          datapoint:
            - attributes["http.route"] == "/actuator/health" or attributes["uri"] == "/actuator/health"
            - attributes["http.route"] == "/actuator/prometheus" or attributes["uri"] == "/actuator/prometheus"
      transform:
        metric_statements:
          - context: datapoint
            statements:
              - delete_key(attributes, "inner.client.ms.name")
              - delete_key(attributes, "inner.client.ms.namespace")
              - delete_key(attributes, "inner.client.cluster.name")
              - delete_key(attributes, "inner.client.env.type")
              - set(attributes["namespace"], resource.attributes["k8s.namespace.name"])
              - set(attributes["container"], resource.attributes["k8s.container.name"])
              - set(attributes["service_name"], resource.attributes["service.name"])
              - set(attributes["pod"], resource.attributes["k8s.pod.name"])
      memory_limiter:
        check_interval: 5s
        limit_percentage: 85
        spike_limit_percentage: 25
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    service:
      extensions:
        - health_check
      pipelines:
        metrics:
          exporters:
            - debug
            - prometheus
          processors:
            - memory_limiter
            - filter/metric_apis
            - transform
            - batch
          receivers:
            - otlp
        traces:
          exporters:
            - debug
            - otlp
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
      telemetry:
        logs:
          level: info
        metrics:
          address: 0.0.0.0:8888
          level: detailed
  managementState: managed
  mode: deployment
  replicas: 1
  resources:
    limits:
      cpu: "2"
      memory: 1Gi
    requests:
      cpu: 250m
      memory: 512Mi
  securityContext:
    readOnlyRootFilesystem: true
    runAsNonRoot: true
  serviceAccount: otel-collector
  tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
  upgradeStrategy: automatic
EOF
)
  _apply_resource "$yaml_content"
}

_install_instrumentation() {
  local yaml_content=$(cat <<EOF
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: acp-common-java
  namespace: $TARGET_NAMESPACE
spec:
  env:
    - name: SERVICE_CLUSTER
      value: "$CLUSTER_NAME"
    - name: OTEL_TRACES_EXPORTER
      value: otlp
    - name: OTEL_METRICS_EXPORTER
      value: otlp
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: http://otel-collector.$TARGET_NAMESPACE:4317
    - name: OTEL_SERVICE_NAME
      value: \$(SERVICE_NAME).\$(SERVICE_NAMESPACE)
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: service.namespace=\$(SERVICE_NAMESPACE),cluster.name=\$(SERVICE_CLUSTER)
  sampler:
    type: parentbased_traceidratio
    argument: "1"
EOF
)
  _apply_resource "$yaml_content"
}

_install_service_monitor() {
  local yaml_content=$(cat <<EOF
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    monitoring: services
    prometheus: kube-prometheus
  name: otel-collector-monitoring
  namespace: $TARGET_NAMESPACE
spec:
  endpoints:
    - interval: 60s
      path: /metrics
      port: monitoring
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames:
      - $TARGET_NAMESPACE
  selector:
    matchLabels:
      app.kubernetes.io/instance: $TARGET_NAMESPACE.otel
      operator.opentelemetry.io/collector-service-type: monitoring
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    monitoring: services
    prometheus: kube-prometheus
  name: otel-collector
  namespace: $TARGET_NAMESPACE
spec:
  endpoints:
    - honorLabels: true
      interval: 60s
      path: /metrics
      port: prometheus
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames:
      - $TARGET_NAMESPACE
  selector:
    matchLabels:
      app.kubernetes.io/instance: $TARGET_NAMESPACE.otel
      operator.opentelemetry.io/collector-service-type: base
EOF
)
  _apply_resource "$yaml_content"
}

_install_rbac
_install_otel_collector
_install_instrumentation
_install_service_monitor

echo "Установка OpenTelemetry завершена"
```

:::

**Пример выполнения скрипта**:

```bash
./install-otel.sh
```

**Пример вывода скрипта**:

```bash
CLUSTER_NAME: cluster-xxx
serviceaccount/otel-collector created
clusterrolebinding.rbac.authorization.k8s.io/otel-collector:cpaas-system:cluster-admin created
opentelemetrycollector.opentelemetry.io/otel created
instrumentation.opentelemetry.io/acp-common-java created
servicemonitor.monitoring.coreos.com/otel-collector-monitoring created
servicemonitor.monitoring.coreos.com/otel-collector created
Установка OpenTelemetry завершена
```

## Включение переключателя функций

Система трассировки в настоящее время находится на стадии **Alpha** и требует, чтобы вы вручную включили переключатель функции `acp-tracing-ui` в представлении **Переключатель функций**.

Затем перейдите в представление **Контейнерная платформа** и перейдите в **Наблюдаемость** → **Трассировка**, чтобы просмотреть функцию трассировки.

## Удаление трассировки

### Удаление экземпляра OpenTelemetry

Войдите в установленный кластер и выполните следующие команды для удаления экземпляра OpenTelemetry и его связанных ресурсов.

```bash
kubectl -n cpaas-system delete servicemonitor otel-collector-monitoring
kubectl -n cpaas-system delete servicemonitor otel-collector
kubectl -n cpaas-system delete instrumentation acp-common-java
kubectl -n cpaas-system delete opentelemetrycollector otel
kubectl delete clusterrolebinding otel-collector:cpaas-system:cluster-admin
kubectl -n cpaas-system delete serviceaccount otel-collector
```

### Удаление оператора OpenTelemetry

Вы можете удалить оператор OpenTelemetry с помощью представления **Управление платформой** в веб-консоли.

**Шаги**

- Из **Marketplace** → **OperatorHub** → используйте **строку поиска**, чтобы найти `Alauda build of OpenTelemetry`.
- Нажмите на заголовок **Alauda build of OpenTelemetry**, чтобы войти в его детали.
- На странице деталей **Alauda build of OpenTelemetry** нажмите кнопку **Удалить** в правом верхнем углу.
- В окне **Удалить "opentelemetry-operator"?** нажмите **Удалить**.

### Удаление экземпляра Jaeger

Войдите в установленный кластер и выполните следующие команды для удаления экземпляра Jaeger и его связанных ресурсов.

```bash
kubectl -n cpaas-system delete ingress jaeger-query
kubectl -n cpaas-system delete podmonitor jaeger-monitor
kubectl -n cpaas-system delete jaeger jaeger-prod
kubectl -n cpaas-system delete rolebinding jaeger-prod-acp
kubectl -n cpaas-system delete role jaeger-prod-acp
kubectl -n cpaas-system delete serviceaccount jaeger-prod-acp
kubectl -n cpaas-system delete secret jaeger-oauth2-proxy
kubectl -n cpaas-system delete secret jaeger-elasticsearch-basic-auth
kubectl -n cpaas-system delete configmap jaeger-oauth2-proxy
```

### Удаление оператора Jaeger

Вы можете удалить оператор Jaeger с помощью представления **Управление платформой** в веб-консоли.

**Шаги**

- Из **Marketplace** → **OperatorHub** → используйте **строку поиска**, чтобы найти `Alauda build of Jaeger`.
- Нажмите на заголовок **Alauda build of Jaeger**, чтобы войти в его детали.
- На странице деталей **Alauda build of Jaeger** нажмите кнопку **Удалить** в правом верхнем углу.
- В окне **Удалить "jaeger-operator"?** нажмите **Удалить**.
