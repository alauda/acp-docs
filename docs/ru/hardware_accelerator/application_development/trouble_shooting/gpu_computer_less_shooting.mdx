---
weight: 10
sourceSHA: 8693c4a00fdfa2433cf6c7bf52c31dcd29af8adc33e648a6e05886da1b7b66ac
---

# Устранение неполадок: ошибка "float16 поддерживается только на графических процессорах с вычислительной мощностью как минимум xx" в vLLM

## Описание проблемы

### Окружение

- **Аппаратное обеспечение**: графические процессоры NVIDIA с вычислительной мощностью \<8.0 (например, Tesla V100, T4)
- **Типы моделей**: LLM, требующие точности bfloat16/FP8 (например, LLaMA-2-70B, GPT-NeoX-20B)

### Симптомы

1. Явное сообщение об ошибке:
   ```
   ValueError: float16/bfloat16 поддерживается только на графических процессорах с вычислительной мощностью как минимум 8.0
   ```
2. Неудачная компиляция ядра во время загрузки модели

### Связанные журналы

```bash
# стек вызовов ошибок vLLM
Файл "/usr/local/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/__init__.py", строка 37, в _verify_cuda_compute_capability
    вызвать ValueError(
ValueError: bfloat16 поддерживается только на графических процессорах с вычислительной мощностью как минимум 8.0. Текущий графический процессор: Tesla V100-PCIE-16GB, вычислительная мощность 7.0
```

## Коренная причина

### Основная причина

**Недостаточная вычислительная мощность GPU**
Вычислительная мощность GPU (CC) не соответствует минимальным требованиям для определенных типов данных:

- **bfloat16/FP8**: Требуется CC ≥8.0 (архитектура Ampere или новее)
- **Оптимизация тензорных ядер FP16**: Требуется CC \≥7.0 (архитектура Volta или новее)

### Технический анализ

1. **Ограничения архитектуры**:
   - Графические процессоры до архитектуры Ampere (CC \<8.0) не имеют выделенных единиц матричных вычислений для операций bfloat16
   - Тензорные ядра в Volta/Turing (CC 7.0-7.5) поддерживают только смешанную точность FP16/FP32

2. **Обязанности фреймворка**:
   ```python
   # проверка возможностей vLLM (упрощенная)
   def _verify_cuda_compute_capability():
       if device.compute_capability < MIN_REQUIRED_CC:
           raise ValueError(f"Требуется вычислительная мощность ≥{MIN_REQUIRED_CC}")
   ```

## Устранение неполадок

### Шаг 1: Проверьте вычислительную мощность GPU

```python
import torch
print(f"Вычислительная мощность: {torch.cuda.get_device_capability()}")
```

### Шаг 2: Проверьте требования к точности модели

```bash
cat model/config.json | grep "torch_dtype"
# Ожидаемый вывод: "bfloat16" или "float16"
```

### Шаг 3: Проверьте совместимость фреймворка

```python
from vllm import _is_cuda_compute_capability_compatible as compat
print(f"bfloat16 поддерживается: {compat((8,0))}")
```

## Решение

### Решение для недостаточной вычислительной мощности

#### Соображения

- Ожидается ухудшение производительности при снижении точности
- Точность модели может варьироваться в зависимости от различных типов точности

#### Предварительные условия

- CUDA Toolkit ≥11.8

#### Шаги

1. **Измените yaml InferenceService**:
   добавьте аргументы, такие как --dtype=half
   ```yaml
   apiVersion: serving.kserve.io/v1beta1
   kind: InferenceService
   metadata:
     name: llama-2-service
     annotations:
       serving.kserve.io/enable-prometheus-scraping: "true"
   spec:
     predictor:
       containers:
       - name: kserve-container
         image: vllm/vllm-serving:0.3.2
         args:
           - --model=meta-llama/Llama-2-7b-chat-hf
           - --dtype=half  # Принудительное использование точности FP16
           - --tensor-parallel-size=1
         resources:
           limits:
             nvidia.com/gpu: "1"
   ```
2. **Подождите перезагрузки развертывания**

## Профилактические меры

1. **Предварительные проверки**:
   ```python
   from vllm import LLM
   LLM.validate_environment(model_dtype="bfloat16")
   ```

2. **Конфигурация кластера**:
   ```bash
   # конфигурация плагина устройства NVIDIA
   helm upgrade -i nvidia-device-plugin \
     --set compatabilityPolicy=strict \
     --set computeCapabilities=8.0+
   ```

3. **Оптимизация модели**:
   ```python
   # Применение квантования AWQ
   llm = LLM(model="codellama/CodeLlama-34b",
             quantization="awq",
             load_format="awq")
   ```

## Связанный контент

### Справочник по вычислительной мощности GPU

| Архитектура | Диапазон CC | Поддерживаемые точности       |
| ------------ | ----------- | ------------------------------ |
| Volta        | 7.0-7.2     | Тензорное ядро FP16           |
| Turing       | 7.5         | FP16/INT8                     |
| Ampere       | 8.0-8.9     | bfloat16/TF32/FP8             |
| Hopper       | 9.0+        | FP4/FP8 с динамическим масштабом |

### Официальные ссылки

1. [Таблица вычислительной мощности NVIDIA](https://developer.nvidia.com/cuda-gpus)
2. [Требования к аппаратному обеспечению vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html)

```
```
