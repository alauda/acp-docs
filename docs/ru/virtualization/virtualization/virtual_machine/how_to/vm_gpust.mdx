---
weight: 50
sourceSHA: 65d6354ced772598f14622f8391d8196e047690c4dd1117ae17d6e80c8fb1eeb
---

# Подготовка среды физического прохода GPU

Физический проход GPU в виртуальных машинах относится к процессу прямого выделения фактического графического процессора (GPU) для виртуальной машины в рамках виртуализационной среды. Это позволяет виртуальной машине напрямую получать доступ и использовать физический GPU, достигая графической производительности, эквивалентной той, которая достигается при запуске на физической машине. Это позволяет избежать узких мест производительности, вызванных виртуальными графическими адаптерами, и таким образом повышает общую производительность.

## Ограничения и лимитации

Функциональность физического прохода GPU требует использования kubevirt-gpu-device-plugin; однако в настоящее время нет доступного образа ARM64 для kubevirt-gpu-device-plugin, что означает, что эта функциональность не может быть использована в операционной системе с архитектурой процессора ARM64.

## Предварительные требования

### Подготовка Chart и образов

Получите следующие Chart и образы и загрузите их в репозиторий образов. Этот документ использует `build-harbor.example.cn` в качестве адреса репозитория. Для получения конкретного метода получения Chart и образов, пожалуйста, свяжитесь с соответствующими специалистами.

**Chart**

- build-harbor.example.cn/example/chart-gpu-operator:v23.9.1

**Образы**

- build-harbor.example.cn/3rdparty/nvidia/gpu-operator:v23.9.0
- build-harbor.example.cn/3rdparty/nvidia/cloud-native/gpu-operator-validator:v23.9.0
- build-harbor.example.cn/3rdparty/nvidia/cuda:12.3.1-base-ubi8
- build-harbor.example.cn/3rdparty/nvidia/kubevirt-gpu-device-plugin:v1.2.4
- build-harbor.example.cn/3rdparty/nvidia/nfd/node-feature-discovery:v0.14.2

### Включение IOMMU

Процедура включения IOMMU различается в зависимости от операционной системы. Пожалуйста, обратитесь к документации соответствующей операционной системы. Этот документ использует CentOS в качестве примера, и все команды следует выполнять в терминале.

1. Отредактируйте файл `/etc/default/grub` и добавьте `intel_iommu=on iommu=pt` к параметру конфигурации `GRUB_CMDLINE_LINUX`.

   ```
   GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=centos/root rhgb quiet intel_iommu=on iommu=pt"
   ```

2. Выполните следующую команду для генерации файла `grub.cfg`.

   ```
   grub2-mkconfig -o /boot/grub2/grub.cfg
   ```

3. Перезапустите сервер.

4. Выполните следующую команду, чтобы подтвердить, что IOMMU был успешно включен. Если вывод содержит `IOMMU enabled`, это означает, что он был успешно включен.

   ```
   dmesg | grep -i iommu
   ```

## Операционные действия

**Примечание**: Все команды ниже должны выполняться в инструменте CLI на соответствующем узле Master кластера, если не указано иное.

### Создание пространства имен

Выполните следующую команду для создания пространства имен с именем `gpu-system`. Если вывод отображает `namespace/gpu-system created`, это означает, что создание прошло успешно.

```
kubectl create ns gpu-system
```

### Развертывание gpu-оператора

1. Выполните следующую команду для развертывания gpu-оператора.

   ```
   export REGISTRY=<registry> # Замените <registry> на адрес репозитория, где находится образ gpu-оператора, например: export REGISTRY=build-harbor.example.cn
     
   cat <<EOF | kubectl create -f -
   apiVersion: operator.alauda.io/v1alpha1
   kind: AppRelease
   metadata:
     annotations:
       auto-recycle: "true"
       interval-sync: "true"
     name: gpu-operator
     namespace: gpu-system
   spec:
     destination:
       cluster: ""
       namespace: "gpu-operator"
     source:
       charts:
       - name: <chartName> # Замените <chartName> на фактический путь к chart, например: name = example/chart-gpu-operator
         releaseName: gpu-operator
         targetRevision: v23.9.1
       repoURL: $REGISTRY
     timeout: 120
     values:
       global:
         registry:
           address: $REGISTRY
       nfd:
         enabled: true
       sandboxWorkloads:
         enabled: true
         defaultWorkload: "vm-passthrough"
   EOF
   ```

2. Выполните следующую команду, чтобы проверить, синхронизирован ли gpu-оператор. Если `SYNC` показывает `Synced`, это означает, что он синхронизировался успешно.

   ```
   kubectl -n gpu-system get apprelease gpu-operator
   ```

   Информация для вывода:

   ```
   NAME           SYNC           HEALTH        MESSAGE        UPDATE   AGE
   gpu-operator   Synced         Ready         chart synced   28s      32s
   ```

3. Выполните следующую команду, чтобы получить имена всех узлов и найти имя узла GPU.

   ```
   kubectl get nodes -o wide
   ```

4. Выполните следующую команду, чтобы проверить, есть ли у узла GPU какой-либо графический процессор, способный к проходу. Если вывод содержит информацию о GPU, аналогичную `nvidia.com/GK210GL_TESLA_K80`, это означает, что есть графические процессоры, способные к проходу.

   ```
   kubectl get node <gpu-node-name> -o jsonpath='{.status.allocatable}' # Замените <gpu-node-name> на имя узла GPU, полученное на этапе 3
   ```

   Информация для вывода:

   ```
   {"cpu":"39","devices.kubevirt.io/kvm":"1k","devices.kubevirt.io/tun":"1k","devices.kubevirt.io/vhost-net":"1k","ephemeral-storage":"426562784165","hugepages-1Gi":"0","hugepages-2Mi":"0","memory":"122915848Ki","nvidia.com/GK210GL_TESLA_K80":"8","pods":"256"}
   ```

5. На этом этапе gpu-оператор был успешно развернут.

### Настройка Kubevirt

1. Выполните следующую команду, чтобы включить функцию DisableMDEVConfiguration. Если возвращается сообщение, подобное `hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged patched`, это означает, что включение прошло успешно.

   ```
   kubectl patch hco kubevirt-hyperconverged -n kubevirt --type='json' -p='[{"op": "add", "path": "/spec/featureGates/disableMDevConfiguration", "value": true }]'
   ```

2. В терминале узла GPU выполните следующую команду, чтобы получить pciDeviceSelector. Часть `10de:102d` в выводе - это значение pciDeviceSelector. \{#pciDeviceSelector}

   ```
   lspci -nn | grep -i nvidia
   ```

   Информация для вывода:

   ```
   04:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   05:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   08:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   09:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   85:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   86:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   89:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   8a:00.0 3D controller [0302]: NVIDIA Corporation GK210GL [Tesla K80] [10de:102d] (rev a1)
   ```

3. Выполните следующую команду, чтобы получить имена всех узлов и найти имя узла GPU.

   ```
   kubectl get nodes -o wide
   ```

4. Выполните следующую команду, чтобы получить resourceName. Часть `nvidia.com/GK210GL_TESLA_K80` в выводе - это значение resourceName.

   ```
   kubectl get node <gpu-node-name> -o jsonpath='{.status.allocatable}' # Замените <gpu-node-name> на имя узла GPU, полученное на этапе 3
   ```

   Информация для вывода:

   ```
   {"cpu":"39","devices.kubevirt.io/kvm":"1k","devices.kubevirt.io/tun":"1k","devices.kubevirt.io/vhost-net":"1k","ephemeral-storage":"426562784165","hugepages-1Gi":"0","hugepages-2Mi":"0","memory":"122915848Ki","nvidia.com/GK210GL_TESLA_K80":"8","pods":"256"}
   ```

5. Выполните следующую команду, чтобы добавить графический процессор с проходом.

   **Примечание**: При замене части \<pci-devices-id> в команде ниже на значение pciDeviceSelector, полученное на [Этапе 2](#pciDeviceSelector), **все буквы в pciDeviceSelector должны быть преобразованы в верхний регистр**. Например, если полученное значение pciDeviceSelector равно `10de:102d`, его следует заменить на `export DEVICE=10DE:102D`.

   - Добавление одного графического процессора

     ```
     export DEVICE=<pci-devices-id> # Замените <pci-devices-id> на pciDeviceSelector, полученный на этапе 2, например: export DEVICE=10DE:102D
     export RESOURCE=<resource-name> # Замените <resource-name> на resourceName, полученное на этапе 4, например: export RESOURCE=nvidia.com/GK210GL_TESLA_K80
      
     kubectl patch hco kubevirt-hyperconverged -n kubevirt --type='json' -p='
     [
       {
         "op": "add",
         "path": "/spec/permittedHostDevices",
         "value": {
           "pciHostDevices": [
             {
               "externalResourceProvider": true,
               "pciDeviceSelector": "'"$DEVICE"'",
               "resourceName": "'"$RESOURCE"'"
             }
           ]
         }
       }
     ]'
     ```

   - Добавление нескольких графических процессоров

     **Примечание**: При добавлении нескольких графических процессоров каждое значение pciDeviceSelector, использующееся для замены \<pci-devices-id>, должно быть уникальным.

     ```
     export DEVICE1=<pci-devices-id1> # Замените <pci-devices-id1> на pciDeviceSelector, полученный на этапе 2
     export RESOURCE1=<resource-name1> # Замените <resource-name1> на resourceName, полученное на этапе 4
     export DEVICE2=<pci-devices-id2> # Замените <pci-devices-id2> на pciDeviceSelector, полученный на этапе 2
     export RESOURCE2=<resource-name2> # Замените <resource-name2> на resourceName, полученное на этапе 4
      
     kubectl patch hco kubevirt-hyperconverged -n kubevirt --type='json' -p='
     [
       {
         "op": "add",
         "path": "/spec/permittedHostDevices",
         "value": {
           "pciHostDevices": [
             {
               "externalResourceProvider": true,
               "pciDeviceSelector": "'"$DEVICE"'",
               "resourceName": "'"$RESOURCE"'"
             },
             {
               "externalResourceProvider": true,
               "pciDeviceSelector": "'"$DEVICE2"'",
               "resourceName": "'"$RESOURCE2"'"
             }
           ]
         }
       }
     ]'
     ```

   - Добавление новых графических процессоров после уже добавления графических процессоров

     ```
     export DEVICE=<pci-devices-id> # Замените <pci-devices-id> на pciDeviceSelector, полученный на этапе 2
     export RESOURCE=<resource-name> # Замените <resource-name> на resourceName, полученное на этапе 4
     export INDEX=<index> # index - это индекс массива с нуля, используйте число для замены <index>, например: если уже был добавлен один графический процессор и теперь вы хотите добавить еще один, индекс должен быть 1, т.е. export INDEX=1
       
     kubectl patch hco kubevirt-hyperconverged -n kubevirt --type='json' -p='
     [
       {
         "op": "add",
         "path": "/spec/permittedHostDevices/pciHostDevices/'"${INDEX}"'",
         "value": {
           "externalResourceProvider": true,
           "pciDeviceSelector": "'"$DEVICE"'",
           "resourceName": "'"$RESOURCE"'"
         }
       }
     ]'
     ```

## Проверка результата \{#result}

После завершения вышеуказанных шагов конфигурации, если соответствующий физический GPU можно выбрать при создании виртуальной машины, это указывает на то, что среда физического прохода GPU была успешно подготовлена.

**Примечание**: Если необходимо настроить физический проход GPU, пожалуйста, включите соответствующие функции заранее.

1. Перейдите в **Container Platform**.

2. В левом навигационном меню нажмите **Виртуализация** > **Виртуальные машины**.

3. Нажмите **Создать виртуальную машину**.

4. Настройте параметр **Физический GPU (Alpha)** для виртуальной машины.

   | Параметр                  | Описание                                                                                                     |
   | ------------------------ | ------------------------------------------------------------------------------------------------------------- |
   | **Физический GPU (Alpha)** | Выберите модель настроенного физического GPU. Один физический GPU может быть назначен только одной виртуальной машине. |

5. На этом этапе среда физического прохода GPU была успешно подготовлена.

## Связанные операции

### Удаление виртуальной машины с GPU проходом

1. Перейдите в **Container Platform**.

2. В левом навигационном меню нажмите **Виртуализация** > **Виртуальные машины**.

3. На странице списка нажмите ⋮ справа от виртуальной машины, которую нужно удалить > **Удалить**, или нажмите название виртуальной машины, чтобы перейти на страницу ее детальной информации, и нажмите **Действия** > **Удалить**.

4. Введите подтверждающую информацию для удаления виртуальной машины с GPU проходом.

### Удаление конфигурации, связанной с GPU, из KubeVirt

1. На соответствующем узле Master кластера для GPU используйте инструмент CLI, чтобы выполнить следующую команду для удаления конфигурации, связанной с GPU, из KubeVirt.

   ```
   kubectl patch hco kubevirt-hyperconverged -n kubevirt --type='json' -p='[{"op": "remove", "path": "/spec/permittedHostDevices"}]'
   ```

2. После удаления, если невозможно выбрать соответствующую модель физического GPU при создании виртуальной машины через **Container Platform**, это указывает на то, что удаление прошло успешно. Пожалуйста, обратитесь к [Выбор модели физического GPU](#result) для получения конкретных шагов по созданию виртуальной машины.

### Удаление gpu-оператора

1. Используйте инструмент CLI на соответствующем узле Master кластера для GPU для выполнения следующей команды для удаления gpu-оператора.

   ```
   kubectl -n gpu-system delete apprelease gpu-operator
   ```

   Информация для вывода:

   ```
   apprelease.operator.alauda.io "gpu-operator" deleted
   ```

2. Выполните команду, и если вы получите ответ, аналогичный приведенному ниже, это указывает на то, что gpu-оператор был успешно удален.

   ```
   kubectl -n gpu-system get apprelease gpu-operator
   ```

   Информация для вывода:

   ```
   Error from server (NotFound): appreleases.operator.alauda.io "gpu-operator" not found
   ```
