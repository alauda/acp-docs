---
weight: 10
sourceSHA: 9a5efdb8f266efdf02bf1049bca90ec21a25bda492f6a70a4a86a8a096a8660a
---

# Миграция подов и восстановление после аномального завершения работы узлов виртуальной машины

## Описание проблемы

Независимо от того, было ли узло **гладко завершено** или произошло **аномальное завершение работы**, поды виртуальной машины, работающие на данном узле, не будут автоматически мигрироваться на другие работоспособные узлы.

## Анализ причин

Платформа реализует решение виртуальной машины на основе компонента с открытым исходным кодом KubeVirt. Однако с точки зрения KubeVirt она не может отличить фактическое завершение работы виртуальной машины от сбоя подключения, вызванного сетевыми или другими проблемами. Если виртуальные машины будут мигрировать на другие узлы без разбора, это может привести к тому, что одновременно будут существовать несколько экземпляров одной и той же виртуальной машины.

## Решения

При обслуживании узлов виртуальных машин необходимо выполнять ручные действия в соответствии с данным документом. Как для ситуации **гладкого завершения работы**, так и для ситуации **аномального завершения работы**, поды виртуальной машины должны быть вручную выселены или насильно удалены.

**Примечание**: Следующие команды должны выполняться на главном узле соответствующего кластера.

### Миграция подов виртуальной машины при гладком завершении работы

1. В инструменте CLI выполните следующую команду, чтобы получить информацию об узле. Поле `NAME` в возвращенной информации - это `Node-Name`.

   ```bash
   kubectl get nodes
   ```

   Вывод:

   ```bash
   NAME             STATUS   ROLES                  AGE   VERSION
   1.1.1.211   Ready    control-plane,master   99d   v1.28.8
   ```

2. (По желанию) Выполните следующую команду, чтобы просмотреть экземпляры виртуальных машин на узле.

   ```bash
   kubectl get vmis --all-namespaces -o wide | grep <Node-Name>  # Замените <Node-Name> в команде на Node-Name, полученный на шаге 1
   ```

   Вывод:

   ```bash
   test-test         vm-t-export-clone   13d     Running      1.1.1.1   1.1.1.211   True    False   
   ```

3. Перед гладким завершением работы выполните следующую команду, чтобы выселить все поды виртуальной машины на узле, который будет завершён. Если вывод выглядит следующим образом, это означает, что выселение прошло успешно.

   ```bash
   kubectl drain <Node-Name> --delete-local-data --ignore-daemonsets=true --force --pod-selector=kubevirt.io=virt-launcher   # Замените <Node-Name> в команде на Node-Name узла, который будет завершен
   ```

   Вывод:

   ```bash
   Флаг --delete-local-data устарел, этот параметр будет удалён. Используйте --delete-emptydir-data.
   node/1.1.1.211 cordoned
   evicting pod test-test/virt-launcher-vm-t-export-clone-hmnkk
   pod/virt-launcher-vm-t-export-clone-hmnkk evicted
   node/1.1.1.211 drained
   ```

4. После того как все виртуальные машины будут запущены на других узлах, завершите работу узла.

5. После завершения работы и перезагрузки узла выполните следующую команду, чтобы пометить узел как доступный для планирования.

   ```bash
   kubectl uncordon <Node-Name> # Замените <Node-Name> в команде на Node-Name узла, который был завершен и перезагружен
   ```

   Вывод:

   ```bash
   node/1.1.1.211 uncordoned
   ```

6. На этом этапе первоначальные экземпляры виртуальных машин на этом узле были мигрированы на другие работоспособные узлы, и этот узел теперь доступен для нового планирования подов после перезагрузки.

### Восстановление после аномального завершения работы

1. В инструменте CLI выполните следующую команду, чтобы получить информацию об узле. Поле `NAME` в возвращенной информации - это `Node-Name`.

   ```bash
   kubectl get nodes
   ```

   Вывод:

   ```bash
   NAME             STATUS   ROLES                  AGE   VERSION
   1.1.1.211   Ready    control-plane,master   99d   v1.28.8
   ```

2. Выполните следующую команду, чтобы насильно удалить все поды виртуальной машины на узле.

   ```bash
   kubectl get po -A -l kubevirt.io=virt-launcher -o wide | grep <Node-Name> | awk '{print "kubectl delete pod --force -n " $1, $2}'  | bash  # Замените <Node-Name> в команде на Node-Name узла, который закончил свою работу аномально.
   ```

3. Выполните следующую команду, чтобы удалить присоединения томов на этом узле.

   ```bash
   kubectl get volumeattachments.storage.k8s.io | grep <Node-Name> | awk '{print $1}' | xargs kubectl delete volumeattachments.storage.k8s.io  # Замените <Node-Name> в команде на Node-Name узла, который закончил свою работу аномально.
   ```

4. Выполните следующую команду, чтобы проверить, есть ли на узле, который закончил свою работу аномально, поды с меткой kubevirt.io=virt-api.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-api -o wide | grep <Node-Name> # Замените <Node-Name> в команде на Node-Name узла, который закончил свою работу аномально.
   ```

   Если они существуют, выполните следующую команду для их удаления.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-api -o name | xargs kubectl -n kubevirt delete --force --grace-period=0
   ```

5. Выполните следующую команду, чтобы проверить, есть ли на узле, который закончил свою работу аномально, поды с меткой kubevirt.io=virt-controller.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-controller -o wide | grep <Node-Name> # Замените <Node-Name> в команде на Node-Name узла, который закончил свою работу аномально.
   ```

   Если они существуют, выполните следующую команду для их удаления.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-controller -o name | xargs kubectl -n kubevirt delete --force --grace-period=0
   ```

6. На этом этапе экземпляры виртуальных машин будут мигрированы на другие работоспособные узлы после аномального завершения работы узла.
