---
weight: 10
sourceSHA: 9a5efdb8f266efdf02bf1049bca90ec21a25bda492f6a70a4a86a8a096a8660a
---

# Миграция подов и восстановление после ненормального завершения работы узлов виртуальной машины

## Описание проблемы

Независимо от того, был ли узел **корректно выключен** или произошел **ненормальный сбой**, поды виртуальной машины, работающие на этом узле, не будут автоматически мигрированы на другие здоровые узлы.

## Анализ причины

Платформа реализует решение для виртуальных машин на основе компонента с открытым исходным кодом KubeVirt. Однако, с точки зрения KubeVirt, она не может различить фактический сбой виртуальной машины и сбой соединения, вызванный сетевыми или другими проблемами. Если виртуальные машины мигрировать на другие узлы без разбора, это может привести к появлению нескольких экземпляров одной и той же виртуальной машины одновременно.

## Решения

При обслуживании узлов виртуальных машин необходимо выполнять действия вручную в соответствии с этим документом. Как в ситуациях **корректного завершения**, так и при **ненормальном сбое** поды виртуальных машин должны быть вручную выселены или принудительно удалены.

**Примечание**: Следующие команды должны выполняться на мастер-узле соответствующего кластера.

### Миграция подов виртуальных машин при корректном завершении

1. В инструменте CLI выполните следующую команду, чтобы получить информацию об узле. Поле `NAME` в возвращенной информации — это `Node-Name`.

   ```bash
   kubectl get nodes
   ```

   Вывод:

   ```bash
   NAME             STATUS   ROLES                  AGE   VERSION
   1.1.1.211   Ready    control-plane,master   99d   v1.28.8
   ```

2. (Опционально) Выполните следующую команду, чтобы просмотреть экземпляры виртуальных машин на узле.

   ```bash
   kubectl get vmis --all-namespaces -o wide | grep <Node-Name>  # Замените <Node-Name> в команде на Node-Name, полученный на шаге 1
   ```

   Вывод:

   ```bash
   test-test         vm-t-export-clone   13d     Running      1.1.1.1   1.1.1.211   True    False   
   ```

3. Перед корректным завершением работы выполните следующую команду, чтобы выселить все поды виртуальных машин на узле, который должен быть выключен. Если вывод выглядит следующим образом, это указывает на успешное выселение.

   ```bash
   kubectl drain <Node-Name> --delete-local-data --ignore-daemonsets=true --force --pod-selector=kubevirt.io=virt-launcher   # Замените <Node-Name> в команде на Node-Name узла, который должен быть выключен
   ```

   Вывод:

   ```bash
   Flag --delete-local-data has been deprecated, This option is deprecated and will be deleted. Use --delete-emptydir-data.
   node/1.1.1.211 cordoned
   evicting pod test-test/virt-launcher-vm-t-export-clone-hmnkk
   pod/virt-launcher-vm-t-export-clone-hmnkk evicted
   node/1.1.1.211 drained
   ```

4. После того как все виртуальные машины будут запущены на других узлах, выключите узел.

5. После того как узел будет выключен и перезагружен, выполните следующую команду, чтобы пометить узел как доступный для планирования.

   ```bash
   kubectl uncordon <Node-Name> # Замените <Node-Name> в команде на Node-Name выключенного и перезагрузившегося узла
   ```

   Вывод:

   ```bash
   node/1.1.1.211 uncordoned
   ```

6. На этом этапе оригинальные экземпляры виртуальных машин на этом узле были мигрированы на другие здоровые узлы, и этот узел теперь доступен для нового планирования подов после перезагрузки.

### Восстановление после ненормального завершения работы

1. В инструменте CLI выполните следующую команду, чтобы получить информацию об узле. Поле `NAME` в возвращенной информации — это `Node-Name`.

   ```bash
   kubectl get nodes
   ```

   Вывод:

   ```bash
   NAME             STATUS   ROLES                  AGE   VERSION
   1.1.1.211   Ready    control-plane,master   99d   v1.28.8
   ```

2. Выполните следующую команду для принудительного удаления всех подов виртуальных машин на узле.

   ```bash
   kubectl get po -A -l kubevirt.io=virt-launcher -o wide | grep <Node-Name> | awk '{print "kubectl delete pod --force -n " $1, $2}'  | bash  # Замените <Node-Name> в команде на Node-Name узла, который испытывал ненормальный сбой.
   ```

3. Выполните следующую команду, чтобы удалить объемные подключения на этом узле.

   ```bash
   kubectl get volumeattachments.storage.k8s.io | grep <Node-Name> | awk '{print $1}' | xargs kubectl delete volumeattachments.storage.k8s.io  # Замените <Node-Name> в команде на Node-Name узла, который испытывал ненормальный сбой.
   ```

4. Выполните следующую команду, чтобы проверить, есть ли поды с ярлыком kubevirt.io=virt-api на узле, который испытывал ненормальный сбой.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-api -o wide | grep <Node-Name> # Замените <Node-Name> в команде на Node-Name узла, который испытывал ненормальный сбой.
   ```

   Если они существуют, выполните следующую команду для удаления подов.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-api -o name | xargs kubectl -n kubevirt delete --force --grace-period=0
   ```

5. Выполните следующую команду, чтобы проверить, есть ли поды с ярлыком kubevirt.io=virt-controller на узле, который испытывал ненормальный сбой.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-controller -o wide | grep <Node-Name> # Замените <Node-Name> в команде на Node-Name узла, который испытывал ненормальный сбой.
   ```

   Если они существуют, выполните следующую команду для удаления подов.

   ```bash
   kubectl -n kubevirt get po -l kubevirt.io=virt-controller -o name | xargs kubectl -n kubevirt delete --force --grace-period=0
   ```

6. На этом этапе экземпляры виртуальных машин будут мигрированы на другие здоровые узлы после того, как узел испытал ненормальное завершение работы.
