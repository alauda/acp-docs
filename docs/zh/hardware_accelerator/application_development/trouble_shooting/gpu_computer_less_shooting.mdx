---
weight: 10
sourceSHA: 8693c4a00fdfa2433cf6c7bf52c31dcd29af8adc33e648a6e05886da1b7b66ac
---

# 排查 vLLM 中仅支持计算能力至少为 xx 的 float16 错误

## 问题描述

### 环境

- **硬件**: NVIDIA GPU，计算能力 \<8.0（例如，Tesla V100、T4）
- **模型类型**: 需要 bfloat16/FP8 精度的 LLM（例如，LLaMA-2-70B、GPT-NeoX-20B）

### 症状

1. 明确的错误信息：
   ```
   ValueError: float16/bfloat16 仅在计算能力至少为 8.0 的 GPU 上受支持
   ```
2. 在模型加载期间内核编译失败

### 相关日志

```bash
# vLLM 错误堆栈跟踪
File "/usr/local/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/__init__.py", line 37, in _verify_cuda_compute_capability
    raise ValueError(
ValueError: bfloat16 仅在计算能力至少为 8.0 的 GPU 上受支持。当前 GPU: Tesla V100-PCIE-16GB，计算能力 7.0
```

## 根本原因

### 主要原因

**GPU 计算能力不足**
GPU 的计算能力（CC）未满足特定数据类型的最低要求：

- **bfloat16/FP8**: 需要 CC ≥8.0（安培架构及更新）
- **FP16 张量核心优化**: 需要 CC \≥7.0（伏打架构及更新）

### 技术分析

1. **架构限制**：
   - 前安培 GPU（CC \<8.0）缺乏用于 bfloat16 操作的专用矩阵数学单元
   - 伏打/图灵中的张量核心（CC 7.0-7.5）仅支持 FP16/FP32 混合精度

2. **框架强制性**：
   ```python
   # vLLM 的能力检查（简化版）
   def _verify_cuda_compute_capability():
       if device.compute_capability < MIN_REQUIRED_CC:
           raise ValueError(f"需要计算能力 ≥{MIN_REQUIRED_CC}")
   ```

## 故障排查

### 第一步：验证 GPU 计算能力

```python
import torch
print(f"计算能力: {torch.cuda.get_device_capability()}")
```

### 第二步：检查模型精度要求

```bash
cat model/config.json | grep "torch_dtype"
# 预期输出: "bfloat16" 或 "float16"
```

### 第三步：验证框架兼容性

```python
from vllm import _is_cuda_compute_capability_compatible as compat
print(f"支持 bfloat16: {compat((8,0))}")
```

## 解决方案

### 针对计算能力不足的解决方案

#### 注意事项

- 降低精度时预计性能下降
- 不同精度类型可能导致模型准确性变化

#### 前提条件

- CUDA 工具包 ≥11.8

#### 步骤

1. **修改 InferenceService yaml**：
   添加参数如 --dtype=half
   ```yaml
   apiVersion: serving.kserve.io/v1beta1
   kind: InferenceService
   metadata:
     name: llama-2-service
     annotations:
       serving.kserve.io/enable-prometheus-scraping: "true"
   spec:
     predictor:
       containers:
       - name: kserve-container
         image: vllm/vllm-serving:0.3.2
         args:
           - --model=meta-llama/Llama-2-7b-chat-hf
           - --dtype=half  # 强制 FP16 精度
           - --tensor-parallel-size=1
         resources:
           limits:
             nvidia.com/gpu: "1"
   ```
2. **等待部署重启**

## 预防措施

1. **前期检查**：
   ```python
   from vllm import LLM
   LLM.validate_environment(model_dtype="bfloat16")
   ```

2. **集群配置**：
   ```bash
   # NVIDIA 设备插件配置
   helm upgrade -i nvidia-device-plugin \
     --set compatabilityPolicy=strict \
     --set computeCapabilities=8.0+
   ```

3. **模型优化**：
   ```python
   # 应用 AWQ 量化
   llm = LLM(model="codellama/CodeLlama-34b",
             quantization="awq",
             load_format="awq")
   ```

## 相关内容

### GPU 计算能力参考

| 架构        | CC 范围  | 支持的精度                   |
| ----------- | -------- | ---------------------------- |
| 伏打        | 7.0-7.2  | FP16 张量核心                |
| 图灵        | 7.5      | FP16/INT8                    |
| 安培        | 8.0-8.9  | bfloat16/TF32/FP8            |
| 霍普尔      | 9.0+     | FP4/FP8 动态缩放            |

### 官方参考

1. [NVIDIA 计算能力表](https://developer.nvidia.com/cuda-gpus)
2. [vLLM 硬件要求](https://docs.vllm.ai/en/latest/getting_started/installation.html)
