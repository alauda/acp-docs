---
weight: 62
sourceSHA: 6c72abd503ba410aca04462fa2567480d600311e0101ca3f8859b5ca902d02c5
---

# 清理分布式存储

如果需要删除 rook-ceph 集群并重新部署一个新的集群，请按照本文依次清理与分布式存储服务相关的资源。

## 注意事项

在清理 rook-ceph 之前，请确保所有使用 Ceph 存储的 PVC 和 PV 资源已被删除。

## 操作步骤

<Steps>

### 删除快照类

1. 删除快照类。

    ```
    kubectl delete VolumeSnapshotClass csi-cephfs-snapshotclass csi-rbd-snapshotclass
    ```
    
2. 验证快照类是否已清理完毕。

    ```
    kubectl get VolumeSnapshotClass | grep csi-cephfs-snapshotclass
    kubectl get VolumeSnapshotClass | grep csi-rbd-snapshotclass
    ```
    当这两个命令没有任何输出时，表示清理完毕。

### 删除存储类

1. 进入 **平台管理**。

2. 在左侧导航栏中，单击 **存储管理** > **存储类**。

3. 单击 ⋮ > **删除**，删除所有使用 Ceph 存储解决方案的存储类。

### 删除存储池

此步骤需要在上一个步骤完成后执行。

1. 进入 **平台管理**。

2. 在左侧导航栏中，单击 **存储管理** > **分布式存储**。

3. 在 **存储池区域**，单击 ⋮ > **删除**，逐个删除所有存储池。当存储池区域显示 **无存储池** 时，说明存储池已成功删除。

4. （可选）如果集群模式为 **延伸**，还需在集群的 Master 节点上执行以下命令，以删除创建的内置存储池。

    ```
    kubectl -n rook-ceph delete cephblockpool -l cpaas.io/builtin=true
    ```
    
    回显信息：
    
    ```
    cephblockpool.ceph.rook.io "builtin-mgr" deleted
    ```

### 删除 ceph-cluster

此步骤需要在上一个步骤完成后执行。

1. 更新 ceph-cluster，并启用清理策略。

    ```
    kubectl -n rook-ceph patch cephcluster ceph-cluster --type merge -p '{"spec":{"cleanupPolicy":{"confirmation":"yes-really-destroy-data"}}}'
    ```

2. 删除 ceph-cluster。

    ```
    kubectl delete cephcluster ceph-cluster -n rook-ceph
    ```

3. 删除执行清理的作业。

    ```
    kubectl delete jobs --all -n rook-ceph
    ```

4. 验证 ceph-cluster 清理是否完毕。

    ``` 
    kubectl get cephcluster -n rook-ceph | grep ceph-cluster
    ```

    当该命令没有任何输出时，表示清理完毕。

### 删除 rook-operator

此步骤需要在上一个步骤完成后执行。

1. 删除 rook-operator。

    ```
    kubectl -n rook-ceph delete subscriptions.operators.coreos.com rook-operator
    ```

2. 验证 rook-operator 清理是否完毕。

    ```
    kubectl get subscriptions.operators.coreos.com -n rook-ceph | grep rook-operator
    ```
    
    当该命令没有任何输出时，表示清理完毕。

3. 验证 ConfigMap 是否清理完毕。

    ```
    kubectl get configmap -n rook-ceph
    ```
    
    当该命令没有任何输出时，表示清理完毕。如果有输出结果，请执行以下命令清理，替换 `<configmap>` 为实际输出。

    ```
    kubectl -n rook-ceph patch configmap <configmap> --type merge -p '{"metadata":{"finalizers": []}}'
    ```

4. 验证 Secret 是否清理完毕。

    ```
    kubectl get secret -n rook-ceph
    ```
    
    当该命令没有任何输出时，表示清理完毕。如果有输出结果，请执行以下命令清理，替换 `<secret>` 为实际输出。

    ```
    kubectl -n rook-ceph patch secrets <secret> --type merge -p '{"metadata":{"finalizers": []}}'
    ```

5. 验证 rook-ceph 清理是否完毕。

    ```
    kubectl get all -n rook-ceph
    ```

    当该命令没有任何输出时，表示清理完毕。

### 执行清理脚本

完成上述步骤后，表示 Kubernetes 和 Ceph 相关资源已被清空，接下来需要清理宿主机上残留的 rook-ceph。

#### 清理脚本

清理脚本 clean-rook.sh 的内容如下：

<details> 
<summary>点击查看</summary>
```shell
#!/bin/bash

DISK="$1"

if [ ! -n "$DISK" ]
then
   echo "you must input block dev"
   exit
else
   echo "are you sure to clean device: $DISK ? yes/no"
   read ANSWER
   case $ANSWER in
     [Yy]*)
     echo " you input is y or Y !"
     ;;
     [Nn]*)
     echo " you input a "$ANSWER
     exit
     ;;
   esac
fi

echo "clean /var/lib/rook"
rm -rf /var/lib/rook

echo "clean block dev"

# fdisk 重建分区表并添加分区，中间空行不可删除
echo "g
n



w" | sudo fdisk $DISK

# 将磁盘重置为可用状态（zap-all 是重要的，因为 MBR 必须干净）
# 您需要为所有磁盘运行此步骤。
sgdisk --zap-all $DISK

# 用 dd 清理 HDD
dd if=/dev/zero of="$DISK" bs=1M count=100 oflag=direct,dsync

# 用 blkdiscard 而不是 dd 清理 SSD
blkdiscard $DISK

# 这些步骤只需在每个节点上运行一次
# 如果 rook 使用 ceph-volume 设置 OSD，拆除将保留一些映射设备，使磁盘被锁定。
ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %

# ceph-volume setup 可能在 /dev 中留下 ceph-<UUID> 目录（不必要的杂项）
rm -rf /dev/ceph-*
```
</details>

#### 注意事项

清理脚本依赖于 sgdisk 命令，请确保在执行清理脚本之前已安装该命令。

* Ubuntu 安装命令：`sudo apt install gdisk`
* RedHat 或 CentOS 安装命令：`sudo yum install gdisk`

#### 操作步骤

<Steps>

1. 在每台部署分布式存储的业务集群机器上执行清理脚本 clean-rook.sh。

    ```
    sh clean-rook.sh /dev/[设备名称]
    ```

    示例：`sh clean-rook.sh /dev/vdb`

    执行时将提示确认是否真正清空该设备。如果确认，输入 yes 即可开始清理。

2. 使用 `lsblk -f` 命令检查分区信息。当该命令的输出中 `FSTYPE` 列为空时，表示清理完毕。

</Steps>
</Steps>
