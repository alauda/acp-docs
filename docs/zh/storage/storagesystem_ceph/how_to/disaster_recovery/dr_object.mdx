---
weight: 66
sourceSHA: 03834cc32f46c1e5cff17043b66ff305859c581f126cf0a3756b22625a08053b
---

# 对象存储灾难恢复

Ceph RGW 的 Multi-Site 功能是一种跨集群的异步数据复制机制，旨在在地理分布的 Ceph 集群之间同步对象存储数据，提供高可用性（HA）和灾难恢复（DR）能力。

## 术语表

| 术语                   | 解释                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| :--------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **主集群**             | 当前提供存储服务的集群。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| **备用集群**           | 用于备份目的的待命集群。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **Realm、ZoneGroup、Zone** | <ul><li>Realm：Ceph 对象存储中的最高级逻辑分组，表示一个完整的对象存储命名空间，通常用于多站点复制和同步。一个 Realm 可以跨不同的地理位置或数据中心。</li><li>ZoneGroup：Realm 内的逻辑分组，包含多个 Zone。ZoneGroups 使得在 Zones 之间进行数据同步和复制成为可能，通常在同一地理区域内使用。</li><li>Zone：ZoneGroup 内的逻辑分组，实际存储数据。每个 Zone 独立管理和存储对象，并可以拥有自己的数据/元数据池配置。</li></ul> |

## 前提条件

- 准备两个可用于部署 Rook-Ceph 的集群（主集群和备用集群），并确保它们之间具有网络连接。
- 两个集群必须使用相同的平台版本（v3.12 或更高版本）。
- 确保在主集群和备用集群上均未部署 Ceph 对象存储。
- 请参考 [创建存储服务](../../installation/create_service_stand.mdx) 文档，部署 Operator 并创建集群。创建集群后，**请勿通过向导继续创建对象存储池**，而应按照下面描述的方式使用 CLI 工具进行配置。

## 操作步骤

本指南提供了在同一 ZoneGroup 下，两 Zones 之间的同步解决方案。

<Steps>

  ### 在主集群中创建对象存储

  此步骤创建 Realm、ZoneGroup、主 Zone 和主 Zone 的网关资源。

  在主集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```yaml
      cat << EOF | kubectl apply -f -
      ---
      apiVersion: ceph.rook.io/v1
      kind: CephObjectRealm
      metadata:
        name: <realm-name>
        namespace: rook-ceph
        
      ---
      apiVersion: ceph.rook.io/v1
      kind: CephObjectZoneGroup
      metadata:
        name: <zonegroup-name>
        namespace: rook-ceph
      spec:
        realm: <realm-name>

      ---
      apiVersion: ceph.rook.io/v1
      kind: CephObjectZone
      metadata:
        name: <primary-zone-name>
        namespace: rook-ceph
      spec:
        zoneGroup: <zonegroup-name>
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
            requireSafeReplicaSize: true
        dataPool:
          failureDomain: host
          replicated:
            size: 3
            requireSafeReplicaSize: true
          parameters:
            compression_mode: none
        preservePoolsOnDelete: false

      ---
      cat << EOF | kubectl apply -f -
      apiVersion: ceph.rook.io/v1
      kind: CephObjectStore
      metadata:
        name: <object-store-name>
        namespace: rook-ceph
      spec:
        gateway:
          port: 7480
          instances: 2
        zone:
          name: <zone-name>    
      EOF
      ```
    </Tab>

    <Tab label="回显">
      ```
      cephobjectrealm.ceph.rook.io/<realm-name> created
      cephobjectzonegroup.ceph.rook.io/<zonegroup-name> created
      cephobjectzone.ceph.rook.io/<zone-name> created
      cephobjectstore.ceph.rook.io/<object-store-name> created
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - <a id="realm" />`<realm-name>`：Realm 名称。
  - <a id="zone-group" />`<zonegroup-name>`：ZoneGroup 名称。
  - <a id="primary-zone" />`<primary-zone-name>`：主 Zone 名称。
  - <a id="gateway" />`<object-store-name>`：网关名称。

  ### 为主 Zone 配置外部访问

  1. <a id="uid" />获取 ObjectStore 的 UID

  ```bash
  kubectl -n rook-ceph get cephobjectstore <object-store-name> -o jsonpath='{.metadata.uid}'
  ```

  **参数说明**

  - `<object-store-name>`：在 [步骤 1](#gateway) 中配置的网关名称。

  2. 创建外部访问的 Service

  ```yaml
  cat << EOF | kubectl apply -f -
  apiVersion: v1
  kind: Service
  metadata:
    name: rook-ceph-rgw-<object-store-name>-external
    namespace: rook-ceph
    labels:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: <object-store-name>
    ownerReferences:
      - apiVersion: ceph.rook.io/v1
        kind: CephObjectStore
        name: <object-store-name>
        uid: <object-store-uid>
  spec:
    ports:
      - name: rgw
        port: 7480
        targetPort: 7480
        protocol: TCP
    selector:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: <object-store-name>
    sessionAffinity: None
    type: NodePort
  EOF
  ```

  **参数说明**：

  - `<object-store-name>`：在 [此处](#gateway) 配置的网关名称。
  - `<object-store-uid>`：在 [此处](#uid) 获取的 UID。

  3. 为 CephObjectZone 添加外部访问地址

  ```bash
  kubectl -n rook-ceph patch cephobjectzone <primary-zone-name> --type merge -p '{"spec":{"customEndpoints":["<external-endpoint>"]}}'
  ```

  **参数说明**：

  - `<zone-name>`：在 [此处](#primary-zone) 配置的主 Zone 名称。
  - `<external-endpoint>`：在主集群中 [获取的外部地址](#address)。

  ### <a id="aksk" />获取 `access-key` 和 `secret-key`

  ```bash
  kubectl -n rook-ceph get secrets <realm-name>-keys -o yaml | grep access-key
  kubectl -n rook-ceph get secrets <realm-name>-keys -o yaml | grep secret-key
  ```

  **参数说明**：

  - `<realm-name>`：在 [此处](#realm) 配置的 Realm 名称。

  ### 创建备用 Zone 并配置 Realm 同步

  本节解释如何创建备用 Zone 并从主集群提取 Realm 信息进行配置同步。

  在备用集群的控制节点上执行以下命令：

  ```yaml
  cat << EOF | kubectl apply -f -
  apiVersion: v1
  kind: Secret
  metadata:
    name: <realm-name>-keys
    namespace: rook-ceph
  data:
    access-key: <access-key>
    secret-key: <secret-key>

  ---
  apiVersion: ceph.rook.io/v1
  kind: CephObjectRealm
  metadata:
    name: <realm-name>
    namespace: rook-ceph
  spec:
    pull:
      endpoint: <realm-endpoint>

  ---
  apiVersion: ceph.rook.io/v1
  kind: CephObjectZoneGroup
  metadata:
    name: <zone-group-name>
    namespace: rook-ceph
  spec:
    realm: <realm-name>
    
  ---
  apiVersion: ceph.rook.io/v1
  kind: CephObjectZone
  metadata:
    name: <new-zone-name>
    namespace: rook-ceph
  spec:
    zoneGroup: <zone-group-name>
    metadataPool:
      failureDomain: host
      replicated:
        size: 3
        requireSafeReplicaSize: true
    dataPool:
      failureDomain: host
      replicated:
        size: 3
        requireSafeReplicaSize: true
    preservePoolsOnDelete: false

  ---
  apiVersion: ceph.rook.io/v1
  kind: CephObjectStore
  metadata:
    name: <secondary-object-store-name>
    namespace: rook-ceph
  spec:
    gateway:
      port: 7480
      instances: 2
    zone:
      name: <secondary-zone-name>
  EOF
  ```

  **参数说明**：

  - `<access-key>`：在 [此处](#aksk) 获取的 AK。
  - `<secret-key>`：在 [此处](#aksk) 获取的 SK。
  - `<realm-endpoint>`：在平台中切换至主集群 [获取的外部地址](#address)。
  - `<realm-name>`：[Realm](#realm)。
  - `<zone-group-name>`：[ZoneGroup](#zone-group)。
  - `<secondary-zone-name>`：备用 Zone 名称。
  - `<secondary-object-store-name>`：备用网关的名称。

  ### 为备用 Zone 配置外部访问

  1. <a id="uids" />获取备用网关的 UID

  ```
  kubectl -n rook-ceph get cephobjectstore <secondary-object-store-name> -o jsonpath='{.metadata.uid}'
  ```

  **参数说明**：

  - `<secondary-object-store-name>`：备用网关名称。

  2. 创建外部访问 Service

  ```yaml
  cat << EOF | kubectl apply -f -
  apiVersion: v1
  kind: Service
  metadata:
    name: rook-ceph-rgw-<object-store-name>-external
    namespace: rook-ceph
    labels:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: <object-store-name>
    ownerReferences:
      - apiVersion: ceph.rook.io/v1
        kind: CephObjectStore
        name: <object-store-name>
        uid: <object-store-uid>
  spec:
    ports:
      - name: rgw
        port: 7480
        targetPort: 7480
        protocol: TCP
    selector:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: <object-store-name>
    sessionAffinity: None
    type: NodePort
  EOF
  ```

  **参数说明**：

  - `<secondary-object-store-name>`：备用网关名称。
  - `<secondary-object-store-uid>`：备用网关 UID。

  3. 为备用 CephObjectZone 添加外部访问地址

  ```
  kubectl -n rook-ceph patch cephobjectzone <secondary-zone-name> --type merge -p '{"spec":{"customEndpoints":["<external-endpoint>"]}}'
  ```

  **参数说明**：

  - `<secondary-zone-name>`：备用 Zone 名称。
  - `<secondary-zone-external-endpoint>`：在备用集群中 [获取的外部地址](#address)。

</Steps>

## 故障切换

当主集群出现故障时，需要将备用 Zone 促进为主 Zone。切换后，备用 Zone 的网关可以继续提供对象存储服务。

### 操作步骤

  在备用集群的 `rook-ceph-tools` Pod 中执行

  ```bash
  radosgw-admin zone modify --rgw-realm=<realm-name> --rgw-zonegroup=<zone-group-name> --rgw-zone=<secondary-zone-name> --master
  ```

  **参数说明**

  - `<realm-name>`：Realm 名称。
  - `<zone-group-name>`：Zone Group 名称。
  - `<secondary-zone-name>`：备用 Zone 名称。

## 相关操作

### <a id="address" />获取外部地址

1. 访问 **平台管理**。

2. 在左侧导航栏中，单击 **存储管理** > **分布式存储**。

3. 在 **集群信息** 标签中，滚动到 **存储池** 区域，单击对象存储池右侧的 ⋮，选择 **查看地址**。
