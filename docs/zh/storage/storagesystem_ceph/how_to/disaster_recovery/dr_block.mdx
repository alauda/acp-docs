---
weight: 65
sourceSHA: 26799b20a8c7d8309306745fc5a0a8eb1ebfdf314c2943573b730b0d1953052c
---

## 翻译内容

---
weight: 65
---

# 块存储灾难恢复

RBD Mirror 是 Ceph 块存储（RBD）的一个特性，用于在不同 Ceph 集群之间进行异步数据复制，从而提供跨集群的灾难恢复（Disaster Recovery, DR）。其核心功能是以主备模式同步数据，确保在主集群发生故障时，备集群能够快速接管服务。

:::warning
- RBD Mirror 基于快照进行增量同步，默认快照间隔为每小时一次（可配置）。主备集群之间的差异数据通常对应于一个快照周期内的写入内容。
- RBD Mirror 仅提供底层存储数据的备份，并不处理 Kubernetes 资源的备份。请使用平台的 **备份和恢复** 功能来备份 PVC 和 PV 资源。
:::

## 术语

| 术语                  | 说明                                             |
| :-------------------- | :---------------------------------------------- |
| **主集群**           | 当前提供存储服务的集群。                       |
| **备集群**           | 用于备份目的的待机集群。                       |

## 备份配置

### 前提条件

- 准备两个能够部署 Alauda Container Platform (ACP) Storage with Ceph 的集群：一个主集群和一个备集群，并确保它们之间的网络连接。
- 两个集群必须运行相同的平台版本（v3.12 或更高）。
- [在主集群和备集群中创建分布式存储服务](../../installation/create_service_stand.mdx)。
- 在主集群和备集群中创建**名称相同**的块存储池。
- 请确保以下三个镜像已上传到平台的私有镜像仓库中：
  - `quay.io/csiaddons/k8s-controller:v0.5.0`
  - `quay.io/csiaddons/k8s-sidecar:v0.8.0`
  - `quay.io/brancz/kube-rbac-proxy:v0.8.0`

### 操作步骤

<Steps>
  ### 为主集群的块存储池启用镜像功能

  在主集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl -n rook-ceph patch cephblockpool <block-pool-name> \
      --type merge -p '{"spec":{"mirroring":{"enabled":true,"mode":"image"}}}'
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      cephblockpool.ceph.rook.io/<block-pool-name> patched
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<block-pool-name>`：块存储池名称。

  ### <a id="blocktoken"/>获取对等 token

  该 token 是在集群之间建立镜像连接的关键凭证。

  在主集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl get secret -n rook-ceph \
      $(kubectl get cephblockpool.ceph.rook.io <block-pool-name> -n rook-ceph -o jsonpath='{.status.info.rbdMirrorBootstrapPeerSecretName}') \
      -o jsonpath='{.data.token}' | base64 -d
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      # 因涉及敏感信息该回显已做截断处理
      eyJmc2lkIjoiMjc2N2I3ZmEtY2YwYi00N...
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<block-pool-name>`：块存储池名称。

  ### 在备集群中创建对等 token 密钥

  在备集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl -n rook-ceph create secret generic rbd-primary-site-secret \
      --from-literal=token=<token> \
      --from-literal=pool=<block-pool-name>
      ```
    </Tab>

    <Tab label="回显">
      ```
      secret/rbd-primary-site-secret created
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<token>`：在[步骤 2](#blocktoken)中获取的 token。
  - `<block-pool-name>`：块存储池名称。

  ### 为备集群的块存储池启用镜像功能

  在备集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl -n rook-ceph patch cephblockpool <block-pool-name> --type merge -p \
      '{
        "spec": {
          "mirroring": {
            "enabled": true, 
            "mode": "image", 
            "peers": {
              "secretNames": [
                "rbd-primary-site-secret"
              ]
            }
          }
        }
      }'
      ```
    </Tab>

    <Tab label="回显">
      ```
      cephblockpool.ceph.rook.io/<block-pool-name> patched
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<block-pool-name>`：块存储池名称。

  ### 在备集群中部署镜像守护进程

  该守护进程负责监控和管理 RBD 镜像同步过程，包括数据同步和错误处理。

  在备集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```yaml
      cat << EOF | kubectl apply -f -
      apiVersion: ceph.rook.io/v1
      kind: CephRBDMirror
      metadata:
        name: rbd-mirror
        namespace: rook-ceph
      spec:
        count: 1
      EOF
      ```
    </Tab>

    <Tab label="回显">
      ```
      cephrbdmirror.ceph.rook.io/rbd-mirror created
      ```
    </Tab>
  </Tabs>

  ### 验证镜像状态

  在备集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl get cephblockpools.ceph.rook.io <block-pool-name> -n rook-ceph -o jsonpath='{.status.mirroringStatus.summary}'
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      # 如果回显中的所有状态均为 "OK"，则表示状态正常。
      {"daemon_health":"OK","health":"OK","image_health":"OK","states":{}}
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<block-pool-name>`：块存储池名称。

  ### 启用复制侧车

  此功能可以在不干扰主应用运行的情况下，实现数据的高效复制和同步，增强系统的可靠性和可用性。

  1. 部署 csiaddons-controller

  在主集群和备集群的控制节点上执行以下命令：

  <details>
    <summary>点击查看</summary>

    ```yaml
    kubectl create -f https://raw.githubusercontent.com/csi-addons/kubernetes-csi-addons/v0.5.0/deploy/controller/crds.yaml
    kubectl create -f https://raw.githubusercontent.com/csi-addons/kubernetes-csi-addons/v0.5.0/deploy/controller/rbac.yaml
    
    cat << EOF | kubectl apply -f -
    apiVersion: v1
    data:
      controller_manager_config.yaml: |
        apiVersion: controller-runtime.sigs.k8s.io/v1alpha1
        kind: ControllerManagerConfig
        health:
          healthProbeBindAddress: :8081
        metrics:
          bindAddress: 127.0.0.1:8080
        webhook:
          port: 9443
        leaderElection:
          leaderElect: true
          resourceName: e8cd140a.openshift.io
    kind: ConfigMap
    metadata:
      name: csi-addons-manager-config
      namespace: csi-addons-system
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app.kubernetes.io/name: csi-addons
      name: csi-addons-controller-manager
      namespace: csi-addons-system
    spec:
      replicas: 1
      selector:
        matchLabels:
          app.kubernetes.io/name: csi-addons
      template:
        metadata:
          annotations:
            kubectl.kubernetes.io/default-container: manager
          labels:
            app.kubernetes.io/name: csi-addons
        spec:
          containers:
          - args:
            - --secure-listen-address=0.0.0.0:8443
            - --upstream=http://127.0.0.1:8080/
            - --logtostderr=true
            - --v=10
            image: <registry>/brancz/kube-rbac-proxy:v0.8.0
            name: kube-rbac-proxy
            ports:
            - containerPort: 8443
              name: https
              protocol: TCP
            resources:
              limits:
                cpu: 500m
                memory: 128Mi
              requests:
                cpu: 10m
                memory: 64Mi
          - args:
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=127.0.0.1:8080
            - --leader-elect
            command:
            - /manager
            image: <registry>/csiaddons/k8s-controller:v0.5.0
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8081
              initialDelaySeconds: 15
              periodSeconds: 20
            name: manager
            readinessProbe:
              httpGet:
                path: /readyz
                port: 8081
              initialDelaySeconds: 5
              periodSeconds: 10
            resources:
              limits:
                cpu: 500m
                memory: 128Mi
              requests:
                cpu: 10m
                memory: 64Mi
            securityContext:
              allowPrivilegeEscalation: false
          securityContext:
            runAsNonRoot: true
          serviceAccountName: csi-addons-controller-manager
          terminationGracePeriodSeconds: 10
    EOF
    ```
  </details>

  **参数说明**：

  - `<registry>`：平台的注册表地址。

  2. 启用 csi 侧车

  在主集群和备集群的控制节点上执行以下命令：

  ```bash
  kubectl patch cm rook-ceph-operator-config -n rook-ceph --type json --patch \
  '[
    {
      "op": "add", 
      "path": "/data/CSI_ENABLE_OMAP_GENERATOR", 
      "value": "true"
    }, 
    {
      "op": "add", 
      "path": "/data/CSI_ENABLE_CSIADDONS", 
      "value": "true"
    }
  ]'
  ```

  ### 创建 VolumeReplicationClass

  在主集群和备集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```yaml
      cat << EOF | kubectl apply -f -
      apiVersion: replication.storage.openshift.io/v1alpha1
      kind: VolumeReplicationClass
      metadata:
        name: rbd-volumereplicationclass
      spec:
        provisioner: rook-ceph.rbd.csi.ceph.com
        parameters:
          mirroringMode: snapshot
          schedulingInterval: "<scheduling-interval>" # [!code callout]
          replication.storage.openshift.io/replication-secret-name: rook-csi-rbd-provisioner
          replication.storage.openshift.io/replication-secret-namespace: rook-ceph
      EOF
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      volumereplicationclass.replication.storage.openshift.io/rbd-volumereplicationclass created
      ```
    </Tab>
  </Tabs>

  <Callouts>
    1. `<scheduling-interval>`：执行周期，例如，`schedulingInterval: "1h"` 表示每 1 小时执行一次。
  </Callouts>

  ### 为 PVC 启用镜像

  在主集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```yaml
      cat << EOF | kubectl apply -f -
      apiVersion: replication.storage.openshift.io/v1alpha1
      kind: VolumeReplication
      metadata:
        name: <vr-name> # [!code callout]
        namespace: <namespace> # [!code callout]
      spec:
        autoResync: false
        volumeReplicationClass: rbd-volumereplicationclass
        replicationState: primary
        dataSource:
          apiGroup: ""
          kind: PersistentVolumeClaim
          name: <pvc-name> # [!code callout]
      EOF
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      volumereplication.replication.storage.openshift.io/<mirror-pvc-name> created
      ```
    </Tab>
  </Tabs>

  <Callouts>
    1. `<vr-name>`：VolumeReplication 对象的名称，建议与 PVC 名称相同。
    2. `<namespace>`：VolumeReplication 所属的命名空间，必须与 PVC 的命名空间相同。
    3. `<pvc-name>`：需要启用镜像的 PVC 名称。
  </Callouts>

  **备注**
  启用之后，备集群中的 RBD 镜像将变为只读状态。
</Steps>

## 故障切换

当主集群发生故障时，需要切换 RBD 镜像的主备关系。

### 前提条件

- 主集群的 Kubernetes 资源已备份，并恢复至备集群，包括 PVC、PV、应用工作负载等。

### 操作步骤

<Steps>
  #### 创建 VolumeReplication

  在备集群的控制节点上执行以下命令：

  ```yaml
  cat << EOF | kubectl apply -f -
  apiVersion: replication.storage.openshift.io/v1alpha1
  kind: VolumeReplication
  metadata:
    name: <vr-name> # [!code callout]
    namespace: <namespace> # [!code callout]
  spec:
    autoResync: false
    dataSource:
      apiGroup: ""
      kind: PersistentVolumeClaim
      name: <mirror-pvc-name> # [!code callout]
    replicationHandle: ""
    replicationState: primary
    volumeReplicationClass: rbd-volumereplicationclass
  EOF
  ```

  <Callouts>
    1. `<vr-name>`：VolumeReplication 对象的名称。
    2. `<namespace>`：PVC 所属的命名空间。
    3. `<mirror-pvc-name>`：要启用镜像同步的 PVC 名称。
  </Callouts>

  **备注**
  创建完成后，备集群中的 RBD 镜像将变为主状态，可以读写。
</Steps>
