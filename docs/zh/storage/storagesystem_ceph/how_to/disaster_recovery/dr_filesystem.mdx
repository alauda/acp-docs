---
weight: 64
sourceSHA: 42488c8a6dff243d30ca516a98b158dcd6fe4117eb4a764a43ab08b4fc3ef69f
---

## 文件存储灾难恢复

CephFS Mirror 是 Ceph 文件系统的一个功能，旨在实现不同 Ceph 集群之间的异步数据复制，从而提供跨集群的灾难恢复。其核心功能是以主备模式同步数据，确保在主集群发生故障时，备集群能够迅速接管服务。

:::warning
- CephFS Mirror 基于快照执行增量同步，默认的快照间隔为每小时一次（可配置）。主备集群之间的差异数据通常是一个快照周期内写入的数据量。
- CephFS Mirror 仅提供底层存储数据的备份，无法处理 Kubernetes 资源的备份。请配合平台的 **备份与恢复** 功能以备份 PVC 和 PV 资源。
:::

## 术语

| 术语                  | 说明                                          |
| :-------------------- | :-------------------------------------------- |
| **主集群**           | 目前提供存储服务的集群。                     |
| **备集群**           | 用于备份的集群。                             |

## 备份配置

### 前提条件

- 准备两个适合部署 Alauda 容器平台 (ACP) Storage with Ceph 的集群，即主集群和备集群，确保集群之间的网络互通。
- 两个集群使用的平台版本（v3.12 及以上）必须一致。
- 在主集群和备集群中[创建分布式存储服务](../../installation/create_service_stand.mdx)。
- 在主集群和备集群中创建**名称相同**的文件存储池。

### 操作步骤

<Steps>
  
  ### 启用备集群文件存储池的镜像功能

  在备集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl -n rook-ceph patch cephfilesystem <fs-name> \
      --type merge -p '{"spec":{"mirroring":{"enabled": true}}}'
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      cephfilesystem.ceph.rook.io/<fs-name> patched
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<fs-name>`： 文件存储池名称。

  ### <a id="token" />获取对等令牌

  此令牌是建立两个集群之间镜像连接的关键凭证。

  在备集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl get secret -n rook-ceph \
      $(kubectl -n rook-ceph get cephfilesystem <fs-name> -o jsonpath='{.status.info.fsMirrorBootstrapPeerSecretName}') \
      -o jsonpath='{.data.token}' | base64 -d
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      # 由于涉及敏感信息，输出已被截断。
      eyJmc2lkIjogImMyYjAyNmMzLTA3ZGQtNDA3Z...
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<fs-name>`： 文件存储池名称。

  ### 在主集群中创建对等密钥

  获取备集群的对等令牌后，需要在主集群中创建对等密钥。

  在主集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl -n rook-ceph create secret generic fs-secondary-site-secret \
      --from-literal=token=<token> \
      --from-literal=pool=<fs-name>
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      secret/fs-secondary-site-secret created
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<token>`：在[步骤 2](#token)中获得的令牌。
  - `<fs-name>`：文件存储池名称。

  ### 启用主集群文件存储池的镜像功能

  在主集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```bash
      kubectl -n rook-ceph patch cephfilesystem <fs-name> --type merge -p \
      '{
        "spec": {
          "mirroring": {
            "enabled": true, 
            "peers": {
              "secretNames": [
                "fs-secondary-site-secret"
              ]
            }, 
            "snapshotSchedules": [
              {
                "path": "/", 
                "interval": "<schedule-interval>"
              }
            ], 
            "snapshotRetention": [
              {
                "path": "/", 
                "duration": "<retention-policy>"
              }
            ]
          }
        }
      }'
      ```
    </Tab>

    <Tab label="配置示例">
      ```bash
      kubectl -n rook-ceph patch cephfilesystem cephfs --type merge -p \
      '{
        "spec": {
          "mirroring": {
            "enabled": true, 
            "peers": {
              "secretNames": [
                "fs-secondary-site-secret"
              ]
            }, 
            "snapshotSchedules": [
              {
                "path": "/", 
                "interval": "1h"
              }
            ], 
            "snapshotRetention": [
              {
                "path": "/", 
                "duration": "h 1"
              }
            ]
          }
        }
      }'
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      cephfilesystem.ceph.rook.io/<fs-name> patched
      ```
    </Tab>
  </Tabs>

  **参数说明**：

  - `<fs-name>`：文件存储池名称。
  - `<schedule-interval>`：快照执行周期，具体请参考[官方文档](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-schedules)。
  - `<retention-policy>`：快照保留策略，具体请参考[官方文档](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-retention-policies)。

  ### 在主集群中部署镜像守护进程

  镜像守护进程会持续监控文件存储池（已启用镜像）的数据变动。它会定期创建快照，并通过网络将快照差异推送到备集群。

  在主集群的控制节点上执行以下命令：

  <Tabs>
    <Tab label="命令">
      ```yaml
      cat << EOF | kubectl apply -f -
      apiVersion: ceph.rook.io/v1
      kind: CephFilesystemMirror
      metadata:
        name: cephfs-mirror
        namespace: rook-ceph
      spec:
        placement:
          tolerations:
          - key: NoSchedule
            operator: Exists
        resources:
          limits:
            cpu: "500m"
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"
        priorityClassName: system-node-critical
      EOF
      ```
    </Tab>

    <Tab label="回显">
      ```bash
      cephfilesystemmirror.ceph.rook.io/cephfs-mirror created
      ```
    </Tab>
  </Tabs>

</Steps>

## 故障转移

在主集群发生故障时，您可以直接在备集群中继续使用 CephFS。

### 前提条件

- 主集群的 Kubernetes 资源已备份并恢复到备集群，包括 PVC、PV 和应用工作负载等。
