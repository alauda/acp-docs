---
weight: 10
---

# 如何将日志归档至第三方存储

平台目前产生的日志将会存储至日志存储组件中，但日志的保留时间较短，对于合规性要求较高的企业，通常需要将日志保留更长时间以应对审计需要，并且存储的经济性也是企业重要关注点之一。

基于以上场景，平台提供了日志归档方案，方便用户将日志转存至外部 NFS 或对象存储。

## 转存至外部 NFS

### 前期准备

| 资源 | 说明 |
| ---- | ---- |
|**NFS**| 提前搭建 NFS 服务，并确定需要挂载的 NFS 路径。|
|**Kafka**|提前获取 Kafka 服务地址。|
|**镜像地址**|您须在 global 集群中使用 CLI 工具执行以下命令获取镜像地址：<br />- 获取 alpine 镜像地址：`kubectl get daemonset nevermore -n cpaas-system -o jsonpath='{.spec.template.spec.initContainers[0].image}'` <br />- 获取 razor 镜像地址：`kubectl get deployment razor -n cpaas-system -o jsonpath='{.spec.template.spec.containers[0].image}'`|


### 创建日志同步资源

1. 在左侧导航栏中单击 **集群管理** > **集群**。

2. 单击待转存日志的集群右侧的操作按钮> **CLI 工具**。

3. 根据如下参数说明修改 YAML，修改后，将代码粘贴至打开的 **CLI 工具** 命令行中，并回车执行。

    |资源类型| 字段路径 | 说明 |
    | --- | ---- | ---- |
    | ConfigMap|`data.export.yml.output.compression`|压缩日志文本，支持 **none（不压缩）**、**zlib**、**gzip**。|
    | ConfigMap|`data.export.yml.output.file_type`| 导出的日志文件类型，支持 txt csv json 三种类型。|
    | ConfigMap|`data.export.yml.output.max_size`|单个归档文件轮转的大小，单位 MB，超过该值，将会自动根据 compression 字段的配置压缩日志文本，并归档。|
    | ConfigMap|`data.export.yml.scopes`| 日志转存范围，目前支持转存的日志有：系统日志，应用日志，Kubernetes 日志，产品日志。|
    | Deployment|`spec.template.spec.containers[0].command[7]`|Kafka 服务地址。|
    | Deployment|`spec.template.spec.volumes[3].hostPath.path`|需要挂载的 NFS 路径。|
    | Deployment|`spec.template.spec.initContainers[0].image`|alpine 镜像地址。|
    | Deployment|`spec.template.spec.containers[0].image`|razor 镜像地址。|

    ```yaml
    cat << "EOF" |kubectl apply -f -
    apiVersion: v1
    data:
      export.yml: |
        scopes: # 日志转存的范围，默认仅采集应用日志
          system: false  # 系统日志
          workload: true # 应用日志
          kubernetes: false # Kubernetes 日志
          platform: false # 产品日志
        output:
          type: local                              
          path: /cpaas/data/logarchive           
          layout: TimePrefixed
          # 单个归档文件轮转的大小，单位 MB，超过该值，将会自动根据 compression 字段的配置压缩日志文本，并归档。
          max_size: 200         
          compression: zlib    # 可选 none（不压缩）/ zlib / gzip
          file_type: txt   # 可选 txt csv json
    kind: ConfigMap
    metadata:
      name: log-exporter-config
      namespace: cpaas-system
     
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        service_name: log-exporter
      name: log-exporter
      namespace: cpaas-system
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 5
      selector:
        matchLabels:
          service_name: log-exporter
      strategy:
        rollingUpdate:
          maxSurge: 0
          maxUnavailable: 1
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: lanaya
            cpaas.io/product: Platform-Center
            service_name: log-exporter
            version: v1
          namespace: cpaas-system
        spec:
          affinity:
            podAffinity: {}
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: service_name
                          operator: In
                          values:
                            - log-exporter
                    topologyKey: kubernetes.io/hostname
                  weight: 50
          initContainers:
            - args:
                - -ecx
                - |
                  chown -R 697:697 /cpaas/data/logarchive
              command:
                - /bin/sh
              image: registry.example.cn:60080/ops/alpine:3.16 # alpine 镜像地址
              imagePullPolicy: IfNotPresent
              name: chown
              resources:
                limits:
                  cpu: 100m
                  memory: 200Mi
                requests:
                  cpu: 10m
                  memory: 50Mi
              securityContext:
                runAsUser: 0
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
                - mountPath: /cpaas/data/logarchive
                  name: data
          containers:
            - command:
              - /razor
              - consumer
              - --v=1
              - --kafka-group-log=log-nfs
              - --kafka-auth-enabled=true
              - --kafka-tls-enabled=true
              - --kafka-endpoint=192.168.143.120:9092  # 根据实际环境信息填写
              - --database-type=file
              - --export-config=/etc/log-export/export.yml
              image: registry.example.cn:60080/ait/razor:v3.16.0-beta.3.g3df8e987  # razor的镜像
              imagePullPolicy: Always
              livenessProbe:
                failureThreshold: 5
                httpGet:
                  path: /metrics
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 20
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 3
              name: log-export
              ports:
                - containerPort: 80
                  protocol: TCP
              readinessProbe:
                failureThreshold: 5
                httpGet:
                  path: /metrics
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 20
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 3
              resources:
                limits:
                  cpu: "2"
                  memory: 4Gi
                requests:
                  cpu: 440m
                  memory: 1280Mi
              securityContext:
                runAsGroup: 697
                runAsUser: 697
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
                - mountPath: /etc/secrets/kafka
                  name: kafka-basic-auth
                  readOnly: true
                - mountPath: /etc/log-export
                  name: config
                  readOnly: true
                - mountPath: /cpaas/data/logarchive 
                  name: data
          dnsPolicy: ClusterFirst
          nodeSelector:
            kubernetes.io/os: linux
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext:
            fsGroup: 697
          serviceAccount: lanaya
          serviceAccountName: lanaya
          terminationGracePeriodSeconds: 10
          tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/master
              operator: Exists
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Exists
            - effect: NoSchedule
              key: node-role.kubernetes.io/cpaas-system
              operator: Exists
          volumes:
            - name: kafka-basic-auth
              secret:
                defaultMode: 420
                secretName: kafka-basic-auth
            - name: elasticsearch-basic-auth
              secret:
                defaultMode: 420
                secretName: elasticsearch-basic-auth
            - configMap:
                defaultMode: 420
                name: log-exporter-config
              name: config
            - hostPath:
                path: /cpaas/data/logarchive    # 需要挂载的 NFS 路径
                type: DirectoryOrCreate
              name: data
    EOF
    ```

4. 待容器状态变为 Runing 后，可查看 NFS 路径中持续归档的日志，日志文件目录结构如下：

    ```bash
    /cpaas/data/logarchive/$date/$project/$namespace-$cluster/logfile
    ```
  



## 转存至外部 S3 存储

### 前期准备

| 资源 | 说明 |
| ---- | ---- |
|**S3 存储**| 提前准备 S3 存储服务地址，并获取 `access_key_id` 和 `secret_access_key` 的值；创建待存放日志的存储桶。|
|**Kafka**|提前获取 Kafka 服务地址。|
|**镜像地址**|您须在 global 集群中使用 CLI 工具执行以下命令获取镜像地址：<br />- 获取 alpine 镜像地址：`kubectl get daemonset nevermore -n cpaas-system -o jsonpath='{.spec.template.spec.initContainers[0].image}'` <br />- 获取 razor 镜像地址：`kubectl get deployment razor -n cpaas-system -o jsonpath='{.spec.template.spec.containers[0].image}'`|

### 创建日志同步资源

1. 在左侧导航栏中单击 **集群管理** > **集群**。

2. 单击待转存日志的集群右侧的操作按钮> **CLI 工具**。

3. 根据如下参数说明修改 YAML，修改后，将代码粘贴至打开的 **CLI 工具** 命令行中，并回车执行。

    |资源类型| 字段路径 | 说明 |
    | --- | ---- | ---- |
    | Secret|`data.access_key_id`|将获取到的 access_key_id 使用 base64 编码。|
    | Secret|`data.secret_access_key`|将获取到的 secret_access_key 使用 base64 编码。|
    | ConfigMap|`data.export.yml.output.compression`|压缩日志文本，支持 **none（不压缩）**、**zlib**、**gzip**。|
    | ConfigMap|`data.export.yml.output.file_type`| 导出的日志文件类型，支持 txt csv json 三种类型。|
    | ConfigMap|`data.export.yml.output.max_size`|单个归档文件轮转的大小，单位 MB，超过该值，将会自动根据 compression 字段的配置压缩日志文本，并归档。|
    | ConfigMap|`data.export.yml.scopes`| 日志转存范围，目前支持转存的日志有：系统日志，应用日志，Kubernetes 日志，产品日志。|
    | ConfigMap|`data.export.yml.output.s3.bucket_name`| 存储桶名称。|
    | ConfigMap|`data.export.yml.output.s3.endpoint`|  S3 存储服务地址。| 
    | ConfigMap|`data.export.yml.output.s3.region`|  S3 存储服务的地域信息。| 
    | Deployment|`spec.template.spec.containers[0].command[7]`|Kafka 服务地址。|
    | Deployment|`spec.template.spec.volumes[3].hostPath.path`|需要挂载的本地路径，该路径用于临时存放日志信息，同步至 S3 存储后将自动删除临时存放的日志。|
    | Deployment|`spec.template.spec.initContainers[0].image`|alpine 镜像地址。|
    | Deployment|`spec.template.spec.containers[0].image`|razor 镜像地址。|
    
    ```yaml
    cat << "EOF" |kubectl apply -f -
    apiVersion: v1
    type: Opaque
    data:
      # 必须包含下面两个key
      access_key_id: bWluaW9hZG1pbg==  # 将获取到的 access_key_id 使用 base64 编码
      secret_access_key: bWluaW9hZG1pbg==  #  将获取到的 secret_access_key 使用 base64 编码
    kind: Secret
    metadata:
      name: log-export-s3-secret
      namespace: cpaas-system
      
    ---
    apiVersion: v1
    data:
      export.yml: |
        scopes: # 日志转存的范围，默认仅采集应用日志
          system: false  # 系统日志
          workload: true # 应用日志
          kubernetes: false # Kubernetes 日志
          platform: false # 产品日志
        output:
          type: s3                             
          path: /cpaas/data/logarchive             
          
          s3:
            s3forcepathstyle: true
            bucket_name: baucket_name_s3           # 填写已准备的存储桶名称
            endpoint: http://192.168.179.86:9000   # 填写已准备的 S3 存储服务地址
            region: "dummy"                        # 地域信息
            access_secret: log-export-s3-secret    
            insecure: true
                                
          layout: TimePrefixed
          # 单个归档文件轮转的大小，单位 MB，超过该值，将会自动根据 compression 字段的配置压缩日志文本，并归档。
          max_size: 200                            
          compression: zlib                        # 可选 none（不压缩）/ zlib / gzip
          file_type: txt                           # 可选 txt csv json
    kind: ConfigMap
    metadata:
      name: log-exporter-config
      namespace: cpaas-system
     
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        service_name: log-exporter
      name: log-exporter
      namespace: cpaas-system
    spec:
      progressDeadlineSeconds: 600
      replicas: 1
      revisionHistoryLimit: 5
      selector:
        matchLabels:
          service_name: log-exporter
      strategy:
        rollingUpdate:
          maxSurge: 0
          maxUnavailable: 1
        type: RollingUpdate
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: lanaya
            cpaas.io/product: Platform-Center
            service_name: log-exporter
            version: v1
          namespace: cpaas-system
        spec:
          affinity:
            podAffinity: {}
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: service_name
                          operator: In
                          values:
                            - log-exporter
                    topologyKey: kubernetes.io/hostname
                  weight: 50
          initContainers:
            - args:
                - -ecx
                - |
                  chown -R 697:697 /cpaas/data/logarchive
              command:
                - /bin/sh
              image: registry.example.cn:60080/ops/alpine:3.16 # alpine 镜像地址
              imagePullPolicy: IfNotPresent
              name: chown
              resources:
                limits:
                  cpu: 100m
                  memory: 200Mi
                requests:
                  cpu: 10m
                  memory: 50Mi
              securityContext:
                runAsUser: 0
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
                - mountPath: /cpaas/data/logarchive
                  name: data
          containers:
            - command:
                - /razor
                - consumer
                - --v=1
                - --kafka-group-log=log-s3   
                - --kafka-auth-enabled=true
                - --kafka-tls-enabled=true
                - --kafka-endpoint=192.168.179.86:9092  # 根据实际环境信息填写 kafka 服务地址
                - --database-type=file
                - --export-config=/etc/log-export/export.yml
              image: registry.example.cn:60080/ait/razor:v3.16.0-beta.3.g3df8e987  # razor的镜像
              imagePullPolicy: Always
              livenessProbe:
                failureThreshold: 5
                httpGet:
                  path: /metrics
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 20
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 3
              name: log-export
              ports:
                - containerPort: 80
                  protocol: TCP
              readinessProbe:
                failureThreshold: 5
                httpGet:
                  path: /metrics
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 20
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 3
              resources:
                limits:
                  cpu: "2"
                  memory: 4Gi
                requests:
                  cpu: 440m
                  memory: 1280Mi
              securityContext:
                runAsGroup: 697
                runAsUser: 697
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
                - mountPath: /etc/secrets/kafka
                  name: kafka-basic-auth
                  readOnly: true
                - mountPath: /etc/log-export
                  name: config
                  readOnly: true
                - mountPath: /cpaas/data/logarchive
                  name: data
          dnsPolicy: ClusterFirst
          nodeSelector:
            kubernetes.io/os: linux
          restartPolicy: Always
          schedulerName: default-scheduler
          securityContext:
            fsGroup: 697
          serviceAccount: lanaya
          serviceAccountName: lanaya
          terminationGracePeriodSeconds: 10
          tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/master
              operator: Exists
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Exists
            - effect: NoSchedule
              key: node-role.kubernetes.io/cpaas-system
              operator: Exists
          volumes:
            - name: kafka-basic-auth
              secret:
                defaultMode: 420
                secretName: kafka-basic-auth
            - name: elasticsearch-basic-auth
              secret:
                defaultMode: 420
                secretName: elasticsearch-basic-auth
            - configMap:
                defaultMode: 420
                name: log-exporter-config
              name: config
            - hostPath:
                path: /cpaas/data/logarchive    # 本地宿主机临时存放日志的存储地址
                type: DirectoryOrCreate
              name: data
    EOF
    ```

4. 待容器状态变为 Runing 后，可查看存储桶中持续归档的日志。
