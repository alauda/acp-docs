---
weight: 30
sourceSHA: a724d56d45d3e5b2f0d90f799d70057b7a04427bb4e6e6beaa5dda43c7e15a47
---

# 升级 <Term name="productShort" /> v3.18.0 或 v3.18.1 到 v4.0

## 关键升级注意事项

- **从低于 1.31 版本的 k8s 升级到 1.31 时，所有节点上的 pods 将会重启一次。在此过程中，pods 不会被重建或重新调度，但会发生服务中断。**
- 从 v4.0 开始，为减少升级灾难恢复环境的复杂性，**在升级灾难恢复环境时，请先升级备用集群**，以避免在升级过程中发生灾难恢复切换。
- 本升级文档适用于将 x86 和 ARM 架构版本 v3.18.0 或 v3.18.1 升级到版本 v4.0.0。
- 在升级之前，请确保所有集群已升级到版本 v3.18.0 或 v3.18.1。k8s 版本必须至少升级到 1.28 版本。
- 在升级之前，请检查全局集群的组件列表。如果有组件状态异常，请不要升级，除非您计划通过升级来解决这些问题。
- 如果平台已部署服务网格，并且存在低于 Istio 1.20 的服务网格，则必须在执行平台升级之前先升级服务网格。
- Costmanager 插件和 Kubecost 插件在版本 4.0 中已被停用。如果在平台上部署了 Costmanager 插件和 Kubecost 插件，请在升级之前将其卸载。
- 如果注册表使用 MinIO 作为存储后端，并且 MinIO 使用全局集群的三个主节点的 /cpaas/minio 目录，请确保这三个节点的 /cpaas/minio 目录剩余空间至少为 120GB。如果不足，请扩容。
- 安装包将复制到全局集群的第一个主节点，在解压后将使用最多 120GB 的空间。因此，当安装包和解压目录位于同一磁盘时，该磁盘需要具有超过 250GB 的可用空间。
- 上传镜像大约需要两个小时，请提前规划您的升级时间。
- 如果在升级过程中出现问题，请首先检查升级常见问题文档。

## 灾难恢复环境 (DR) 升级流程

### 数据比较

<Directive type="warning" title="警告">
  在升级之前，请务必执行以下两个步骤。
</Directive>

1. 在升级之前，请按照维护文档执行灾难恢复环境的数据比较脚本，以确保主集群和备集群之间没有不一致。
2. 在主集群和备用全局集群上分别执行以下命令：`kubectl get machines.platform.tkestack.io`，确保返回的主机中没有非运行状态的节点。

### 卸载灾难恢复数据同步程序

1. 使用备用集群的 VIP 访问 **备用集群** 页面
   1. 如果无法通过 IP 访问平台页面，请登录 **备用集群** 的 master1 节点并修改 prdb 以添加其他平台访问地址：
   ```yaml
   spec:
     alternativeURLs:
     - https://192.168.162.10 # 将 IP 替换为实际的备用集群 VIP
   ```
2. 在 **备用集群的** 插件页面上，卸载灾难恢复同步程序插件，并等待卸载完成。

### 升级备用集群

1. 切换集群访问地址的域名指向 **备集群** ，等待 5 分钟后检查解析结果是否更新。
2. 请参考 [上传平台镜像](#upload-platform-images) 段落，将平台镜像上传到 **备用集群** 。
3. 请参考 [升级 global 集群](#upgrade-the-global-cluster) 段落，升级 **备用集群的** global 。
4. 等待 **备用集群的** global 升级完成。

### 升级主集群

1. 切换集群访问地址的域名指向主集群，等待 5 分钟后检查解析结果是否更新。
2. 请参考 [上传平台镜像](#upload-platform-images) 段落，将平台镜像上传到主集群。
3. 请参考 [升级 global 集群](#upgrade-the-global-cluster) 段落，升级主集群的 global。
4. 等待主集群的 global 升级完成。

### 启动同步程序

1. 启动数据同步的 pod。
   1. 分别检查主集群和备用集群 VIP 的 2379 端口，并确认它们已转发到各自的主节点。
   2. 通过 **备用集群** 的 Kubernetes VIP 访问 **备用集群** 平台页面，点击全局集群插件部署页面，然后点击部署 etcd 同步插件。根据屏幕提示输入参数，其中：
      1. 数据检查间隔用于生成监控数据，默认值一般可以选择。
      2. 打印详细日志的开关默认不需要打开，主要用于调试问题。
2. 等待插件部署成功。
3. 登录 **备用集群的** 全局 master1 节点，检查 etcd 同步 pod 是否正常运作。

```shell
kubectl get po -n cpaas-system -l app=etcd-sync # 确保 pod 状态是 1/1 Running

kubectl logs -n cpaas-system $(kubectl get po -n cpaas-system -l app=etcd-sync --no-headers | awk '{print $1}' | head -1) | grep -i "Start Sync update"

# 等待输出返回 "Start Sync update"

# 重新创建其中一个 pod 实例，以确保所有依赖于 ownerreference 的 k8s 资源都被同步。
kubectl delete po -n cpaas-system $(kubectl get po -n cpaas-system -l app=etcd-sync --no-headers | awk '{print $1}' | head -1)
```

4. 登录备用集群的全局 master1 节点检查数据同步是否完成。

```shell
curl "$(kubectl get svc -n cpaas-system etcd-sync-monitor -ojsonpath='{.spec.clusterIP}')/check"

# 输出的意思：
"LOCAL ETCD missed keys:" # 表示主集群中存在这些键，但备用集群中不存在。通常发生这种情况是因为在 pod 启动时，由于资源顺序，键在同步后被 k8s 垃圾回收。在这种情况下，重新启动其中一个 etcd-sync pod 即可。

"LOCAL ETCD surplus keys:" # 表示备用集群中存在但主集群中不存在的键。这些键在备用集群中被认为是多余的。在删除这些键之前，请与运维人员确认。
```

### [升级业务集群](#upgrade-business-clusters-1)

## 备份

<Directive type="warning" title="警告">
  步骤 1 必须在所有集群的每个主节点上执行，包括全局集群（主集群和备用集群）以及所有业务集群。
</Directive>

1. 在所有集群的每个主节点上执行以下命令以备份 etcd。

```shell
tmp_dir="/cpaas/backup_$(date +%Y%m%d%H)"
mkdir -p "${tmp_dir}" && cp -r /etc/kubernetes/ "${tmp_dir}" && cp -r /var/lib/etcd/ "${tmp_dir}"
```

2. 如果 IaaS 条件允许，建议对所有机器进行快照备份。
3. 如果 IaaS 条件允许，建议对第三方工具（如 Harbor、Jenkins、GitLab 和 Nexus）使用的外部存储进行快照备份；否则，可能无法回滚。
4. 如果 IaaS 无法对外部存储执行快照备份，则通过执行命令 `cp -a` 制作第三方工具（如 Harbor、GitLab 和 SonarQube）的数据目录副本。

## 上传平台镜像

1. 将新版本的安装包拷贝到 global 集群的第一个 master 节点，解压。
2. cd 进入新版本安装包解压之后的目录
3. 为降低实际升级所需时间，请在正式升级前，执行 内部镜像仓库、外部镜像仓库 中适合实际场景的对应命令，以上传镜像。
4. **如果是容灾集群，需要先检查平台访问域名是否指向备集群，先往备 global 集群上传镜像。等待备 global 集群升级完毕，切换域名解析到主集群，再往主 global 集群上传镜像。**
5. 如果使用内部镜像仓库部署平台，执行如下命令上传镜像。
```shell
bash upgrade.sh --only-sync-image=true
```
6. 如果使用外部镜像仓库部署平台，执行如下命令上传镜像。
```shell
bash upgrade.sh --registry <external registry address> --username <username> --password <password> --only-sync-image=true

# e.g.
# bash upgrade.sh --registry "example.com" --username admin --password password --only-sync-image=true
```

## 触发升级

<Directive type="info" title="信息">
  对于灾难恢复集群，必须首先根据 "上传镜像" 步骤命令完成镜像上传，然后才能触发升级。
</Directive>

1. 在全局集群的主节点上执行以下命令（即离线安装程序已解压的节点）

```shell
bash upgrade.sh
```

2. 等待升级脚本完成。

## 升级全局集群

1. 访问全局页面，点击平台管理 > 集群管理 > 选择全局集群，选择功能组件，然后点击升级。
2. 升级界面将显示组件升级信息。
3. 阅读信息并确认。

<Directive type="note" title="注意">
  名为 "Kubernetes" 的组件可以不进行升级，但强烈建议与 <Term name="productShort" /> 一起触发 "Kubernetes" 组件的升级。\
  由于 <Term name="productShort" /> 自身的升级可能也会导致工作负载中断，仅仅不升级 "Kubernetes" 组件来避免工作负载中断并不是合适的方法。
</Directive>

4. 点击升级。
5. 等待升级完成。

## 升级业务集群

### 注意事项

1. 在升级任何业务集群之前，必须首先完成 [升级全局集群](#upgrade-the-global-cluster)。
2. 本升级文档适用于将业务集群从版本 v3.18.0 或 v3.18.1 升级到 v4.0.0，适用于 x86\_64 和 arm64 架构。
3. 在升级之前，请检查业务集群的组件列表。如果有组件状态异常，请不要进行升级，除非您计划通过升级来解决问题。
4. 如果集群利用了服务网格功能，请首先参考 [升级服务网格](#upgrade-service-mesh) 部分检查当前业务集群的 Kubernetes 版本是否满足 Istio 的升级要求。如果不满足，您需要将 Kubernetes 升级到符合 Istio 升级要求的版本。
5. 服务网格的 Sidecar 升级将执行 Pods 的滚动更新，这可能导致短暂的业务中断，特别是对于长连接服务。
6. 随着集群升级，现有 PostgreSQL 实例将自动重启以进行升级更新，更新期间将导致短暂的服务中断。
7. 随着集群升级，设置为自动更新策略的 MySQL-PXC、MySQL-MGR、Redis、Kafka 和 RabbitMQ 实例将自动重启进行升级更新，会造成更新期间的短暂服务中断。
8. 如果在升级过程中出现问题，优先检查升级常见问题文档。
9. 一次最多支持升级 20 个业务集群。

### 升级业务集群的功能组件

1. 访问平台管理页面，点击 "集群"，选择要升级的业务集群，点击 "功能组件"，然后点击 "升级"。
2. 升级界面将显示组件升级信息。
3. 阅读信息并确认。

<Directive type="note" title="注意">
  名为 "Kubernetes" 的组件可以不进行升级，但强烈建议与 <Term name="productShort" /> 一起触发 "Kubernetes" 组件的升级。\
  由于 <Term name="productShort" /> 自身的升级可能也会导致工作负载中断，仅仅不升级 "Kubernetes" 组件来避免工作负载中断并不是合适的方法。
</Directive>

4. 点击升级按钮。
5. 如果当前环境已修改配置设置，点击升级时会出现确认弹窗。请联系运维团队确认这些配置是否影响升级。
6. 等待升级完成。

### 升级 DevOps 工具链实例

<Directive type="note" title="注意">
  （如果未部署 DevOps 工具链，跳过此步骤）
</Directive>

1. 转到平台管理 > 工具链管理 > 工具链集成，点击工具链名称。
2. 在集成详情页面上，点击实例名称以进入实例详情页面。
3. 如果工具名称右侧出现升级图标，请点击操作 > 升级，并根据客户要求进行工具升级。点击升级信息图标以实时查看实例更新状态。
4. 如果没有升级图标，则意味着该工具没有可用的升级版本。请检查其他工具。

### 升级服务网格

<Directive type="note" title="注意">
  （如果未部署服务网格，跳过此步骤）
</Directive>

有关升级流程，请参阅产品用户手册：平台管理 > 服务网格 > 升级服务网格。

#### Istio 升级的 Kubernetes 版本要求

<table>
  <thead>
    <tr>
      <th>升级前的 Istio 版本</th>
      <th>升级后的 Istio 版本</th>
      <th>Istio 主版本升级的 Kubernetes 版本要求</th>
      <th>对应的 ACP 升级场景</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>Istio 1.20</td>
      <td>Istio 1.22</td>
      <td>Kubernetes 1.27,1.28,1.29</td>

      <td>
        ACP 3.18 升级到 4.0<br />
        ACP 3.16 升级到 4.0
      </td>
    </tr>
  </tbody>
</table>

## 升级常见问题

目前没有已知问题。
