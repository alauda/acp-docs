---
weight: 10
---

# 纯软数据中心 LB 方案（Alpha）

部署纯软数据中心 LB，通过在集群外部创建高可用负载均衡器，为集群多个 ALB 提供负载均衡能力，保证业务稳定运行。支持仅配置 IPv4、仅配置 IPv6 或 配置 IPv4、IPv6 双栈。



## 前提条件

1. 准备两台或者两台以上的主机节点作为 LB。推荐 LB 节点安装 Ubuntu 22.04 操作系统，可减少 LB 向异常 backend 节点转发流量的时间。

2. 在外部 LB 所有主机节点上提前安装以下软件（本章以两台外部 LB 主机节点为例）：

    * `ipvsadm`
    
    * `Docker（20.10.7）`

2. 使用以下命令确保每台主机 Docker 服务开机自启：`sudo systemctl enable docker.service`。

3. 每台主机节点时钟确保一致。

2. 准备 Keepalived 的镜像，用于启动外部 LB 服务，平台已内置此镜像，镜像地址形式如下：`<镜像仓库地址>/tkestack/keepalived:<版本后缀信息>`。在不同版本中的版本后缀信息可能会略有不同，可使用如下方式获取镜像仓库地址及版本后缀信息，本文档以 `build-harbor.alauda.cn/tkestack/keepalived:v3.16.0-beta.3.g598ce923` 为例进行介绍。
    
    * 可在 global 集群中执行 `kubectl get prdb base -o json | jq .spec.registry.address` 命令获取 **镜像仓库地址** 参数。
    
    * 在安装包解压目录执行 `cat ./installer/res/artifacts.json |grep keepalived -C 2|grep tag|awk '{print $2}'|awk -F '"' '{print $2}'` 命令获取 **版本后缀信息**。

## 操作步骤

**注意**：以下所有操作均在每台外部 LB 主机节点上运行一次，且主机节点的 `hostname` 不允许重复。

1. 将如下配置信息添加至文件 `/etc/modules-load.d/alive.kmod.conf`。

    ```
    ip_vs
    ip_vs_rr
    ip_vs_wrr
    ip_vs_sh
    nf_conntrack_ipv4
    nf_conntrack
    ip6t_MASQUERADE
    nf_nat_masquerade_ipv6
    ip6table_nat
    nf_conntrack_ipv6
    nf_defrag_ipv6
    nf_nat_ipv6
    ip6_tables
    ```
2. 将如下配置信息添加至文件 `/etc/sysctl.d/alive.sysctl.conf`。
    
    ```
    net.ipv4.ip_forward = 1
    net.ipv4.conf.all.arp_accept = 1
    net.ipv4.vs.conntrack = 1
    net.ipv4.vs.conn_reuse_mode = 0
    net.ipv4.vs.expire_nodest_conn = 1
    net.ipv4.vs.expire_quiescent_template = 1
    net.ipv6.conf.all.forwarding=1
    ```
3. 使用 `reboot` 命令重启。

4. 创建 Keepalived 配置文件的文件夹。

    ```
    mkdir -p /etc/keepalived
    mkdir -p /etc/keepalived/kubecfg
    ```
    
5. 按照如下文件的注释修改配置项后，存放在 `/etc/keepalived/` 文件夹中，命名为 `alive.yaml`。

    ```
    instances:
      - vip: # 可以配置多个 vip
          vip: 192.168.128.118 # vip 必须不同
          id: 20 # 每个 vip 的 id 必须不同，optional
          interface: "eth0"
          check_interval: 1 # optional default 1 执行 check 脚本的时间间隔
          check_timeout: 3  # optional default 3 执行 check 脚本的超时时间
          name: "vip-1" # 用来标识这个 instance 只能由字母数字连字符组成，开头不能是连字符
          peer: [ "192.168.128.116", "192.168.128.75" ] # keepalived 节点 ip，实际生成的 keepalived.conf 中会去掉所有在 interface 上的ip  https://github.com/osixia/docker-keepalived/issues/33
          kube_lock:
            kubecfgs: # kube-lock 使用的 kube-config list 会依次尝试这些 kubecfg 来为 keepalived 做 leader election
              - "/live/cfg/kubecfg/kubecfg01.conf"
              - "/live/cfg/kubecfg/kubecfg02.conf"
              - "/live/cfg/kubecfg/kubecfg03.conf"
        ipvs: # optiaon ipvs 的配置
          ips: [ "192.168.143.192", "192.168.138.100","192.168.129.100" ] # ipvs 后端，k8s master node ip 修改为 alb 节点的 node ip 地址。
          ports: # 配置 vip 上每个端口的检查健康逻辑
            - port: 80 # vserver 的端口和 realserver 的端口保持一致
              virtual_server_config: |
                delay_loop 10  # 执行 realserver 健康检查的时间间隔
                lb_algo rr
                lb_kind NAT
                protocol TCP
              raw_check: |
                TCP_CHECK {
                    connect_timeout 10
                    connect_port 1936
                }
      - vip:
          vip: 2004::192:168:128:118
          id: 102
          interface: "eth0"
          peer: [ "2004::192:168:128:75","2004::192:168:128:116" ]
          kube_lock:
            kubecfgs: # kube-lock 使用的 kube-config list 会依次尝试这些 kubecfg 来为 keepalived 做 leader election
              - "/live/cfg/kubecfg/kubecfg01.conf"
              - "/live/cfg/kubecfg/kubecfg02.conf"
              - "/live/cfg/kubecfg/kubecfg03.conf"
        ipvs:
          ips: [ "2004::192:168:143:192","2004::192:168:138:100","2004::192:168:129:100" ]
          ports:
            - port: 80
              virtual_server_config: |
                delay_loop 10
                lb_algo rr
                lb_kind NAT
                protocol TCP
              raw_check: |
                TCP_CHECK {
                    connect_timeout 1
                    connect_port 1936
                }
    ```

6. 在业务集群执行如下命令，检查配置文件的证书过期时间，确保证书处于有效期内。证书过期后 LB 功能将不可用，此时需联系平台管理员更新证书。

    ```
    openssl x509 -in <(cat /etc/kubernetes/admin.conf | grep client-certificate-data | awk '{print $NF}' | base64 -d ) -noout -dates
    ```

7. 在本平台 Kubernetes 集群三个 Master 节点中的 `/etc/kubernetes/admin.conf` 文件复制到外部 LB 节点的 `/etc/keepalived/kubecfg` 文件夹中，并以序号命名，例如：`kubecfg01.conf`，再将这三个文件中的 `apiserver` 节点地址修改成 Kubernetes 集群对应的 `apiserver` 真实节点地址。

    **注意**：平台证书更新后，需要重新执行本步骤，并覆盖原有文件。

7. 检测证书有效性。

    1. 将业务集群 Master 节点上的 `/usr/bin/kubectl` 拷贝到 LB 节点。

    2. 执行 `chmod +x /usr/bin/kubectl` 命令赋予执行权限。

    3. 执行如下命令确认证书有效性。

        ```
        kubectl --kubeconfig=/etc/keepalived/kubecfg/kubecfg01.conf get node
        kubectl --kubeconfig=/etc/keepalived/kubecfg/kubecfg02.conf get node
        kubectl --kubeconfig=/etc/keepalived/kubecfg/kubecfg03.conf get node
        ```

        当返回如下结果时，说明证书可用。

        ```
        kubectl --kubeconfig=/etc/keepalived/kubecfg/kubecfg01.conf get node
        ## 回显结果
        NAME              STATUS   ROLES                  AGE     VERSION
        192.168.129.100   Ready    <none>                 7d22h   v1.25.6
        192.168.134.167   Ready    control-plane,master   7d22h   v1.25.6
        192.168.138.100   Ready    <none>                 7d22h   v1.25.6
        192.168.143.116   Ready    control-plane,master   7d22h   v1.25.6
        192.168.143.192   Ready    <none>                 7d22h   v1.25.6
        192.168.143.79    Ready    control-plane,master   7d22h   v1.25.6
        
        kubectl --kubeconfig=/etc/keepalived/kubecfg/kubecfg02.conf get node
        ## 回显结果
        NAME              STATUS   ROLES                  AGE     VERSION
        192.168.129.100   Ready    <none>                 7d22h   v1.25.6
        192.168.134.167   Ready    control-plane,master   7d22h   v1.25.6
        192.168.138.100   Ready    <none>                 7d22h   v1.25.6
        192.168.143.116   Ready    control-plane,master   7d22h   v1.25.6
        192.168.143.192   Ready    <none>                 7d22h   v1.25.6
        192.168.143.79    Ready    control-plane,master   7d22h   v1.25.6
        
        kubectl --kubeconfig=/etc/keepalived/kubecfg/kubecfg03.conf get node
        ## 回显结果
        NAME              STATUS   ROLES                  AGE     VERSION
        192.168.129.100   Ready    <none>                 7d22h   v1.25.6
        192.168.134.167   Ready    control-plane,master   7d22h   v1.25.6
        192.168.138.100   Ready    <none>                 7d22h   v1.25.6
        192.168.143.116   Ready    control-plane,master   7d22h   v1.25.6
        192.168.143.192   Ready    <none>                 7d22h   v1.25.6
        192.168.143.79    Ready    control-plane,master   7d22h   v1.25.6
        ```


8. 上传 Keepalived 的镜像到外部 LB 节点上并使用 Docker 运行 Keepalived。

    ```
    docker run -dt --restart=always --privileged --network=host -v /etc/keepalived:/live/cfg build-harbor.alauda.cn/tkestack/keepalived:v3.16.0-beta.3.g598ce923
    ```

9. 在访问 `keepalvied` 的节点上运行以下命令：`sysctl -w net.ipv4.conf.all.arp_accept=1`。

## 验证    

1. 运行 `ipvsadm -ln` 命令查看 IPVS 规则，您将看到适用于业务集群 ALB 的 IPv4 和 IPv6 规则。

    ```
    IP Virtual Server version 1.2.1 (size=4096)
    Prot LocalAddress:Port Scheduler Flags
      -> RemoteAddress:Port           Forward Weight        ActiveConn InActConn
    TCP  192.168.128.118:80 rr
      -> 192.168.129.100:80           Masq    1      0          0        
      -> 192.168.138.100:80           Masq    1      0          0        
      -> 192.168.143.192:80           Masq    1      0          0        
    TCP  [2004::192:168:128:118]:80 rr
      -> [2004::192:168:129:100]:80   Masq    1      0          0        
      -> [2004::192:168:138:100]:80   Masq    1      0          0        
      -> [2004::192:168:143:192]:80   Masq    1      0          0
    ```
    
2. 关闭 VIP 所在的 LB 节点，测试 IPv4 和 IPv6 的 VIP 是否可以正常迁移到另外一个节点，通常 20 秒内可完成迁移。

3. 使用 `curl` 命令在非 LB 节点测试与 VIP 的通信是否正常。

    ```
    curl 192.168.128.118
    
    <!DOCTYPE html>
    <html>
    <head>
    <title>Welcome to nginx!</title>
    <style>
    html { color-scheme: light dark; }
    body { width: 35em; margin: 0 auto;
    font-family: Tahoma, Verdana, Arial, sans-serif; }
    </style>
    </head>
    <body>
    <h1>Welcome to nginx!</h1>
    <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p>
    
    <p>For online documentation and support please refer to <a href="http://nginx.org/">nginx.org</a>.<br/>
    Commercial support is available at <a href="http://nginx.com/">nginx.com</a>.</p>
 
    <p><em>Thank you for using nginx.</em></p>
    </body>
    </html>
    ```
    
    ```
    curl -6 [2004::192:168:128:118]:80 -g
    
    <!DOCTYPE html>
    <html>
    <head>
    <title>Welcome to nginx!</title>
    <style>
    html { color-scheme: light dark; }
    body { width: 35em; margin: 0 auto;
    font-family: Tahoma, Verdana, Arial, sans-serif; }
    </style>
    </head>
    <body>
    <h1>Welcome to nginx!</h1>
    <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p>
 
    <p>For online documentation and support please refer to <a href="http://nginx.org/">nginx.org</a>.<br/>
    Commercial support is available at<a href="http://nginx.com/">nginx.com</a>.</p>
 
    <p><em>Thank you for using nginx.</em></p>
    </body>
    </html>
    ```

